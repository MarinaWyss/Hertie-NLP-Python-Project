{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data sets\n",
    "breitbart = pd.read_csv('data/breitbart_data.csv')\n",
    "fox = pd.read_csv('data/fox_data.csv')\n",
    "wt = pd.read_csv('data/wt_data.csv')\n",
    "ap = pd.read_csv('data/ap_data.csv')\n",
    "nbc = pd.read_csv('data/nbc_data.csv')\n",
    "nyt = pd.read_csv('data/nyt_data.csv')\n",
    "politico = pd.read_csv('data/politico_data.csv')\n",
    "buzzfeed = pd.read_csv('data/buzzfeed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make dates comparable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox['date'] = [x.split('T')[0] for x in fox['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt['date'] = [x.replace(' -\\n\\t\\t\\t\\n\\t\\t\\t\\tAssociated Press\\n -    Updated:', '') for x in wt['date']]\n",
    "wt['date'] = [x.replace(' -\\n\\t\\t\\t\\n\\t\\t\\t\\tThe Washington Times\\n -    Updated:', '') for x in wt['date']]\n",
    "wt['date'] = [parse(x) for x in wt['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap['date'] = [x.split('T')[0] for x in ap['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc = nbc.copy().dropna()\n",
    "nbc['date'] = [parse(x) for x in nbc['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "buzzfeed['date'] = [x.replace('Posted on ', '').replace('Last updated on ', '') for x in buzzfeed['date']]\n",
    "buzzfeed['date'] = [x.strip() for x in buzzfeed['date']]\n",
    "buzzfeed['date'] = [x.split(',')[0:2] for x in buzzfeed['date']]\n",
    "buzzfeed['date'] = [''.join(x) for x in buzzfeed['date']]\n",
    "buzzfeed['date'] = [parse(x) for x in buzzfeed['date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.concat([\n",
    "    breitbart,\n",
    "    fox,\n",
    "    wt,\n",
    "    ap,\n",
    "    nbc,\n",
    "    nyt,\n",
    "    politico,\n",
    "    buzzfeed\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = ['Trump', 'Bernie', 'Sanders', 'Biden', 'Warren', 'Buttigieg', \n",
    "              'Bloomberg', 'Klobuchar', 'Yang', 'Steyer', 'Gabbard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cand in candidates:\n",
    "    full_data[cand] = pd.np.where(full_data['article_text'].str.contains(cand), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['Sanders'] = pd.np.where(full_data['Bernie'] == 1, 1, full_data['Sanders'])\n",
    "full_data = full_data.drop(columns = 'Bernie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'Bernie' from candidates list before summing\n",
    "candidates.remove('Bernie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['candidates_mentioned'] = full_data.loc[:, candidates].sum(axis = 1)\n",
    "full_data = full_data[full_data['candidates_mentioned'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of entries in new data: 1784\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_data = pd.read_csv('data/full_data.csv')\n",
    "num_old = len(old_data)\n",
    "\n",
    "# append new data\n",
    "full_data = old_data.append(full_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "full_data.to_csv(\"data/full_data.csv\", index = False)\n",
    "num_now = len(full_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))\n",
    "print(\"difference: {}\".format(num_now - num_old))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Articles to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create article id #\n",
    "data_for_sentences = full_data[['article_text', 'article_title', 'date', 'link', 'publisher']].copy()\n",
    "data_for_sentences = data_for_sentences.reset_index()\n",
    "data_for_sentences = data_for_sentences.reset_index().rename(columns = {'level_0': 'article_id'}).drop(columns = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split article text to sentences\n",
    "sentences = data_for_sentences['article_text'].copy().str.split('.').apply(pd.Series, 1).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add correct article id # to each sentence\n",
    "sentences.index.droplevel(-1) \n",
    "sentences.name = 'article_text'\n",
    "sentences = sentences.reset_index().drop(columns = 'level_1').rename(columns = {'level_0': 'article_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop paragraph-form article text\n",
    "data_for_sentences = data_for_sentences.drop(columns = 'article_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge sentence article text\n",
    "sentence_data = data_for_sentences.merge(sentences, how='left', on='article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "mask = sentence_data['article_text'].astype(str).str.len() < 15\n",
    "sentence_data.loc[mask, 'article_text'] = ''\n",
    "sentence_data = sentence_data[(sentence_data['article_text'] != '')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_sent = ['Trump', 'Bernie', 'Sanders', 'Biden', 'Warren', 'Buttigieg', \n",
    "                   'Bloomberg', 'Klobuchar', 'Yang', 'Steyer', 'Gabbard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cand in candidates_sent:\n",
    "    sentence_data[cand] = pd.np.where(sentence_data['article_text'].str.contains(cand), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_data['Sanders'] = pd.np.where(sentence_data['Bernie'] == 1, 1, sentence_data['Sanders'])\n",
    "sentence_data = sentence_data.drop(columns = 'Bernie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'Bernie' from candidates list to create sum\n",
    "candidates_sent.remove('Bernie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary rows\n",
    "sentence_data['candidates_mentioned'] = sentence_data.loc[:, candidates_sent].sum(axis = 1)\n",
    "sentence_data = sentence_data[sentence_data['candidates_mentioned'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of entries in new data: 18462\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_sentence_data = pd.read_csv('data/sentence_data.csv')\n",
    "num_sentence_old = len(old_sentence_data)\n",
    "\n",
    "# append new data\n",
    "sentence_data = old_sentence_data.append(sentence_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "sentence_data.to_csv(\"data/sentence_data.csv\", index = False)\n",
    "num_sentence_now = len(sentence_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_sentence_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_sentence_now))\n",
    "print(\"difference: {}\".format(num_sentence_now - num_sentence_old))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
