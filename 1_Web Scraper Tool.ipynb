{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper Tool for 5 US Media Outlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Breitbart - Very Conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "breitbart_request = requests.get('https://www.breitbart.com/politics/')\n",
    "breitbart_homepage = breitbart_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "breitbart_soup = BeautifulSoup(breitbart_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# locate article URLs\n",
    "breitbart_tags = breitbart_soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_articles = 30\n",
    "\n",
    "# get article titles, content, and links\n",
    "breitbart_links = []\n",
    "breitbart_titles = []\n",
    "breitbart_dates = []\n",
    "breitbart_contents = []\n",
    "\n",
    "for n in np.arange(0, number_of_articles):\n",
    "\n",
    "    # get article link\n",
    "    link = breitbart_tags[n].find('a')['href']\n",
    "    link = \"https://www.breitbart.com\" + link\n",
    "    breitbart_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = breitbart_tags[n].find('a').get_text()\n",
    "    breitbart_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.time.attrs['datetime']\n",
    "    date = date[:-10]\n",
    "    breitbart_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', class_='entry-content')\n",
    "    x = body[0].find_all('p')\n",
    "    \n",
    "    # combine paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    breitbart_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling data\n",
    "breitbart_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'Breitbart',\n",
    "    'date': breitbart_dates,\n",
    "    'link': breitbart_links,\n",
    "    'article_title': breitbart_titles,\n",
    "    'article_text': breitbart_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A group of Conservative party grandees will se...</td>\n",
       "      <td>Tories Rebel over Huawei: As ‘Ridiculous’ as G...</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>https://www.breitbart.com/europe/2020/03/07/to...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Greece has accused the Turkish government of s...</td>\n",
       "      <td>Greece Denounces ‘Fake News’ as Turkey Claims ...</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>https://www.breitbart.com/europe/2020/03/07/gr...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dramatic pictures and video are emerging of fi...</td>\n",
       "      <td>PICTURES: Greek Border in Flames as Migrants K...</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>https://www.breitbart.com/europe/2020/03/07/pi...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Finland’s millennial feminist-led government f...</td>\n",
       "      <td>Finland’s Millennial Feminist Govt to Help Gre...</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>https://www.breitbart.com/europe/2020/03/07/fi...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A total of 76 per cent of the Greek public sup...</td>\n",
       "      <td>Nearly 8 in 10 Greeks Support Government’s Bor...</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>https://www.breitbart.com/europe/2020/03/07/ne...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  \\\n",
       "0  A group of Conservative party grandees will se...   \n",
       "1  Greece has accused the Turkish government of s...   \n",
       "2  Dramatic pictures and video are emerging of fi...   \n",
       "3  Finland’s millennial feminist-led government f...   \n",
       "4  A total of 76 per cent of the Greek public sup...   \n",
       "\n",
       "                                       article_title        date  \\\n",
       "0  Tories Rebel over Huawei: As ‘Ridiculous’ as G...  2020-03-07   \n",
       "1  Greece Denounces ‘Fake News’ as Turkey Claims ...  2020-03-07   \n",
       "2  PICTURES: Greek Border in Flames as Migrants K...  2020-03-07   \n",
       "3  Finland’s Millennial Feminist Govt to Help Gre...  2020-03-07   \n",
       "4  Nearly 8 in 10 Greeks Support Government’s Bor...  2020-03-07   \n",
       "\n",
       "                                                link  publisher  \n",
       "0  https://www.breitbart.com/europe/2020/03/07/to...  Breitbart  \n",
       "1  https://www.breitbart.com/europe/2020/03/07/gr...  Breitbart  \n",
       "2  https://www.breitbart.com/europe/2020/03/07/pi...  Breitbart  \n",
       "3  https://www.breitbart.com/europe/2020/03/07/fi...  Breitbart  \n",
       "4  https://www.breitbart.com/europe/2020/03/07/ne...  Breitbart  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure it looks nice\n",
    "breitbart_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to csv\n",
    "date = breitbart_data.date[0]\n",
    "filename = \"breitbart_data_{}.csv\".format(date)\n",
    "\n",
    "breitbart_data.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fox - Conservative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get links from the Fox Politics homepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.foxnews.com/category/politics/2020-presidential-election <- 127 links // 15  without junk\n",
    "# https://www.foxnews.com/politics <- 150 links (but junk?) // 31 without junk\n",
    "# i also tried running this for each candidate's section but there is overlap in articles\n",
    "\n",
    "# load the HTML content using requests and save into a variable\n",
    "r2 = requests.get('https://www.foxnews.com/politics')\n",
    "homepage2 = r2.content\n",
    "\n",
    "# create a soup to allow BeautifulSoup to work\n",
    "soup2 = BeautifulSoup(homepage2, 'html.parser')\n",
    "\n",
    "# locate and retrieve all links - WAS NOT ABLE TO ISOLATE JUST ARTICLE LINKS BECUASE NO UNIQUE TAG ELEMENT!!\n",
    "# you can see how complex the homepage is: print(soup2.prettify())\n",
    "homepage_tags2 = soup2.find_all('a')\n",
    "homepage_links2 = []\n",
    "\n",
    "for link in homepage_tags2:\n",
    "    homepage_links2.append(link.get('href'))\n",
    "\n",
    "# remove duplicates by turning list into a set\n",
    "homepage_links2 = set(homepage_links2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove junk links from list <- MAYBE THIS COULD BE DONE IN A BETTER WAY??\n",
    "# creating a list of exact junk links that will never be needed when pulling from the homepage\n",
    "junk_links2 = ['/', '/us', '/world', '/opinion', '/politics', '/entertainment', '//www.foxbusiness.com', \n",
    "                  '/lifestyle', '/shows', '//www.foxnews.com/shows/fox-nation', '//radio.foxnews.com/podcast',\n",
    "                  '#', '//foxnews.com/weather/your-weather/index.html', '#', '#', \n",
    "                  '//video.foxnews.com/v/5614615980001/?#sp=watch-live', '#', '/us', '/world', '/opinion', '/politics', \n",
    "                  '/official-polls', '/category/politics/elections', \n",
    "                  '/entertainment', '//video.foxnews.com/playlist/entertainment-latest-entertainment/', \n",
    "                  '//www.foxbusiness.com/', '//www.foxbusiness.com/markets', '//www.foxbusiness.com/politics', \n",
    "                  '//www.foxbusiness.com/category/technology', '//www.foxbusiness.com/features', \n",
    "                  '//www.foxbusiness.com/category/business-leaders', '/lifestyle', '/food-drink', '/auto', \n",
    "                  '/travel', '/family', '/science', '/tech', '/health', '/shows', '/shows', \n",
    "                  '/person/personalities', '//video.foxnews.com/v/5614615980001/?#sp=watch-live', \n",
    "                  '//video.foxnews.com/playlist/episodic-most-recent-episodes/', \n",
    "                  '//video.foxnews.com/#sp=show-clips', '//video.foxnews.com/#sp=news-clips', \n",
    "                  '//www.foxnews.com/contact', '//foxcareers.com/Search/SearchResults?brand=Fox%20News%20Careers', \n",
    "                  '/foxaroundtheworld/', 'mailto:adsales@foxnews.com?subject=Advertising%20Inquiry', \n",
    "                  '//press.foxnews.com/media-contacts/', '//press.foxnews.com/', '/compliance', \n",
    "                  'https://supplierdiversity.foxnews.com/', '//www.foxnews.com/shows/fox-nation', '//shop.foxnews.com', \n",
    "                  '/go', '//radio.foxnews.com/', '/alerts/subscribe', '/newsletter-signup/alerts', '//radio.foxnews.com/podcast', \n",
    "                  '/apps-products', '//www.foxnews.com', '/terms-of-use', '/privacy-policy', '/donotsell', '/closed-captioning', \n",
    "                  '//help.foxnews.com', '/contact', '//www.facebook.com/FoxNews', '//twitter.com/foxnews', '//www.google.com/+FoxNews', \n",
    "                  '//www.instagram.com/foxnews', '/about/rss/', '/alerts/subscribe', 'https://flipboard.com/@FoxNews', \n",
    "                  '//www.foxnews.com/rss/index.html', '//www.foxnews.com/alerts/subscribe.html', \n",
    "                   '/accessibility-statement', 'https://video.foxnews.com/v/', 'https://foxnews.com/elections']\n",
    "\n",
    "# removing all links that lead to section pages AND other specified junk URLs\n",
    "homepage_links2 = [x for x in homepage_links2 if \"/category\" not in x] \n",
    "homepage_links2 = [x for x in homepage_links2 if \"/v/\" not in x] \n",
    "homepage_links2 = [x for x in homepage_links2 if x not in junk_links2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.foxnews.com/media/chris-matthews-resignation-from-msnbc-hastened-by-series-of-blunders', 'https://www.foxnews.com/politics/warren-huddles-with-advisers-as-progressive-pressure-for-her-to-drop-out-mounts', 'https://www.foxnews.com/politics/supreme-court-at-apparent-odds-over-key-abortion-case-over-clinic-access-restrictions', 'https://www.foxnews.com/politics/fox-news-poll-sanders-knocks-biden-out-of-first-majority-thinks-trump-wins', 'https://www.foxnews.com/politics/issa-mounts-a-comeback-in-california', 'https://www.foxnews.com/politics/biden-campaign-hits-back-at-sanders-ad-showing-obama-praise', 'https://www.foxnews.com/media/francis-tarlov-joe-biden-trump-clinton', 'https://www.foxnews.com/politics/pence-says-passengers-on-flights-from-italy-and-south-korea-will-be-screened-multiple-times-for-coronavirus', 'https://www.foxnews.com/politics/mike-bloomberg-suspends-presidential-campaign-after-super-tuesday-show', 'https://www.foxnews.com/politics/aoc-aligned-progressive-candidates-fall-flat-in-super-tuesday-contests', 'https://www.foxnews.com/politics/texas-congressional-race-results', 'https://www.foxnews.com/politics/super-tuesday-results-where-the-2020-democratic-candidates-stand', 'https://www.foxnews.com/media/biden-shocks-the-pundits-again-with-surprisingly-strong-super-tuesday', 'https://www.foxnews.com/media/bidens-super-tuesday-challenge-after-defying-the-medias-obituaries', 'https://www.foxnews.com/politics/ex-cuban-prisoner-alan-gross-accuses-sanders-of-praising-cuba-during-visit', 'https://www.foxnews.com/media/markets-sigh-relief-bernie-falls-behind-biden', 'https://www.foxnews.com/politics/fox-news-poll-capitalism-socialism-bernie-sanders', 'https://www.foxnews.com/politics/biden-thumps-bernie', 'https://www.foxnews.com/opinion/newt-gingrich-50-hours-that-changed-the-2020-democratic-presidential-race', 'https://www.foxnews.com/politics/fox-news-voter-analysis-biden-sanders-emerge-from-dem-pack-on-super-tuesday', 'https://www.foxnews.com/politics/trump-calls-warren-selfish-for-staying-in-2020-democratic-contest', 'https://www.foxnews.com/politics/fox-news-poll-trump-economy-failed-to-unify']\n"
     ]
    }
   ],
   "source": [
    "# add https://www.foxnews.com to links that don't have this\n",
    "# create new list for prepared links\n",
    "hp_links2 = []\n",
    "\n",
    "for item in homepage_links2:\n",
    "    new_item = 'https://www.foxnews.com' + item\n",
    "    hp_links2.append(new_item)\n",
    "\n",
    "print(hp_links2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: check for duplicates to set of existing URLs to make sure we only pull data for new ones\n",
    "    # import full dataset in\n",
    "    # remove urls in homepage_links3 that are already included in the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get article text and other relevent data from new articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas dataframe where all of the retrieved data will be stored\n",
    "#article_data2 = pd.DataFrame()\n",
    "\n",
    "#article_data2['headline'] = []\n",
    "#article_data2['date_published'] = []\n",
    "#article_data2['date_modified'] = []\n",
    "#article_data2['description'] = []\n",
    "#article_data2['author'] = []\n",
    "#article_data2['url'] = []\n",
    "\n",
    "#export empty df to file\n",
    "#article_data2.to_csv('Fox_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "headline = []\n",
    "date_published = []\n",
    "date_modified = []\n",
    "description = []\n",
    "author = []\n",
    "url = []\n",
    "\n",
    "for link in hp_links2:\n",
    "    # load the HTML content using requests and save into a variable\n",
    "    r2_article = requests.get(link)\n",
    "    article2 = r2_article.content\n",
    "\n",
    "    # create a soups to allow BeautifulSoup to work\n",
    "    article_soup2 = BeautifulSoup(article2, 'html.parser')\n",
    "\n",
    "    # get article text\n",
    "    article_text = article_soup2.find(\"body\").get_text()\n",
    "    text.append(text)\n",
    "    \n",
    "    # retrieve article specific metadata\n",
    "    metadata2 = article_soup2.find_all(\"script\")[2].get_text()\n",
    "    metadata2 = metadata2.split(\",\")\n",
    "\n",
    "    for item in metadata2:\n",
    "        if 'headline' in item:\n",
    "            headline.append(item)\n",
    "        elif 'datePublished' in item:\n",
    "            date_published.append(item)\n",
    "        elif 'dateModified' in item:\n",
    "            date_modified.append(item)\n",
    "        elif 'description' in item:\n",
    "            description.append(item)\n",
    "        elif 'name' in item and 'Fox News' not in item:\n",
    "            author.append(item)\n",
    "        elif 'mainEntityOfPage' in item:\n",
    "            url.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 22\n",
      "headline 22\n",
      "date_published 22\n",
      "date_modified 22\n",
      "description 22\n",
      "author 21\n",
      "url 22\n"
     ]
    }
   ],
   "source": [
    "print('text', len(text))\n",
    "print('headline', len(headline))\n",
    "print('date_published', len(date_published))\n",
    "print('date_modified', len(date_modified))\n",
    "print('description', len(description))\n",
    "print('author', len(author))\n",
    "print('url', len(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-931919f5c54e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marticle_data2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mheadline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date_published'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdate_published\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date_modified'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdate_modified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'description'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'author'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'url'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    409\u001b[0m             )\n\u001b[1;32m    410\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         ]\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arrays must all be same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#article_data2 = pd.DataFrame({'text':text,'title':headline, 'date_published':date_published, 'date_modified':date_modified, 'description':description, 'author':author, 'url':url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import collection of all responses\n",
    "all_articles2 = pd.read_csv(\"Fox_Data.csv\", index_col=[0])\n",
    "\n",
    "# merge new increment of data to the csv\n",
    "all_articles2 = article_data2.append(article_data2, ignore_index=True)\n",
    "\n",
    "#export with new increment to save\n",
    "all_articles.to_csv('Fox_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<meta content=\"Dana Blanton\" data-hid=\"dc.creator\" data-n-head=\"true\" name=\"dc.creator\" scheme=\"dcterms.creator\"/>\n"
     ]
    }
   ],
   "source": [
    "# another spot the author is located just in case\n",
    "# test = article_soup3.find_all(\"meta\")[17]\n",
    "#print(yay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Wall Street Journal - Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
