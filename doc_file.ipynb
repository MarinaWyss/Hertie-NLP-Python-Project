{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import UnicodeDammit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    \"\"\"\n",
    "    A class used to scrape articles from 8 different U.S. media outlets.\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    Breitbart(self)\n",
    "        Scrapes new articles from breitbart.com/politics/ and saves them to a .csv\n",
    "    FoxNews(self)\n",
    "        Scrapes new articles from foxnews.com/politics/ and saves them to a .csv\n",
    "    WashingtonTimes(self)\n",
    "        Scrapes new articles from washingtontimes.com/news/politics/ and saves them to a .csv\n",
    "    AP(self)\n",
    "        Scrapes new articles from apnews.com/apf-politics and saves them to a .csv\n",
    "    NYT(self)\n",
    "        Scrapes new articles from nytimes.com/section/politics and saves them to a .csv\n",
    "    NBC(self)\n",
    "        Scrapes new articles from nbcnews.com/politics and saves them to a .csv\n",
    "    Politico(self)\n",
    "        Scrapes new articles from politico.com/politics and saves them to a .csv\n",
    "    Buzzfeed(self)\n",
    "        Scrapes new articles from buzzfeednews.com/section/politics and saves them to a .csv\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.Breitbart = None\n",
    "        self.FoxNews = None\n",
    "        self.WashingtonTimes = None\n",
    "        self.AP = None\n",
    "        self.NYT = None\n",
    "        self.NBC = None\n",
    "        self.Politico = None\n",
    "        self.Buzzfeed = None\n",
    "        \n",
    "    def Breitbart(self):\n",
    "        \"\"\"\n",
    "        Scrapes new articles from breitbart.com/politics/ and saves them to a .csv\n",
    "        \"\"\"\n",
    "        # load the HTML content using requests and save into a variable\n",
    "        breitbart_request = requests.get('https://www.breitbart.com/politics/')\n",
    "        breitbart_homepage = breitbart_request.content\n",
    "        \n",
    "        # create soup \n",
    "        breitbart_soup = BeautifulSoup(breitbart_homepage, 'html.parser')\n",
    "        \n",
    "        # locate article URLs\n",
    "        breitbart_tags = breitbart_soup.find_all('h2')\n",
    "        \n",
    "        # setup\n",
    "        number_of_articles = min(len(breitbart_tags), 30)\n",
    "\n",
    "        breitbart_links = []\n",
    "        breitbart_titles = []\n",
    "        breitbart_dates = []\n",
    "        breitbart_contents = []\n",
    "        \n",
    "        # get article titles, content, and links\n",
    "        for n in np.arange(0, number_of_articles):\n",
    "\n",
    "            # get article link\n",
    "            link = breitbart_tags[n].find('a')['href']\n",
    "            link = \"https://www.breitbart.com\" + link\n",
    "            breitbart_links.append(link)\n",
    "\n",
    "        # get article title\n",
    "        title = breitbart_tags[n].find('a').get_text()\n",
    "        breitbart_titles.append(title)\n",
    "\n",
    "        # prep article content\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "\n",
    "        # get publication datetime\n",
    "        date = soup_article.time.attrs['datetime']\n",
    "        date = date[:-10]\n",
    "        breitbart_dates.append(date)\n",
    "\n",
    "        # get article content\n",
    "        body = soup_article.find_all('div', class_='entry-content')\n",
    "        x = body[0].find_all('p')\n",
    "\n",
    "        # combine paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        breitbart_contents.append(final_article)\n",
    "        \n",
    "        # assembling data\n",
    "        breitbart_data = pd.DataFrame.from_dict({\n",
    "        'publisher': 'Breitbart',\n",
    "        'date': breitbart_dates,\n",
    "        'link': breitbart_links,\n",
    "        'article_title': breitbart_titles,\n",
    "        'article_text': breitbart_contents \n",
    "        })\n",
    "        \n",
    "        # read in old data\n",
    "        #old_breitbart_data = pd.read_csv('data/breitbart_data.csv')\n",
    "        #num_old = len(old_breitbart_data)\n",
    "\n",
    "        # append new data\n",
    "        #breitbart_data = old_breitbart_data.append(breitbart_data).drop_duplicates()\n",
    "\n",
    "        # save new .csv\n",
    "        #breitbart_data.to_csv(\"data/breitbart_data.csv\", index = False)\n",
    "        #num_now = len(breitbart_data)\n",
    "\n",
    "        #print(\"number of entries in old data: {}\".format(num_old))\n",
    "        #print(\"total number of entries in new data: {}\".format(num_now))\n",
    "\n",
    "    def FoxNews(self):\n",
    "        \"\"\"\n",
    "        Scrapes new articles from foxnews.com/politics/ and saves them to a .csv\n",
    "        \"\"\"\n",
    "        # load the HTML content using requests and save into a variable\n",
    "        fox_requests = requests.get('https://www.foxnews.com/politics')\n",
    "        fox_homepage = fox_requests.content\n",
    "        # create a soup to allow BeautifulSoup to work\n",
    "        fox_soup = BeautifulSoup(fox_homepage, 'html.parser')\n",
    "        # locate article links\n",
    "        fox_tags = fox_soup.find_all('article')\n",
    "        # setup\n",
    "        fox_links = []\n",
    "        fox_text = []\n",
    "        fox_titles = []\n",
    "        fox_dates = []\n",
    "        number_of_articles = 30\n",
    "        # get homepage article links\n",
    "        for n in np.arange(0, number_of_articles):\n",
    "            link = fox_tags[n].find('a')\n",
    "            link = link.get('href')\n",
    "            link = \"https://foxnews.com\" + link\n",
    "            fox_links.append(link)\n",
    "            fox_links = [x for x in fox_links if \"/v/\" not in x]\n",
    "            \n",
    "        # prep for article content\n",
    "        for link in fox_links:\n",
    "            fox_article_request = requests.get(link)\n",
    "            fox_article = fox_article_request.content\n",
    "            fox_article_soup = BeautifulSoup(fox_article, 'html.parser')\n",
    "\n",
    "            # get article metadata\n",
    "            fox_metadata = fox_article_soup.find_all('script')[2].get_text()\n",
    "            fox_metadata = fox_metadata.split(\",\")\n",
    "\n",
    "            for item in fox_metadata:\n",
    "\n",
    "                # get article title\n",
    "                if 'headline' in item:\n",
    "                    item = item.replace('\\n',\"\")\n",
    "                    item = item.replace('headline', \"\")\n",
    "                    item = item.replace(':', \"\")\n",
    "                    item = item.replace('\"', '')\n",
    "                    fox_titles.append(item)\n",
    "\n",
    "                # get article date\n",
    "                elif 'datePublished' in item:\n",
    "                    item = item.replace('\\n',\"\")\n",
    "                    item = item.replace('datePublished', \"\")\n",
    "                    item = item.replace(':', \"\")\n",
    "                    item = item.replace('\"', '')\n",
    "                    fox_dates.append(item)\n",
    "\n",
    "            # get article text\n",
    "            body = fox_article_soup.find_all('div')\n",
    "            x = body[0].find_all('p')\n",
    "\n",
    "            # combine paragraphs\n",
    "            list_paragraphs = []\n",
    "            for p in np.arange(0, len(x)):\n",
    "                paragraph = x[p].get_text()\n",
    "                paragraph = paragraph.replace('\\n',\"\")\n",
    "                list_paragraphs.append(paragraph)\n",
    "\n",
    "                # removing copyright info and newsletter junk from the article\n",
    "                final_article = \" \".join(list_paragraphs)\n",
    "                final_article = final_article.replace(\"This material may not be published, broadcast, rewritten, or redistributed. ©2020 FOX News Network, LLC. All rights reserved. All market data delayed 20 minutes.\", \" \")\n",
    "                final_article = final_article.replace(\"This material may not be published, broadcast, rewritten,\", \" \")\n",
    "                final_article = final_article.replace(\"or redistributed. ©2020 FOX News Network, LLC. All rights reserved.\", \" \")\n",
    "                final_article = final_article.replace(\"All market data delayed 20 minutes.\", \" \")\n",
    "                final_article = final_article.replace(\"Get all the stories you need-to-know from the most powerful name in news delivered first thing every morning to your inbox Subscribed You've successfully subscribed to this newsletter!\", \" \")\n",
    "            fox_text.append(final_article)\n",
    "\n",
    "            # join fox data\n",
    "            fox_data = pd.DataFrame.from_dict({\n",
    "            'publisher': 'Fox',\n",
    "            'date': fox_dates,\n",
    "            'link': fox_links,\n",
    "            'article_title': fox_titles,\n",
    "            'article_text': fox_text \n",
    "            })\n",
    "\n",
    "        # read in old data\n",
    "        #old_fox_data = pd.read_csv('data/fox_data.csv')\n",
    "        #num_old = len(old_fox_data)\n",
    "\n",
    "        # append new data\n",
    "        #fox_data = old_fox_data.append(fox_data).drop_duplicates()\n",
    "\n",
    "        # save new .csv\n",
    "        #fox_data.to_csv(\"data/fox_data.csv\", index = False)\n",
    "        #num_now = len(fox_data)\n",
    "        \n",
    "    def WashingtonTimes(self):\n",
    "        \"\"\"\n",
    "        Scrapes new articles from washingtontimes.com/news/politics/ and saves them to a .csv\n",
    "        \"\"\"\n",
    "        # load the HTML content using requests and save into a variable\n",
    "        wt_request = requests.get('https://www.washingtontimes.com/news/politics/')\n",
    "        wt_homepage = wt_request.content\n",
    "        # create soup \n",
    "        wt_soup = BeautifulSoup(wt_homepage, 'html.parser')\n",
    "        # locate article URLs\n",
    "        wt_tags = wt_soup.find_all('h2', class_=\"article-headline\")\n",
    "        # setup\n",
    "        number_of_articles = len(wt_tags)\n",
    "\n",
    "        # get article titles, content, and links\n",
    "        wt_links = []\n",
    "        wt_titles = []\n",
    "        wt_dates = []\n",
    "        wt_contents = []\n",
    "        \n",
    "        # get article titles, content, and links\n",
    "        for n in np.arange(0, number_of_articles):\n",
    "\n",
    "            # get article link\n",
    "            link = wt_tags[n].find('a')['href']\n",
    "            link = 'https://www.washingtontimes.com' + link\n",
    "            wt_links.append(link)\n",
    "\n",
    "            # get article title\n",
    "            title = wt_tags[n].find('a').get_text()\n",
    "            wt_titles.append(title)\n",
    "\n",
    "            # prep article content\n",
    "            article = requests.get(link)\n",
    "            article_content = article.content\n",
    "            soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "\n",
    "            # get publication datetime\n",
    "            meta = soup_article.find(\"div\", class_=\"meta\").find(\"span\", class_=\"source\").text\n",
    "            strip = meta.replace(' -\\n\\t\\t\\t\\n\\t\\t\\t\\tAssociated Press\\n -\\n                      \\n                        \\n                        ', '')\n",
    "            strip = strip.replace(' -\\n\\t\\t\\t\\n\\t\\t\\t\\tThe Washington Times\\n -\\n                      \\n                        \\n                        ', '')\n",
    "            date = strip.replace('\\n                      \\n                    ', '')\n",
    "            wt_dates.append(date)\n",
    "\n",
    "            # get article content\n",
    "            for div in soup_article.find_all(\"div\", {'class':'article-toplinks'}): \n",
    "                div.decompose()\n",
    "\n",
    "            body = soup_article.find_all('div', class_= 'bigtext')  \n",
    "            x = body[0].find_all('p')\n",
    "\n",
    "            # combine paragraphs\n",
    "            list_paragraphs = []\n",
    "            for p in np.arange(0, len(x)):\n",
    "                paragraph = x[p].get_text()\n",
    "                list_paragraphs.append(paragraph)\n",
    "                final_article = \" \".join(list_paragraphs).split(\"\\n\")[0]\n",
    "\n",
    "            wt_contents.append(final_article)\n",
    "            \n",
    "        # assembling data\n",
    "        wt_data = pd.DataFrame.from_dict({\n",
    "            'publisher': 'washington_times',\n",
    "            'date': wt_dates,\n",
    "            'link': wt_links,\n",
    "            'article_title': wt_titles,\n",
    "            'article_text': wt_contents \n",
    "        })\n",
    "        \n",
    "        # read in old data\n",
    "        #old_wt_data = pd.read_csv('data/wt_data.csv')\n",
    "        #num_old = len(old_wt_data)\n",
    "\n",
    "        # append new data\n",
    "        #wt_data = old_wt_data.append(wt_data).drop_duplicates()\n",
    "\n",
    "        # save new .csv\n",
    "        #wt_data.to_csv(\"data/wt_data.csv\", index = False)\n",
    "        #num_now = len(wt_data)\n",
    "\n",
    "        #print(\"number of entries in old data: {}\".format(num_old))\n",
    "        #print(\"total number of entries in new data: {}\".format(num_now))\n",
    "        \n",
    "    def AP(self):\n",
    "        \"\"\"\n",
    "        Scrapes new articles from apnews.com/apf-politics and saves them to a .csv\n",
    "        \"\"\" \n",
    "        # load the HTML content using requests and save into a variable\n",
    "        ap_requests = requests.get('https://apnews.com/apf-politics')\n",
    "        ap_homepage = ap_requests.content\n",
    "        \n",
    "        # create a soup to allow BeautifulSoup to work\n",
    "        ap_soup = BeautifulSoup(ap_homepage, 'html.parser')\n",
    "        \n",
    "        # locate articles\n",
    "        ap_tags = ap_soup.find_all('a', class_=\"Component-headline-0-2-105\")\n",
    "        \n",
    "        # setup\n",
    "        number_of_articles = min(len(ap_tags), 30)\n",
    "\n",
    "        ap_links = []\n",
    "        ap_text = []\n",
    "        ap_titles = []\n",
    "        ap_dates = []\n",
    "        \n",
    "        # get homepage article links\n",
    "        for link in ap_tags:\n",
    "            link = link.get('href')\n",
    "            link = \"https://apnews.com\" + link\n",
    "            ap_links.append(link)\n",
    "            \n",
    "        # prep for article content\n",
    "        for link in ap_links:\n",
    "            ap_article_request = requests.get(link)\n",
    "            ap_article = ap_article_request.content\n",
    "            ap_article_soup = BeautifulSoup(ap_article, 'html.parser')\n",
    "\n",
    "            # article titles\n",
    "            title = ap_article_soup.find_all('meta')[14]\n",
    "            title = title['content']\n",
    "            ap_titles.append(title)\n",
    "\n",
    "            # article date\n",
    "            date = ap_article_soup.find_all('meta')[24]\n",
    "            date = date['content']\n",
    "            ap_dates.append(date)\n",
    "\n",
    "            # article content: <div class=\"Article\" data-key=Article.\n",
    "            body = ap_article_soup.find_all('div')\n",
    "            x = body[0].find_all('p')\n",
    "\n",
    "            # combine paragraphs\n",
    "            list_paragraphs = []\n",
    "            for p in np.arange(0, len(x)):\n",
    "                paragraph = x[p].get_text()\n",
    "                paragraph = paragraph.replace('\\n',\"\")\n",
    "                paragraph = paragraph.replace('CHICAGO (AP) -',\"\")\n",
    "                paragraph = paragraph.replace('DETROIT (AP) -',\"\")\n",
    "                paragraph = paragraph.replace('WASHINGTON (AP) -',\"\")\n",
    "                paragraph = paragraph.replace('___ Catch up on the 2020 election campaign with AP experts on our weekly politics podcast, “Ground Game.',\"\")\n",
    "                list_paragraphs.append(paragraph)\n",
    "                final_article = \" \".join(list_paragraphs)\n",
    "            ap_text.append(final_article)\n",
    "            \n",
    "        # join ap data\n",
    "        ap_data = pd.DataFrame.from_dict({\n",
    "            'publisher': 'AP',\n",
    "            'date': ap_dates,\n",
    "            'link': ap_links,\n",
    "            'article_title': ap_titles,\n",
    "            'article_text': ap_text \n",
    "        })\n",
    "            \n",
    "        # read in old data\n",
    "        #old_ap_data = pd.read_csv('data/ap_data.csv')\n",
    "        #num_old = len(old_ap_data)\n",
    "\n",
    "        # append new data\n",
    "        #ap_data = old_ap_data.append(ap_data).drop_duplicates()\n",
    "\n",
    "        # save new .csv\n",
    "        #ap_data.to_csv(\"data/ap_data.csv\", index = False)\n",
    "        #num_now = len(ap_data)\n",
    "            \n",
    "        # see number of articles\n",
    "        #print(\"number of entries in old data: {}\".format(num_old))\n",
    "        #print(\"total number of entries in new data: {}\".format(num_now))\n",
    "        #print(\"difference: {}\".format(num_now-num_old))\n",
    "            \n",
    "    def NBC(self):\n",
    "        \"\"\"\n",
    "        Scrapes new articles from nbcnews.com/politics and saves them to a .csv\n",
    "        \"\"\" \n",
    "        # load the HTML content using requests and save into a variable\n",
    "        nbc_request = requests.get('https://www.nbcnews.com/politics')\n",
    "        nbc_homepage = nbc_request.content\n",
    "\n",
    "        # create soup \n",
    "        nbc_soup = BeautifulSoup(nbc_homepage, 'html.parser')\n",
    "\n",
    "        # locate article URLs\n",
    "        nbc_tags = nbc_soup.find_all('h2', class_=\"teaseCard__headline\") + nbc_soup.find_all('h2', class_=\"title___2T5qK\")\n",
    "\n",
    "        # setup\n",
    "        number_of_articles = len(nbc_tags)\n",
    "\n",
    "        # get article titles, content, and links\n",
    "        nbc_links = []\n",
    "        nbc_titles = []\n",
    "        nbc_dates = []\n",
    "        nbc_contents = []\n",
    "\n",
    "        # get article titles, content, and links\n",
    "        for n in np.arange(0, number_of_articles):\n",
    "\n",
    "            # get article link\n",
    "            link = nbc_tags[n].find('a')['href']\n",
    "            nbc_links.append(link)\n",
    "\n",
    "            # get article title\n",
    "            title = nbc_tags[n].find('a').get_text()\n",
    "            nbc_titles.append(title)\n",
    "\n",
    "            # prep article content\n",
    "            article = requests.get(link)\n",
    "            article_content = article.content\n",
    "            soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "\n",
    "            # get publication datetime\n",
    "            if soup_article.time != None:\n",
    "                date = soup_article.time.attrs['datetime']\n",
    "                date = date[4:-24] \n",
    "            else:\n",
    "                date = None\n",
    "            nbc_dates.append(date)\n",
    "\n",
    "            # get article content\n",
    "            body = soup_article.find_all('div', class_= 'article-body__content')    \n",
    "            final_article = \" \".join([item.text for item in body])\n",
    "\n",
    "            nbc_contents.append(final_article)\n",
    "\n",
    "        # assembling data\n",
    "        nbc_data = pd.DataFrame.from_dict({\n",
    "            'publisher': 'nbc',\n",
    "            'date': nbc_dates,\n",
    "            'link': nbc_links,\n",
    "            'article_title': nbc_titles,\n",
    "            'article_text': nbc_contents \n",
    "        })\n",
    "\n",
    "        # dropping rows that are not text articles (these will have NA in date)\n",
    "        nbc_data = nbc_data.dropna()\n",
    "\n",
    "        # read in old data\n",
    "        #old_nbc_data = pd.read_csv('data/nbc_data.csv')\n",
    "        #num_old = len(old_nbc_data)\n",
    "\n",
    "        # append new data\n",
    "        #nbc_data = old_nbc_data.append(nbc_data).drop_duplicates()\n",
    "\n",
    "        # save new .csv\n",
    "        #nbc_data.to_csv(\"data/nbc_data.csv\", index = False)\n",
    "        #num_now = len(nbc_data)\n",
    "\n",
    "        #print(\"number of entries in old data: {}\".format(num_old))\n",
    "        #print(\"total number of entries in new data: {}\".format(num_now))\n",
    "\n",
    "    def NYT(self):\n",
    "        \"\"\"\n",
    "        Scrapes new articles from nytimes.com/section/politics and saves them to a .csv\n",
    "        \"\"\"\n",
    "        # load the HTML content using requests and save into a variable\n",
    "        nyt_request = requests.get('https://www.nytimes.com/section/politics')\n",
    "        nyt_homepage = nyt_request.content\n",
    "\n",
    "        # create soup \n",
    "        nyt_soup = BeautifulSoup(nyt_homepage, 'html.parser')\n",
    "\n",
    "        # homepage URLs\n",
    "        nyt_tags_home = nyt_soup.find_all('h2', class_=\"css-l2vidh e4e4i5l1\")\n",
    "\n",
    "        # archive URLs\n",
    "        nyt_tags_archive = nyt_soup.find_all('div', class_='css-1l4spti')\n",
    "\n",
    "        # setup \n",
    "        nyt_links = []\n",
    "        nyt_titles = []\n",
    "        nyt_dates = []\n",
    "        nyt_contents = []\n",
    "\n",
    "        # homepage articles\n",
    "        for n in np.arange(0, len(nyt_tags_home)):\n",
    "\n",
    "            # get article link\n",
    "            link = nyt_tags_home[n].find('a')['href']\n",
    "            link = \"https://www.nytimes.com\" + link\n",
    "            nyt_links.append(link)\n",
    "\n",
    "            # get article title\n",
    "            title = nyt_tags_home[n].find('a').get_text()\n",
    "            nyt_titles.append(title)\n",
    "\n",
    "            # prep article content\n",
    "            article = requests.get(link)\n",
    "            article_content = article.content\n",
    "            soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "\n",
    "            # get publication datetime\n",
    "            date = soup_article.time.attrs['datetime']\n",
    "            date = date[:-15]\n",
    "            nyt_dates.append(date)\n",
    "\n",
    "            # get article content\n",
    "            for div in soup_article.find_all(\"div\", {'class':'css-9tf9ac'}): \n",
    "                div.decompose()\n",
    "\n",
    "            body = soup_article.find_all('div', {'class':['css-53u6y8', 'css-1fanzo5']})\n",
    "            final_article = \" \".join([item.text for item in body])\n",
    "\n",
    "            nyt_contents.append(final_article)\n",
    "\n",
    "        # archive articles\n",
    "        for n in np.arange(0, len(nyt_tags_archive)):\n",
    "\n",
    "            # get article link\n",
    "            link = nyt_tags_archive[n].find('a')['href']\n",
    "            link = \"https://www.nytimes.com\" + link\n",
    "            nyt_links.append(link)\n",
    "\n",
    "            # get article title\n",
    "            title = nyt_tags_archive[n].find('a').get_text()\n",
    "            nyt_titles.append(title)\n",
    "\n",
    "            # prep article content\n",
    "            article = requests.get(link)\n",
    "            article_content = article.content\n",
    "            soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "\n",
    "            # get publication datetime\n",
    "            date = soup_article.time.attrs['datetime']\n",
    "            date = date[:-15]\n",
    "            nyt_dates.append(date)\n",
    "\n",
    "            # get article content\n",
    "            body = soup_article.find_all('div', attrs = {'class':['css-53u6y8', 'css-1fanzo5 StoryBodyCompanionColumn']})\n",
    "            final_article = \" \".join([item.text for item in body])\n",
    "\n",
    "            nyt_contents.append(final_article)\n",
    "\n",
    "        # assembling data\n",
    "        nyt_data = pd.DataFrame.from_dict({\n",
    "            'publisher': 'new_york_times',\n",
    "            'date': nyt_dates,\n",
    "            'link': nyt_links,\n",
    "            'article_title': nyt_titles,\n",
    "            'article_text': nyt_contents \n",
    "        })\n",
    "\n",
    "        # read in old data\n",
    "        #old_nyt_data = pd.read_csv('data/nyt_data.csv')\n",
    "        #num_old = len(old_nyt_data)\n",
    "\n",
    "        # append new data\n",
    "        #nyt_data = old_nyt_data.append(nyt_data).drop_duplicates()\n",
    "\n",
    "        # save new .csv\n",
    "        #nyt_data.to_csv(\"data/nyt_data.csv\", index = False)\n",
    "        #num_now = len(nyt_data)\n",
    "\n",
    "        #print(\"number of entries in old data: {}\".format(num_old))\n",
    "        #print(\"total number of entries in new data: {}\".format(num_now))\n",
    "\n",
    "    def Politico(self):\n",
    "        \"\"\"\n",
    "        Scrapes new articles from politico.com/politics and saves them to a .csv\n",
    "        \"\"\"\n",
    "        # load the HTML content using requests and save into a variable\n",
    "        politico_request = requests.get('https://www.politico.com/politics')\n",
    "        politico_homepage = politico_request.content\n",
    "\n",
    "        # create soup \n",
    "        politico_soup = BeautifulSoup(politico_homepage, 'html.parser')\n",
    "\n",
    "        # locate article URLs\n",
    "        politico_tags = politico_soup.find_all('h3')\n",
    "\n",
    "        # setup\n",
    "        number_of_articles = len(politico_tags)\n",
    "\n",
    "        # get article titles, content, and links\n",
    "        politico_links = []\n",
    "        politico_titles = []\n",
    "        politico_dates = []\n",
    "        politico_contents = []\n",
    "\n",
    "        # get article titles, content, and links\n",
    "        for n in np.arange(0, number_of_articles):\n",
    "\n",
    "            # get article link\n",
    "            link = politico_tags[n].find('a')['href']\n",
    "            politico_links.append(link)\n",
    "\n",
    "            # get article title\n",
    "            title = politico_tags[n].find('a').get_text()\n",
    "            politico_titles.append(title)\n",
    "\n",
    "            # prep article content\n",
    "            article = requests.get(link)\n",
    "            article_content = article.content\n",
    "            soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "\n",
    "            # get publication datetime\n",
    "            date = soup_article.time.attrs['datetime']\n",
    "            date = date[:-9]\n",
    "            politico_dates.append(date)\n",
    "\n",
    "            # get article content\n",
    "            body = soup_article.find_all('p', attrs={'class':'story-text__paragraph'})\n",
    "            final_article = \" \".join([item.text for item in body])\n",
    "\n",
    "            politico_contents.append(final_article)\n",
    "\n",
    "        # assembling data\n",
    "        politico_data = pd.DataFrame.from_dict({\n",
    "            'publisher': 'politico',\n",
    "            'date': politico_dates,\n",
    "            'link': politico_links,\n",
    "            'article_title': politico_titles,\n",
    "            'article_text': politico_contents \n",
    "        })\n",
    "\n",
    "        # dropping rows that are not text articles (these will have NA in text)\n",
    "        politico_data = politico_data.dropna()\n",
    "\n",
    "        # read in old data\n",
    "        #old_politico_data = pd.read_csv('data/politico_data.csv')\n",
    "        #num_old = len(old_politico_data)\n",
    "\n",
    "        # append new data\n",
    "        #politico_data = old_politico_data.append(politico_data).drop_duplicates()\n",
    "\n",
    "        # save new .csv\n",
    "        #politico_data.to_csv(\"data/politico_data.csv\", index = False)\n",
    "        #num_now = len(politico_data)\n",
    "\n",
    "        #print(\"number of entries in old data: {}\".format(num_old))\n",
    "        #print(\"total number of entries in new data: {}\".format(num_now))\n",
    "            \n",
    "    def Buzzfeed(self):\n",
    "        \"\"\"\n",
    "        Scrapes new articles from buzzfeednews.com/section/politics and saves them to a .csv\n",
    "        \"\"\"\n",
    "        # load the HTML content using requests and save into a variable\n",
    "        buzz_request = requests.get('https://www.buzzfeednews.com/section/politics')\n",
    "        buzz_homepage = buzz_request.content\n",
    "\n",
    "        # create soup \n",
    "        buzz_soup = BeautifulSoup(buzz_homepage, 'html.parser')\n",
    "\n",
    "        # locate article URLs\n",
    "        buzz_tags = buzz_soup.find_all('h2')\n",
    "\n",
    "        # setup\n",
    "        number_of_articles = min(len(buzz_tags), 30)\n",
    "\n",
    "        # get article titles, content, and links\n",
    "        buzz_links = []\n",
    "        buzz_titles = []\n",
    "        buzz_dates = []\n",
    "        buzz_contents = []\n",
    "\n",
    "        # get article titles, content, and links\n",
    "        for n in np.arange(0, number_of_articles):\n",
    "\n",
    "            # get article link\n",
    "            link = buzz_tags[n].find('a')['href']\n",
    "            buzz_links.append(link)\n",
    "\n",
    "            # get article title\n",
    "            title = buzz_tags[n].find('a').get_text()\n",
    "            buzz_titles.append(title)\n",
    "\n",
    "            # prep article content\n",
    "            article = requests.get(link)\n",
    "            article_content = article.content\n",
    "            soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "\n",
    "            # get publication datetime\n",
    "            date = soup_article.find_all('div', class_=\"news-article-header__timestamps\")    \n",
    "            date = \" \".join([item.text for item in date]).replace('\\n', '')\n",
    "            buzz_dates.append(date)\n",
    "\n",
    "            # get article content\n",
    "            body = soup_article.find_all('div', attrs={'data-module':'subbuzz-text'})\n",
    "            article = \" \".join([item.text for item in body]).replace('\\n', '')\n",
    "            final_article = re.sub(r' {[^}]*}', '', article)\n",
    "\n",
    "            buzz_contents.append(final_article)\n",
    "\n",
    "        # assembling data\n",
    "        buzz_data = pd.DataFrame.from_dict({\n",
    "            'publisher': 'buzzfeed',\n",
    "            'date': buzz_dates,\n",
    "            'link': buzz_links,\n",
    "            'article_title': buzz_titles,\n",
    "            'article_text': buzz_contents \n",
    "        })\n",
    "\n",
    "        buzz_data.head()\n",
    "        \n",
    "        # read in old data\n",
    "        #old_buzz_data = pd.read_csv('data/buzzfeed_data.csv')\n",
    "        #num_old = len(old_buzz_data)\n",
    "\n",
    "        ## append new data\n",
    "        #buzz_data = old_buzz_data.append(buzz_data).drop_duplicates()\n",
    "\n",
    "        ## save new .csv\n",
    "        #buzz_data.to_csv(\"data/buzzfeed_data.csv\", index = False)\n",
    "        #num_now = len(buzz_data)\n",
    "\n",
    "        #print(\"number of entries in old data: {}\".format(num_old))\n",
    "        #print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrep():\n",
    "    \"\"\"\n",
    "    Returns a merged dataset, articles about candidates and sentences about candidates from 8 scraped sources.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.candidates = ['Trump', 'Sanders', 'Biden', 'Warren', 'Buttigieg', \n",
    "                           'Bloomberg', 'Klobuchar', 'Yang', 'Steyer', 'Gabbard']\n",
    "        self._full_data = None\n",
    "        self._article_data = None\n",
    "        self._sentence_data = None\n",
    "        \n",
    "    def full_data(self):\n",
    "        \"\"\"\n",
    "        Returns fully merged dataset from articles scraped from 8 different U.S. media outlets\n",
    "        \"\"\"\n",
    "        # load all data sets\n",
    "        breitbart = pd.read_csv('data/breitbart_data.csv')\n",
    "        fox = pd.read_csv('data/fox_data.csv')\n",
    "        wt = pd.read_csv('data/wt_data.csv')\n",
    "        ap = pd.read_csv('data/ap_data.csv')\n",
    "        nbc = pd.read_csv('data/nbc_data.csv')\n",
    "        nyt = pd.read_csv('data/nyt_data.csv')\n",
    "        politico = pd.read_csv('data/politico_data.csv')\n",
    "        buzzfeed = pd.read_csv('data/buzzfeed_data.csv')\n",
    "        \n",
    "        # make dates comparable\n",
    "        fox['date'] = [x.split('T')[0] for x in fox['date']]\n",
    "        wt['date'] = [x.replace(' -\\n\\t\\t\\t\\n\\t\\t\\t\\tAssociated Press\\n -    Updated:', '') for x in wt['date']]\n",
    "        wt['date'] = [x.replace(' -\\n\\t\\t\\t\\n\\t\\t\\t\\tThe Washington Times\\n -    Updated:', '') for x in wt['date']]\n",
    "        wt['date'] = [parse(x) for x in wt['date']]\n",
    "        ap['date'] = [x.split('T')[0] for x in ap['date']]\n",
    "        nbc = nbc.copy().dropna()\n",
    "        nbc['date'] = [parse(x) for x in nbc['date']]\n",
    "        buzzfeed['date'] = [x.replace('Posted on ', '').replace('Last updated on ', '') for x in buzzfeed['date']]\n",
    "        buzzfeed['date'] = [x.strip() for x in buzzfeed['date']]\n",
    "        buzzfeed['date'] = [x.split(',')[0:2] for x in buzzfeed['date']]\n",
    "        buzzfeed['date'] = [''.join(x) for x in buzzfeed['date']]\n",
    "        buzzfeed['date'] = [parse(x) for x in buzzfeed['date']]\n",
    "        \n",
    "        # merge\n",
    "        self._full_data = pd.concat([\n",
    "            breitbart,\n",
    "            fox,\n",
    "            wt,\n",
    "            ap,\n",
    "            nbc,\n",
    "            nyt,\n",
    "            politico,\n",
    "            buzzfeed\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        return self._full_data\n",
    "    \n",
    "    def article_data(self):\n",
    "        \"\"\"\n",
    "        Returns articles that mention various U.S. candidates for president in 2020.\n",
    "        \"\"\"\n",
    "        article_data = self.full_data()\n",
    "\n",
    "        # identify candidates \n",
    "        article_data['Trump'] = pd.np.where(article_data['article_text'].str.contains('Trump'), 1, 0)\n",
    "        article_data['Sanders'] = pd.np.where(article_data['article_text'].str.contains('Bernie'), 1, \n",
    "                                              (np.where(article_data['article_text'].str.contains('Sanders'), 1, 0)))\n",
    "        article_data['Biden'] = pd.np.where(article_data['article_text'].str.contains('Biden'), 1, 0)\n",
    "        article_data['Warren'] = pd.np.where(article_data['article_text'].str.contains('Warren'), 1, 0)\n",
    "        article_data['Buttigieg'] = pd.np.where(article_data['article_text'].str.contains('Buttigieg'), 1, 0)\n",
    "        article_data['Bloomberg'] = pd.np.where(article_data['article_text'].str.contains('Bloomberg'), 1, 0)\n",
    "        article_data['Klobuchar'] = pd.np.where(article_data['article_text'].str.contains('Klobuchar'), 1, 0)\n",
    "        article_data['Yang'] = pd.np.where(article_data['article_text'].str.contains('Yang'), 1, 0)\n",
    "        article_data['Steyer'] = pd.np.where(article_data['article_text'].str.contains('Steyer'), 1, 0)\n",
    "        article_data['Gabbard'] = pd.np.where(article_data['article_text'].str.contains('Gabbard'), 1, 0)\n",
    "        \n",
    "        # limit to only articles where candidate is mentioned\n",
    "        article_data['candidates_mentioned'] = article_data[self.candidates].sum(axis = 1)\n",
    "        article_data = article_data[article_data['candidates_mentioned'] != 0]\n",
    "       \n",
    "        self._article_data = article_data\n",
    "            \n",
    "        return self._article_data\n",
    "    \n",
    "    def save_article_data(self):\n",
    "        \"\"\"\n",
    "        Saves articles that mention various U.S. candidates for president in 2020 as a csv.\n",
    "        \"\"\"\n",
    "        # read in old data\n",
    "        old_data = pd.read_csv('data/article_data.csv')\n",
    "        num_old = len(old_data)\n",
    "\n",
    "        # append new data\n",
    "        article_data = old_data.append(self.article_data()).drop_duplicates()\n",
    "        \n",
    "        # save new .csv\n",
    "        article_data.to_csv(\"data/article_data.csv\", index = False)\n",
    "        num_now = len(article_data)\n",
    "\n",
    "        print(\"number of entries in old data: {}\".format(num_old))\n",
    "        print(\"total number of entries in new data: {}\".format(num_now))\n",
    "        print(\"difference: {}\".format(num_now - num_old))\n",
    "\n",
    "    def sentence_data(self):\n",
    "        \"\"\"\n",
    "        Returns articles that mention various U.S. candidates for president in 2020.\n",
    "        \"\"\"\n",
    "        sentence_data = self.full_data()\n",
    "\n",
    "        # articles to sentences\n",
    "        # create article id #\n",
    "        data_for_sentences = sentence_data[['article_text', 'article_title', 'date', 'link', 'publisher']].copy()\n",
    "        data_for_sentences = data_for_sentences.reset_index()\n",
    "        data_for_sentences = data_for_sentences.reset_index().rename(columns = {'level_0': 'article_id'}).drop(columns = 'index')\n",
    "        \n",
    "        # split article text to sentences\n",
    "        sentences = data_for_sentences['article_text'].copy().str.split('.').apply(pd.Series, 1).stack()\n",
    "\n",
    "        # add correct article id # to each sentence\n",
    "        sentences.index.droplevel(-1) \n",
    "        sentences.name = 'article_text'\n",
    "        sentences = sentences.reset_index().drop(columns = 'level_1').rename(columns = {'level_0': 'article_id'})\n",
    "        \n",
    "        # drop original article text\n",
    "        data_for_sentences = data_for_sentences.drop(columns = 'article_text')\n",
    "        \n",
    "        # merge sentence article text\n",
    "        sentence_data = data_for_sentences.merge(sentences, how='left', on='article_id')\n",
    "        \n",
    "        # clean up\n",
    "        mask = sentence_data['article_text'].astype(str).str.len() < 15\n",
    "        sentence_data.loc[mask, 'article_text'] = ''\n",
    "        sentence_data = sentence_data[(sentence_data['article_text'] != '')]\n",
    "        \n",
    "        # identify candidates \n",
    "        sentence_data['Trump'] = pd.np.where(sentence_data['article_text'].str.contains('Trump'), 1, 0)\n",
    "        sentence_data['Sanders'] = pd.np.where(sentence_data['article_text'].str.contains('Bernie'), 1, \n",
    "                                              (np.where(sentence_data['article_text'].str.contains('Sanders'), 1, 0)))\n",
    "        sentence_data['Biden'] = pd.np.where(sentence_data['article_text'].str.contains('Biden'), 1, 0)\n",
    "        sentence_data['Warren'] = pd.np.where(sentence_data['article_text'].str.contains('Warren'), 1, 0)\n",
    "        sentence_data['Buttigieg'] = pd.np.where(sentence_data['article_text'].str.contains('Buttigieg'), 1, 0)\n",
    "        sentence_data['Bloomberg'] = pd.np.where(sentence_data['article_text'].str.contains('Bloomberg'), 1, 0)\n",
    "        sentence_data['Klobuchar'] = pd.np.where(sentence_data['article_text'].str.contains('Klobuchar'), 1, 0)\n",
    "        sentence_data['Yang'] = pd.np.where(sentence_data['article_text'].str.contains('Yang'), 1, 0)\n",
    "        sentence_data['Steyer'] = pd.np.where(sentence_data['article_text'].str.contains('Steyer'), 1, 0)\n",
    "        sentence_data['Gabbard'] = pd.np.where(sentence_data['article_text'].str.contains('Gabbard'), 1, 0)\n",
    "        \n",
    "        # limit to only articles where candidate is mentioned\n",
    "        sentence_data['candidates_mentioned'] = sentence_data[self.candidates].sum(axis = 1)\n",
    "        sentence_data = sentence_data[sentence_data['candidates_mentioned'] != 0]\n",
    "       \n",
    "        self._sentence_data = sentence_data\n",
    "            \n",
    "        return self._sentence_data\n",
    "    \n",
    "    def save_sentence_data(self):\n",
    "        \"\"\"\n",
    "        Saves sentences that mention various U.S. candidates for president in 2020 as a csv.\n",
    "        \"\"\"\n",
    "        # read in old data\n",
    "        old_data = pd.read_csv('data/sentence_data.csv')\n",
    "        num_old = len(old_data)\n",
    "\n",
    "        # append new data\n",
    "        sentence_data = old_data.append(self.sentence_data()).drop_duplicates()\n",
    "        \n",
    "        # save new .csv\n",
    "        sentence_data.to_csv(\"data/sentence_data.csv\", index = False)\n",
    "        num_now = len(sentence_data)\n",
    "\n",
    "        print(\"number of entries in old data: {}\".format(num_old))\n",
    "        print(\"total number of entries in new data: {}\".format(num_now))\n",
    "        print(\"difference: {}\".format(num_now - num_old))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
