{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.dates import DateFormatter\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_sentence_data = pd.read_csv('data/sentence_data.csv')\n",
    "        \n",
    "# setup\n",
    "sentences = self_sentence_data['article_text']\n",
    "\n",
    "# lowercase everything\n",
    "sentences = [sentences.lower() for sentences in sentences]\n",
    "\n",
    "# remove punctuation\n",
    "sentences = [s.replace(\"â€™s\",'') for s in sentences] # remove apostrophe s first\n",
    "sentences = [re.sub(r'[^\\w\\s]','',s) for s in sentences]\n",
    "\n",
    "# remove numbers\n",
    "sentences = [re.sub('[0-9]','', s) for s in sentences]\n",
    "\n",
    "# remove double space\n",
    "sentences = [s.replace(\"  \",' ') for s in sentences]\n",
    "\n",
    "# remove stopwords\n",
    "clean = []\n",
    "for item in sentences:\n",
    "    for word in stopwords.words('english'):\n",
    "        item = item.replace(\" \" + word + \" \", ' ')\n",
    "    clean.append(item)\n",
    "\n",
    "self_sentence_data['article_text_clean'] = clean\n",
    "\n",
    "# SENTIMENT ANALYSIS\n",
    "# setup\n",
    "data = self_sentence_data\n",
    "text = self_sentence_data['article_text_clean']\n",
    "score = []\n",
    "        \n",
    "for sentence in text:\n",
    "    sentence = TextBlob(sentence)\n",
    "    x = sentence.sentiment\n",
    "    x = sentence.sentiment.polarity\n",
    "    score.append(x)\n",
    "\n",
    "data['score'] = score\n",
    "\n",
    "# Convert float score to category based on binning to get 5 levels\n",
    "data['sentiment'] = pd.cut(data['score'],\n",
    "                    bins=5,\n",
    "                    labels=[1, 2, 3, 4, 5])\n",
    "data['sentiment'] = pd.to_numeric(data['sentiment'])\n",
    "data = data.drop('score', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Candidate Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sanders mean\", sanders['sentiment'].mean())\n",
    "print(\"Biden mean\", biden['sentiment'].mean())\n",
    "print(\"Both mean\", combined['sentiment'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "# filter sentences only about one candidate\n",
    "\n",
    "#sanders\n",
    "sanders = data.loc[data['Sanders'] == 1]\n",
    "biden = data.loc[data['Biden'] == 1]      \n",
    "combined = pd.concat([sanders, biden], axis=0)\n",
    "        \n",
    "#candidate_sentiment = data.loc[data['candidates_mentioned'] == 1]\n",
    "candidate_sentiment = combined\n",
    "\n",
    "candidates = ['Sanders', 'Biden']\n",
    "# create new column with candidate name\n",
    "candidate_sentiment['candidate'] = candidate_sentiment['article_text'].str.extract('({})'.format('|'.join(candidates)),\n",
    "                                            flags = re.IGNORECASE, expand = False).str.lower().fillna('')\n",
    "candidate_sentiment['candidate'] = np.where(candidate_sentiment['article_text'].str.contains('bernie'), 'sanders', candidate_sentiment['candidate'])\n",
    "candidate_sentiment = candidate_sentiment[['date', 'sentiment', 'candidate']]\n",
    "candidate_sentiment = candidate_sentiment[candidate_sentiment.candidate != '']\n",
    "\n",
    "# make dates consistent and filter for time frame\n",
    "candidate_sentiment['date'] = pd.to_datetime(candidate_sentiment['date'], errors='coerce')\n",
    "mask = (candidate_sentiment['date'].astype('str') >= \"2020-03-01\") & (candidate_sentiment['date'].astype('str') < \"2020-04-19\")\n",
    "candidate_sentiment = candidate_sentiment.loc[mask]\n",
    "\n",
    "# mean sentiment per day\n",
    "mean_per_day = candidate_sentiment.groupby(['date', 'candidate']).mean()\n",
    "mean_per_day.reset_index(inplace = True)\n",
    "                 \n",
    "self_sentiment_time = mean_per_day\n",
    "      \n",
    "chart = sns.lineplot(x = 'date', y = 'sentiment', hue = 'candidate', palette=\"PuBu\", data = self_sentiment_time)\n",
    "plt.setp(chart.get_xticklabels(), rotation = 45)\n",
    "plt.setp(chart.get_xticklabels(), rotation = 45)\n",
    "plt.title('Average Sentiment Over Time')\n",
    "plt.title('Average Sentiment Over Time')\n",
    "chart.legend(loc='center right', bbox_to_anchor=(1.3, 0.5), ncol=1)\n",
    "plt.savefig('sentiment_time.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bernie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanders = data.loc[data['Sanders'] == 1]\n",
    "\n",
    "# AP sentiment\n",
    "AP = sanders.loc[sanders['publisher'] == \"AP\"]\n",
    "AP_sent = AP['sentiment'].sort_values(ascending=True)\n",
    "# get sentiment counts\n",
    "AP_sent_count = AP_sent.value_counts().sort_index()\n",
    "# get percent per sentiment category\n",
    "AP_1 = AP_sent_count[1]/len(AP_sent)\n",
    "AP_2 = AP_sent_count[2]/len(AP_sent)\n",
    "AP_3 = AP_sent_count[3]/len(AP_sent)\n",
    "AP_4 = AP_sent_count[4]/len(AP_sent)\n",
    "AP_5 = AP_sent_count[5]/len(AP_sent)\n",
    "\n",
    "# Breitbart sentiment\n",
    "Breitbart = sanders.loc[sanders['publisher'] == \"Breitbart\"]\n",
    "Breitbart_sent = Breitbart['sentiment'].sort_values(ascending=True)\n",
    "# get sentiment counts\n",
    "Breitbart_sent_count = Breitbart_sent.value_counts().sort_index()\n",
    "# get percent per sentiment category\n",
    "Breitbart_1 = Breitbart_sent_count[1]/len(Breitbart_sent)\n",
    "Breitbart_2 = Breitbart_sent_count[2]/len(Breitbart_sent)\n",
    "Breitbart_3 = Breitbart_sent_count[3]/len(Breitbart_sent)\n",
    "Breitbart_4 = Breitbart_sent_count[4]/len(Breitbart_sent)\n",
    "Breitbart_5 = Breitbart_sent_count[5]/len(Breitbart_sent)\n",
    "\n",
    "# Fox sentiment\n",
    "Fox = sanders.loc[sanders['publisher'] == \"Fox\"]\n",
    "Fox_sent = Fox['sentiment'].sort_values(ascending=True)\n",
    "# get sentiment counts\n",
    "Fox_sent_count = Fox_sent.value_counts().sort_index()\n",
    "# get percent per sentiment category\n",
    "Fox_1 = Fox_sent_count[1]/len(Fox_sent)\n",
    "Fox_2 = Fox_sent_count[2]/len(Fox_sent)\n",
    "Fox_3 = Fox_sent_count[3]/len(Fox_sent)\n",
    "Fox_4 = Fox_sent_count[4]/len(Fox_sent)\n",
    "Fox_5 = Fox_sent_count[5]/len(Fox_sent)\n",
    "\n",
    "# Buzzfeed sentiment\n",
    "buzzfeed = sanders.loc[sanders['publisher'] == \"buzzfeed\"]\n",
    "buzzfeed_sent = buzzfeed['sentiment'].sort_values(ascending=True)\n",
    "# get sentiment counts\n",
    "buzzfeed_sent_count = buzzfeed_sent.value_counts().sort_index()\n",
    "# get percent per sentiment category\n",
    "buzzfeed_1 = buzzfeed_sent_count[1]/len(buzzfeed_sent)\n",
    "buzzfeed_2 = buzzfeed_sent_count[2]/len(buzzfeed_sent)\n",
    "buzzfeed_3 = buzzfeed_sent_count[3]/len(buzzfeed_sent)\n",
    "buzzfeed_4 = buzzfeed_sent_count[4]/len(buzzfeed_sent)\n",
    "buzzfeed_5 = buzzfeed_sent_count[5]/len(buzzfeed_sent)\n",
    "\n",
    "# NBC\n",
    "nbc = sanders.loc[sanders['publisher'] == \"nbc\"]\n",
    "nbc_sent = nbc['sentiment'].sort_values(ascending=True)\n",
    "# get sentiment counts\n",
    "nbc_sent_count = nbc_sent.value_counts().sort_index()\n",
    "# get percent per sentiment category\n",
    "nbc_1 = nbc_sent_count[1]/len(nbc_sent)\n",
    "nbc_2 = nbc_sent_count[2]/len(nbc_sent)\n",
    "nbc_3 = nbc_sent_count[3]/len(nbc_sent)\n",
    "nbc_4 = nbc_sent_count[4]/len(nbc_sent)\n",
    "nbc_5 = nbc_sent_count[5]/len(nbc_sent)\n",
    "\n",
    "# New York Times\n",
    "new_york_times = sanders.loc[sanders['publisher'] == \"new_york_times\"]\n",
    "new_york_times_sent = new_york_times['sentiment'].sort_values(ascending=True)\n",
    "# get sentiment counts\n",
    "new_york_times_sent_count = new_york_times_sent.value_counts().sort_index()\n",
    "# get percent per sentiment category\n",
    "new_york_times_1 = new_york_times_sent_count[1]/len(new_york_times_sent)\n",
    "new_york_times_2 = new_york_times_sent_count[2]/len(new_york_times_sent)\n",
    "new_york_times_3 = new_york_times_sent_count[3]/len(new_york_times_sent)\n",
    "new_york_times_4 = new_york_times_sent_count[4]/len(new_york_times_sent)\n",
    "new_york_times_5 = new_york_times_sent_count[5]/len(new_york_times_sent)\n",
    "\n",
    "# Politico\n",
    "politico = sanders.loc[sanders['publisher'] == \"politico\"]\n",
    "politico_sent = politico['sentiment'].sort_values(ascending=True)\n",
    "# get sentiment counts\n",
    "politico_sent_count = politico_sent.value_counts().sort_index()\n",
    "# get percent per sentiment category\n",
    "politico_1 = politico_sent_count[1]/len(politico_sent)\n",
    "politico_2 = politico_sent_count[2]/len(politico_sent)\n",
    "politico_3 = politico_sent_count[3]/len(politico_sent)\n",
    "politico_4 = politico_sent_count[4]/len(politico_sent)\n",
    "politico_5 = politico_sent_count[5]/len(politico_sent)\n",
    "\n",
    "# Washington Times\n",
    "washington_times = sanders.loc[sanders['publisher'] == \"washington_times\"]\n",
    "washington_times_sent = washington_times['sentiment'].sort_values(ascending=True)\n",
    "# get sentiment counts\n",
    "washington_times_sent_count = washington_times_sent.value_counts().sort_index()\n",
    "# get percent per sentiment category\n",
    "washington_times_1 = washington_times_sent_count[1]/len(washington_times_sent)\n",
    "washington_times_2 = washington_times_sent_count[2]/len(washington_times_sent)\n",
    "washington_times_3 = washington_times_sent_count[3]/len(washington_times_sent)\n",
    "washington_times_4 = washington_times_sent_count[4]/len(washington_times_sent)\n",
    "washington_times_5 = washington_times_sent_count[5]/len(washington_times_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Extremely negative', 'Negative','Neutral', 'Positive', 'Extremely positive']\n",
    "publishers = {\n",
    "    'AP': [AP_1, AP_2, AP_3, AP_4, AP_5],\n",
    "    'Breitbart': [Breitbart_1, Breitbart_2, Breitbart_3, Breitbart_4, Breitbart_5],\n",
    "    'Fox': [Fox_1, Fox_2, Fox_3, Fox_4, Fox_5],\n",
    "    'Buzzfeed': [buzzfeed_1, buzzfeed_2, buzzfeed_3, buzzfeed_4, buzzfeed_5],\n",
    "    'NBC': [nbc_1, nbc_2, nbc_3, nbc_4, nbc_5],\n",
    "    'New York Times': [new_york_times_1, new_york_times_2, new_york_times_3, new_york_times_4, new_york_times_5],\n",
    "    'Politico': [politico_1, politico_2, politico_3, politico_4, politico_5],\n",
    "    'Washington Times': [washington_times_1, washington_times_2, washington_times_3, washington_times_4, washington_times_5]\n",
    "}\n",
    "\n",
    "def survey(publishers, category_names):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        A mapping from question labels to a list of answers per category.\n",
    "        It is assumed all lists contain the same number of entries and that\n",
    "        it matches the length of *category_names*.\n",
    "    category_names : list of str\n",
    "        The category labels.\n",
    "    \"\"\"\n",
    "    labels = list(publishers.keys())\n",
    "    data = np.array(list(publishers.values()))\n",
    "    data_cum = data.cumsum(axis=1)\n",
    "    category_colors = plt.get_cmap('RdYlGn')(\n",
    "        np.linspace(0.15, 0.85, data.shape[1]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_xlim(0, np.sum(data, axis=1).max())\n",
    "\n",
    "    for i, (colname, color) in enumerate(zip(category_names, category_colors)):\n",
    "        widths = data[:, i]\n",
    "        starts = data_cum[:, i] - widths\n",
    "        ax.barh(labels, widths, left=starts, height=0.5,\n",
    "                label=colname, color=color)\n",
    "        xcenters = starts + widths / 2\n",
    "\n",
    "        r, g, b, _ = color\n",
    "        text_color = 'white' if r * g * b < 0.5 else 'darkgrey'\n",
    "        for y, (x, c) in enumerate(zip(xcenters, widths)):\n",
    "            ax.text(x, y, str(\"{0:.0%}\".format(c)), ha='center', va='center',\n",
    "                    color=text_color)\n",
    "    ax.legend(ncol=len(category_names), bbox_to_anchor=(0, 1),\n",
    "              loc='lower left', fontsize='small')\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "survey(publishers, category_names)\n",
    "plt.savefig('sentiment_count_sanders.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Biden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden = data.loc[data['Biden'] == 1]\n",
    "\n",
    "\n",
    "# AP sentiment\n",
    "AP = biden.loc[biden['publisher'] == \"AP\"]\n",
    "AP_sent = AP['sentiment'].sort_values(ascending=True)\n",
    "# get sentiment counts\n",
    "AP_sent_count = AP_sent.value_counts().sort_index()\n",
    "# get percent per sentiment category\n",
    "AP_1 = AP_sent_count[1]/len(AP_sent)\n",
    "AP_2 = AP_sent_count[2]/len(AP_sent)\n",
    "AP_3 = AP_sent_count[3]/len(AP_sent)\n",
    "AP_4 = AP_sent_count[4]/len(AP_sent)\n",
    "AP_5 = AP_sent_count[5]/len(AP_sent)\n",
    "\n",
    "# Breitbart sentiment\n",
    "Breitbart = biden.loc[biden['publisher'] == \"Breitbart\"]\n",
    "Breitbart_sent = Breitbart['sentiment'].sort_values(ascending=True)\n",
    "# get sentiment counts\n",
    "Breitbart_sent_count = Breitbart_sent.value_counts().sort_index()\n",
    "# get percent per sentiment category\n",
    "Breitbart_1 = Breitbart_sent_count[1]/len(Breitbart_sent)\n",
    "Breitbart_2 = Breitbart_sent_count[2]/len(Breitbart_sent)\n",
    "Breitbart_3 = Breitbart_sent_count[3]/len(Breitbart_sent)\n",
    "Breitbart_4 = Breitbart_sent_count[4]/len(Breitbart_sent)\n",
    "Breitbart_5 = Breitbart_sent_count[5]/len(Breitbart_sent)\n",
    "\n",
    "# Fox sentiment\n",
    "Fox = biden.loc[biden['publisher'] == \"Fox\"]\n",
    "Fox_sent = Fox['sentiment'].sort_values(ascending=True)\n",
    "# get sentiment counts\n",
    "Fox_sent_count = Fox_sent.value_counts().sort_index()\n",
    "# get percent per sentiment category\n",
    "Fox_1 = Fox_sent_count[1]/len(Fox_sent)\n",
    "Fox_2 = Fox_sent_count[2]/len(Fox_sent)\n",
    "Fox_3 = Fox_sent_count[3]/len(Fox_sent)\n",
    "Fox_4 = Fox_sent_count[4]/len(Fox_sent)\n",
    "Fox_5 = Fox_sent_count[5]/len(Fox_sent)\n",
    "\n",
    "# Buzzfeed sentiment\n",
    "buzzfeed = biden.loc[biden['publisher'] == \"buzzfeed\"]\n",
    "buzzfeed_sent = buzzfeed['sentiment'].sort_values(ascending=True)\n",
    "# get sentiment counts\n",
    "buzzfeed_sent_count = buzzfeed_sent.value_counts().sort_index()\n",
    "# get percent per sentiment category\n",
    "buzzfeed_1 = buzzfeed_sent_count[1]/len(buzzfeed_sent)\n",
    "buzzfeed_2 = buzzfeed_sent_count[2]/len(buzzfeed_sent)\n",
    "buzzfeed_3 = buzzfeed_sent_count[3]/len(buzzfeed_sent)\n",
    "buzzfeed_4 = buzzfeed_sent_count[4]/len(buzzfeed_sent)\n",
    "buzzfeed_5 = buzzfeed_sent_count[5]/len(buzzfeed_sent)\n",
    "\n",
    "# NBC\n",
    "nbc = biden.loc[biden['publisher'] == \"nbc\"]\n",
    "nbc_sent = nbc['sentiment'].sort_values(ascending=True)\n",
    "# get sentiment counts\n",
    "nbc_sent_count = nbc_sent.value_counts().sort_index()\n",
    "# get percent per sentiment category\n",
    "nbc_1 = nbc_sent_count[1]/len(nbc_sent)\n",
    "nbc_2 = nbc_sent_count[2]/len(nbc_sent)\n",
    "nbc_3 = nbc_sent_count[3]/len(nbc_sent)\n",
    "nbc_4 = nbc_sent_count[4]/len(nbc_sent)\n",
    "nbc_5 = nbc_sent_count[5]/len(nbc_sent)\n",
    "\n",
    "# New York Times\n",
    "new_york_times = biden.loc[biden['publisher'] == \"new_york_times\"]\n",
    "new_york_times_sent = new_york_times['sentiment'].sort_values(ascending=True)\n",
    "# get sentiment counts\n",
    "new_york_times_sent_count = new_york_times_sent.value_counts().sort_index()\n",
    "# get percent per sentiment category\n",
    "new_york_times_1 = new_york_times_sent_count[1]/len(new_york_times_sent)\n",
    "new_york_times_2 = new_york_times_sent_count[2]/len(new_york_times_sent)\n",
    "new_york_times_3 = new_york_times_sent_count[3]/len(new_york_times_sent)\n",
    "new_york_times_4 = new_york_times_sent_count[4]/len(new_york_times_sent)\n",
    "new_york_times_5 = new_york_times_sent_count[5]/len(new_york_times_sent)\n",
    "\n",
    "# Politico\n",
    "politico = biden.loc[biden['publisher'] == \"politico\"]\n",
    "politico_sent = politico['sentiment'].sort_values(ascending=True)\n",
    "# get sentiment counts\n",
    "politico_sent_count = politico_sent.value_counts().sort_index()\n",
    "# get percent per sentiment category\n",
    "politico_1 = politico_sent_count[1]/len(politico_sent)\n",
    "politico_2 = politico_sent_count[2]/len(politico_sent)\n",
    "politico_3 = politico_sent_count[3]/len(politico_sent)\n",
    "politico_4 = politico_sent_count[4]/len(politico_sent)\n",
    "politico_5 = politico_sent_count[5]/len(politico_sent)\n",
    "\n",
    "# Washington Times\n",
    "washington_times = biden.loc[biden['publisher'] == \"washington_times\"]\n",
    "washington_times_sent = washington_times['sentiment'].sort_values(ascending=True)\n",
    "# get sentiment counts\n",
    "washington_times_sent_count = washington_times_sent.value_counts().sort_index()\n",
    "# get percent per sentiment category\n",
    "washington_times_1 = washington_times_sent_count[1]/len(washington_times_sent)\n",
    "washington_times_2 = washington_times_sent_count[2]/len(washington_times_sent)\n",
    "washington_times_3 = washington_times_sent_count[3]/len(washington_times_sent)\n",
    "washington_times_4 = washington_times_sent_count[4]/len(washington_times_sent)\n",
    "washington_times_5 = washington_times_sent_count[5]/len(washington_times_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Extremely negative', 'Negative','Neutral', 'Positive', 'Extremely positive']\n",
    "publishers = {\n",
    "    'AP': [AP_1, AP_2, AP_3, AP_4, AP_5],\n",
    "    'Breitbart': [Breitbart_1, Breitbart_2, Breitbart_3, Breitbart_4, Breitbart_5],\n",
    "    'Fox': [Fox_1, Fox_2, Fox_3, Fox_4, Fox_5],\n",
    "    'Buzzfeed': [buzzfeed_1, buzzfeed_2, buzzfeed_3, buzzfeed_4, buzzfeed_5],\n",
    "    'NBC': [nbc_1, nbc_2, nbc_3, nbc_4, nbc_5],\n",
    "    'New York Times': [new_york_times_1, new_york_times_2, new_york_times_3, new_york_times_4, new_york_times_5],\n",
    "    'Politico': [politico_1, politico_2, politico_3, politico_4, politico_5],\n",
    "    'Washington Times': [washington_times_1, washington_times_2, washington_times_3, washington_times_4, washington_times_5]\n",
    "}\n",
    "\n",
    "def survey(publishers, category_names):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        A mapping from question labels to a list of answers per category.\n",
    "        It is assumed all lists contain the same number of entries and that\n",
    "        it matches the length of *category_names*.\n",
    "    category_names : list of str\n",
    "        The category labels.\n",
    "    \"\"\"\n",
    "    labels = list(publishers.keys())\n",
    "    data = np.array(list(publishers.values()))\n",
    "    data_cum = data.cumsum(axis=1)\n",
    "    category_colors = plt.get_cmap('RdYlGn')(\n",
    "        np.linspace(0.15, 0.85, data.shape[1]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_xlim(0, np.sum(data, axis=1).max())\n",
    "\n",
    "    for i, (colname, color) in enumerate(zip(category_names, category_colors)):\n",
    "        widths = data[:, i]\n",
    "        starts = data_cum[:, i] - widths\n",
    "        ax.barh(labels, widths, left=starts, height=0.5,\n",
    "                label=colname, color=color)\n",
    "        xcenters = starts + widths / 2\n",
    "\n",
    "        r, g, b, _ = color\n",
    "        text_color = 'white' if r * g * b < 0.5 else 'darkgrey'\n",
    "        for y, (x, c) in enumerate(zip(xcenters, widths)):\n",
    "            ax.text(x, y, str(\"{0:.0%}\".format(c)), ha='center', va='center',\n",
    "                    color=text_color)\n",
    "    ax.legend(ncol=len(category_names), bbox_to_anchor=(0, 1),\n",
    "              loc='lower left', fontsize='small')\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "survey(publishers, category_names)\n",
    "plt.savefig('sentiment_count_biden.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
