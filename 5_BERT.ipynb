{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import transformers \n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT():\n",
    "    \"\"\"\n",
    "    Reads in sentence-level data. \n",
    "    Filters for sentences related to Biden or Sanders. \n",
    "    Removes the candidate names from the text and moves them to a new label column.\n",
    "    Applies BERT embeddings \n",
    "    return: BERT embedded train, test, and val datasets, as train, test, and val labels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.candidates = ['Trump', 'Bernie', 'Sanders', 'Biden', 'Warren', 'Buttigieg', 'Bloomberg', \n",
    "                           'Klobuchar', 'Yang', 'Steyer', 'Gabbard']\n",
    "        self._data = pd.read_csv('data/sentence_data.csv')[['article_text']]\n",
    "        self._prepped_data = None\n",
    "\n",
    "    def data(self):\n",
    "        \"\"\"\n",
    "        Removes candidate names from the text and uses names to create a label column.\n",
    "        Filters for Sanders and Biden.\n",
    "        return: pd.DataFrame\n",
    "        \"\"\"\n",
    "        data = self._data.copy()\n",
    "        data = data.loc[data['candidates_mentioned'] == 1][['article_text']]\n",
    "        \n",
    "        # add labels\n",
    "        data['label'] = data['article_text'].str.extract('({})'.format('|'.join(self.candidates)), \n",
    "                            flags = re.IGNORECASE, expand = False).str.lower().fillna('')\n",
    "        data['label'] = np.where(data['label'].str.contains('bernie'), 'sanders', data['label'])\n",
    "\n",
    "        # filter data set\n",
    "        data = data.loc[data['label'].isin(['biden', 'sanders'])]\n",
    "\n",
    "        # remove candidate names\n",
    "        data['article_text'] = data['article_text'].str.replace('Bernie', '[candidate]')\n",
    "        data['article_text'] = data['article_text'].str.replace('Bernard', '[candidate]')\n",
    "        data['article_text'] = data['article_text'].str.replace('Sanders', '[candidate]')\n",
    "        data['article_text'] = data['article_text'].str.replace('Senator', '[candidate]')\n",
    "        data['article_text'] = data['article_text'].str.replace('Joe', '[candidate]')\n",
    "        data['article_text'] = data['article_text'].str.replace('Biden', '[candidate]')\n",
    "        data['article_text'] = data['article_text'].str.replace('Vice President', '[candidate]')\n",
    "        \n",
    "        data = data.drop_duplicates()\n",
    "        \n",
    "        # label to numeric\n",
    "        data['label'] = np.where(data['label'] == \"sanders\", 1, 0)\n",
    "\n",
    "        self._data = data\n",
    "\n",
    "        return self._data\n",
    "    \n",
    "    def prepped_data(self):\n",
    "        \"\"\"\n",
    "        Prepares the article text for the BERT embeddings\n",
    "        return: pd.DataFrame\n",
    "        \"\"\"\n",
    "        data = self.data()\n",
    "        \n",
    "        sentences = data['article_text']\n",
    "\n",
    "        # lowercase everything\n",
    "        sentences = [sentences.lower() for sentences in sentences]\n",
    "\n",
    "        # remove punctuation\n",
    "        sentences = [re.sub(r'[^\\w\\s]','',s) for s in sentences]\n",
    "\n",
    "        # remove numbers\n",
    "        sentences = [re.sub('[0-9]','', s) for s in sentences]\n",
    "\n",
    "        # remove stopwords\n",
    "        clean = []\n",
    "        for item in sentences:\n",
    "            for word in stopwords.words('english'):\n",
    "                item = item.replace(\" \" + word + \" \", ' ')\n",
    "            clean.append(item)\n",
    "\n",
    "        data['article_text'] = clean\n",
    "        \n",
    "        self._prepped_data = data\n",
    "        \n",
    "        return self._prepped_data\n",
    "    \n",
    "    def train_test_split(self):\n",
    "        \"\"\"\n",
    "        Selects a random sample of the prepped data for the train, val, and test datasets\n",
    "        return: pd.DataFrame\n",
    "        \"\"\"\n",
    "        # shuffle\n",
    "        np.random.seed(42)\n",
    "        data = self.prepped_data().sample(frac = 1)\n",
    "        data = data.reset_index(drop = True)\n",
    "\n",
    "        # small sample for training and reshuffle again\n",
    "        sample_data = data.groupby('label').apply(pd.DataFrame.sample, n = 10000, replace = True)\n",
    "        sample_data = sample_data.sample(frac = 1)\n",
    "        sample_data = sample_data.reset_index(drop = True)\n",
    "        \n",
    "        # train test split\n",
    "        df_train = sample_data[:7000].reset_index(drop=True)\n",
    "        df_val = sample_data[7000:9000].reset_index(drop=True)\n",
    "        df_test = sample_data[9000:10000].reset_index(drop=True)\n",
    "        \n",
    "        return df_train, df_val, df_test\n",
    "        \n",
    "\n",
    "    def bert_model(self):\n",
    "        \"\"\"\n",
    "        Creates the BERT model to apply BERT embeddings.\n",
    "        return: BERT embedding datasets\n",
    "        \"\"\"\n",
    "        # import model and tokenizer\n",
    "        model_class = transformers.BertModel\n",
    "        tokenizer_class = transformers.BertTokenizer\n",
    "        pretrained_weights = 'bert-base-uncased'\n",
    "\n",
    "        # Load pretrained model/tokenizer\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "        bert_model = model_class.from_pretrained(pretrained_weights)\n",
    "        \n",
    "        # grab data\n",
    "        df_train, df_val, df_test = self.train_test_split()\n",
    "        \n",
    "        # max seq. is longest sentence in characters\n",
    "        max_seq = max( \n",
    "            df_train['article_text'].str.len().max(),\n",
    "            df_val['article_text'].str.len().max(),\n",
    "            df_test['article_text'].str.len().max()\n",
    "        )\n",
    "        \n",
    "        # transform each sentence into a 2D matrix\n",
    "        \n",
    "        def tokenize_text(df, max_seq):\n",
    "            return [tokenizer.encode(text, add_special_tokens=True)[:max_seq] for text in df.article_text.values]\n",
    "\n",
    "        def pad_text(tokenized_text, max_seq):\n",
    "            return np.array([el + [0] * (max_seq - len(el)) for el in tokenized_text])\n",
    "\n",
    "        def tokenize_and_pad_text(df, max_seq):\n",
    "            tokenized_text = tokenize_text(df, max_seq)\n",
    "            padded_text = pad_text(tokenized_text, max_seq)\n",
    "            return torch.tensor(padded_text)\n",
    "\n",
    "        def targets_to_tensor(df, target_columns):\n",
    "            return torch.tensor(df[target_columns].values, dtype=torch.float32)\n",
    "        \n",
    "        # tokenize and pad text\n",
    "        train_indices = tokenize_and_pad_text(df_train, max_seq)\n",
    "        val_indices = tokenize_and_pad_text(df_val, max_seq)\n",
    "        test_indices = tokenize_and_pad_text(df_test, max_seq)\n",
    "        \n",
    "        # create BERT embeddings for features\n",
    "        with torch.no_grad():\n",
    "            x_train = bert_model(train_indices)[0] \n",
    "            x_val = bert_model(val_indices)[0] \n",
    "            x_test = bert_model(test_indices)[0]\n",
    "                    \n",
    "        # transform labels into tensors\n",
    "        target_columns = \"label\"\n",
    "        y_train = targets_to_tensor(df_train, target_columns)\n",
    "        y_val = targets_to_tensor(df_val, target_columns)\n",
    "        y_test = targets_to_tensor(df_test, target_columns)\n",
    "        \n",
    "        return x_train, x_val, x_test, y_train, y_val, y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
