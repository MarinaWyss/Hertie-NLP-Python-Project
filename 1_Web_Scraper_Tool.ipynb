{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper Tool for US Media Outlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import UnicodeDammit\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Breitbart - Very Conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "breitbart_request = requests.get('https://www.breitbart.com/politics/')\n",
    "breitbart_homepage = breitbart_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "breitbart_soup = BeautifulSoup(breitbart_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# locate article URLs\n",
    "breitbart_tags = breitbart_soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "number_of_articles = min(len(breitbart_tags), 30)\n",
    "\n",
    "breitbart_links = []\n",
    "breitbart_titles = []\n",
    "breitbart_dates = []\n",
    "breitbart_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get article titles, content, and links\n",
    "for n in np.arange(0, number_of_articles):\n",
    "\n",
    "    # get article link\n",
    "    link = breitbart_tags[n].find('a')['href']\n",
    "    link = \"https://www.breitbart.com\" + link\n",
    "    breitbart_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = breitbart_tags[n].find('a').get_text()\n",
    "    breitbart_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.time.attrs['datetime']\n",
    "    date = date[:-10]\n",
    "    breitbart_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', class_='entry-content')\n",
    "    x = body[0].find_all('p')\n",
    "    \n",
    "    # combine paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    breitbart_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling data\n",
    "breitbart_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'Breitbart',\n",
    "    'date': breitbart_dates,\n",
    "    'link': breitbart_links,\n",
    "    'article_title': breitbart_titles,\n",
    "    'article_text': breitbart_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Friday, Rep. Matt Gaetz (R-FL) applauded Presi...</td>\n",
       "      <td>Gaetz Slams Dems for Using Coronavirus Threat ...</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.breitbart.com/clips/2020/03/14/gae...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nigel Farage has said that the actions of gove...</td>\n",
       "      <td>Farage on Coronavirus: ‘We Are All Nationalist...</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.breitbart.com/europe/2020/03/14/fa...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The European Court of Human Rights (ECHR) has ...</td>\n",
       "      <td>European Human Rights Court Rules Against Chri...</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.breitbart.com/europe/2020/03/14/eu...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UNITED NATIONS (AP) – Allegations of sexual ab...</td>\n",
       "      <td>Sexual Abuse Allegations Against UN Civilian S...</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.breitbart.com/europe/2020/03/14/se...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish Public Health Authority has decide...</td>\n",
       "      <td>Sweden to Stop Publishing Coronavirus Numbers,...</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.breitbart.com/europe/2020/03/14/sw...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  \\\n",
       "0  Friday, Rep. Matt Gaetz (R-FL) applauded Presi...   \n",
       "1  Nigel Farage has said that the actions of gove...   \n",
       "2  The European Court of Human Rights (ECHR) has ...   \n",
       "3  UNITED NATIONS (AP) – Allegations of sexual ab...   \n",
       "4  The Swedish Public Health Authority has decide...   \n",
       "\n",
       "                                       article_title        date  \\\n",
       "0  Gaetz Slams Dems for Using Coronavirus Threat ...  2020-03-14   \n",
       "1  Farage on Coronavirus: ‘We Are All Nationalist...  2020-03-14   \n",
       "2  European Human Rights Court Rules Against Chri...  2020-03-14   \n",
       "3  Sexual Abuse Allegations Against UN Civilian S...  2020-03-14   \n",
       "4  Sweden to Stop Publishing Coronavirus Numbers,...  2020-03-14   \n",
       "\n",
       "                                                link  publisher  \n",
       "0  https://www.breitbart.com/clips/2020/03/14/gae...  Breitbart  \n",
       "1  https://www.breitbart.com/europe/2020/03/14/fa...  Breitbart  \n",
       "2  https://www.breitbart.com/europe/2020/03/14/eu...  Breitbart  \n",
       "3  https://www.breitbart.com/europe/2020/03/14/se...  Breitbart  \n",
       "4  https://www.breitbart.com/europe/2020/03/14/sw...  Breitbart  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure it looks nice\n",
    "breitbart_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 180\n",
      "total number of entries in new data: 210\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_breitbart_data = pd.read_csv('data/breitbart_data.csv')\n",
    "num_old = len(old_breitbart_data)\n",
    "\n",
    "# append new data\n",
    "breitbart_data = old_breitbart_data.append(breitbart_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "breitbart_data.to_csv(\"data/breitbart_data.csv\", index = False)\n",
    "num_now = len(breitbart_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fox - Conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "fox_requests = requests.get('https://www.foxnews.com/politics')\n",
    "fox_homepage = fox_requests.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a soup to allow BeautifulSoup to work\n",
    "fox_soup = BeautifulSoup(fox_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate article links\n",
    "fox_tags = fox_soup.find_all('article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "fox_links = []\n",
    "fox_text = []\n",
    "fox_titles = []\n",
    "fox_dates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_articles = 30\n",
    "\n",
    "# get homepage article links\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    link = fox_tags[n].find('a')\n",
    "    link = link.get('href')\n",
    "    link = \"https://foxnews.com\" + link\n",
    "    fox_links.append(link)\n",
    "    fox_links = [x for x in fox_links if \"/v/\" not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep for article content\n",
    "for link in fox_links:\n",
    "    fox_article_request = requests.get(link)\n",
    "    fox_article = fox_article_request.content\n",
    "    fox_article_soup = BeautifulSoup(fox_article, 'html.parser')\n",
    "    \n",
    "    # get article metadata\n",
    "    fox_metadata = fox_article_soup.find_all('script')[2].get_text()\n",
    "    fox_metadata = fox_metadata.split(\",\")\n",
    "    \n",
    "    for item in fox_metadata:\n",
    "\n",
    "        # get article title\n",
    "        if 'headline' in item:\n",
    "            item = item.replace('\\n',\"\")\n",
    "            item = item.replace('headline', \"\")\n",
    "            item = item.replace(':', \"\")\n",
    "            item = item.replace('\"', '')\n",
    "            fox_titles.append(item)\n",
    "        \n",
    "        # get article date\n",
    "        elif 'datePublished' in item:\n",
    "            item = item.replace('\\n',\"\")\n",
    "            item = item.replace('datePublished', \"\")\n",
    "            item = item.replace(':', \"\")\n",
    "            item = item.replace('\"', '')\n",
    "            fox_dates.append(item)\n",
    "    \n",
    "    # get article text\n",
    "    body = fox_article_soup.find_all('div')\n",
    "    x = body[0].find_all('p')\n",
    "    \n",
    "    # combine paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        paragraph = paragraph.replace('\\n',\"\")\n",
    "        list_paragraphs.append(paragraph)\n",
    "        \n",
    "        # removing copyright info and newsletter junk from the article\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        final_article = final_article.replace(\"This material may not be published, broadcast, rewritten, or redistributed. ©2020 FOX News Network, LLC. All rights reserved. All market data delayed 20 minutes.\", \" \")\n",
    "        final_article = final_article.replace(\"This material may not be published, broadcast, rewritten,\", \" \")\n",
    "        final_article = final_article.replace(\"or redistributed. ©2020 FOX News Network, LLC. All rights reserved.\", \" \")\n",
    "        final_article = final_article.replace(\"All market data delayed 20 minutes.\", \" \")\n",
    "        final_article = final_article.replace(\"Get all the stories you need-to-know from the most powerful name in news delivered first thing every morning to your inbox Subscribed You've successfully subscribed to this newsletter!\", \" \")\n",
    "    fox_text.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>President Trump and ...</td>\n",
       "      <td>Trump declares national emergency ove...</td>\n",
       "      <td>2020-03-13T140714-0400</td>\n",
       "      <td>https://foxnews.com/politics/trump-declares-na...</td>\n",
       "      <td>Fox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lawmakers work on co...</td>\n",
       "      <td>House OKs coronavirus relief bill</td>\n",
       "      <td>2020-03-13T182340-0400</td>\n",
       "      <td>https://foxnews.com/politics/house-oks-coronav...</td>\n",
       "      <td>Fox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Democratic president...</td>\n",
       "      <td>Democracy 2020 Digest Virtual campaig...</td>\n",
       "      <td>2020-03-13T172206-0400</td>\n",
       "      <td>https://foxnews.com/politics/democracy-2020-di...</td>\n",
       "      <td>Fox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why the global numbe...</td>\n",
       "      <td>De Blasio refuses to shutter NYC scho...</td>\n",
       "      <td>2020-03-14T091759-0400</td>\n",
       "      <td>https://foxnews.com/politics/de-blasio-nyc-sch...</td>\n",
       "      <td>Fox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lawmakers work on co...</td>\n",
       "      <td>House OKs coronavirus relief bill</td>\n",
       "      <td>2020-03-13T182340-0400</td>\n",
       "      <td>https://foxnews.com/politics/house-oks-coronav...</td>\n",
       "      <td>Fox</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  \\\n",
       "0                            President Trump and ...   \n",
       "1                            Lawmakers work on co...   \n",
       "2                            Democratic president...   \n",
       "3                            Why the global numbe...   \n",
       "4                            Lawmakers work on co...   \n",
       "\n",
       "                                       article_title  \\\n",
       "0           Trump declares national emergency ove...   \n",
       "1                  House OKs coronavirus relief bill   \n",
       "2           Democracy 2020 Digest Virtual campaig...   \n",
       "3           De Blasio refuses to shutter NYC scho...   \n",
       "4                  House OKs coronavirus relief bill   \n",
       "\n",
       "                              date  \\\n",
       "0           2020-03-13T140714-0400   \n",
       "1           2020-03-13T182340-0400   \n",
       "2           2020-03-13T172206-0400   \n",
       "3           2020-03-14T091759-0400   \n",
       "4           2020-03-13T182340-0400   \n",
       "\n",
       "                                                link publisher  \n",
       "0  https://foxnews.com/politics/trump-declares-na...       Fox  \n",
       "1  https://foxnews.com/politics/house-oks-coronav...       Fox  \n",
       "2  https://foxnews.com/politics/democracy-2020-di...       Fox  \n",
       "3  https://foxnews.com/politics/de-blasio-nyc-sch...       Fox  \n",
       "4  https://foxnews.com/politics/house-oks-coronav...       Fox  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join fox data\n",
    "fox_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'Fox',\n",
    "    'date': fox_dates,\n",
    "    'link': fox_links,\n",
    "    'article_title': fox_titles,\n",
    "    'article_text': fox_text \n",
    "})\n",
    "\n",
    "fox_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in old data\n",
    "old_fox_data = pd.read_csv('data/fox_data.csv')\n",
    "num_old = len(old_fox_data)\n",
    "\n",
    "# append new data\n",
    "fox_data = old_fox_data.append(fox_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "fox_data.to_csv(\"data/fox_data.csv\", index = False)\n",
    "num_now = len(fox_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 67\n",
      "total number of entries in new data: 81\n",
      "difference: 14\n"
     ]
    }
   ],
   "source": [
    "# see number of articles\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))\n",
    "print(\"difference: {}\".format(num_now-num_old))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Washington Times - Center Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "wt_request = requests.get('https://www.washingtontimes.com/news/politics/')\n",
    "wt_homepage = wt_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "wt_soup = BeautifulSoup(wt_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate article URLs\n",
    "wt_tags = wt_soup.find_all('h2', class_=\"article-headline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "number_of_articles = len(wt_tags)\n",
    "\n",
    "# get article titles, content, and links\n",
    "wt_links = []\n",
    "wt_titles = []\n",
    "wt_dates = []\n",
    "wt_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get article titles, content, and links\n",
    "for n in np.arange(0, number_of_articles):\n",
    "\n",
    "    # get article link\n",
    "    link = wt_tags[n].find('a')['href']\n",
    "    link = 'https://www.washingtontimes.com' + link\n",
    "    wt_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = wt_tags[n].find('a').get_text()\n",
    "    wt_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    meta = soup_article.find(\"div\", class_=\"meta\").find(\"span\", class_=\"source\").text\n",
    "    strip = meta.replace(' -\\n\\t\\t\\t\\n\\t\\t\\t\\tAssociated Press\\n -\\n                      \\n                        \\n                        ', '')\n",
    "    strip = strip.replace(' -\\n\\t\\t\\t\\n\\t\\t\\t\\tThe Washington Times\\n -\\n                      \\n                        \\n                        ', '')\n",
    "    date = strip.replace('\\n                      \\n                    ', '')\n",
    "    wt_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    for div in soup_article.find_all(\"div\", {'class':'article-toplinks'}): \n",
    "        div.decompose()\n",
    "    \n",
    "    body = soup_article.find_all('div', class_= 'bigtext')  \n",
    "    x = body[0].find_all('p')\n",
    "    \n",
    "    # combine paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs).split(\"\\n\")[0]\n",
    "       \n",
    "    wt_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# assembling data\n",
    "wt_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'washington_times',\n",
    "    'date': wt_dates,\n",
    "    'link': wt_links,\n",
    "    'article_title': wt_titles,\n",
    "    'article_text': wt_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WASHINGTON (AP) — The Democratic-controlled Ho...</td>\n",
       "      <td>House passes bipartisan coronavirus relief bil...</td>\n",
       "      <td>Saturday, March 14, 2020</td>\n",
       "      <td>https://www.washingtontimes.com/news/2020/mar/...</td>\n",
       "      <td>washington_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The federal appeals court in Washington, D.C.,...</td>\n",
       "      <td>Full appeals court to hear challenges over bor...</td>\n",
       "      <td>Friday, March 13, 2020</td>\n",
       "      <td>https://www.washingtontimes.com/news/2020/mar/...</td>\n",
       "      <td>washington_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The top Democrat on the Senate Foreign Relatio...</td>\n",
       "      <td>Menendez calls on Trump to admit Iranian deter...</td>\n",
       "      <td>Friday, March 13, 2020</td>\n",
       "      <td>https://www.washingtontimes.com/news/2020/mar/...</td>\n",
       "      <td>washington_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In his quest to regain a Senate seat he once h...</td>\n",
       "      <td>Jeff Sessions gets the conservative establishm...</td>\n",
       "      <td>Friday, March 13, 2020</td>\n",
       "      <td>https://www.washingtontimes.com/news/2020/mar/...</td>\n",
       "      <td>washington_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Count Sen. Bernie Sanders among the people who...</td>\n",
       "      <td>Sanders says Trump should get tested for coron...</td>\n",
       "      <td>Friday, March 13, 2020</td>\n",
       "      <td>https://www.washingtontimes.com/news/2020/mar/...</td>\n",
       "      <td>washington_times</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  \\\n",
       "0  WASHINGTON (AP) — The Democratic-controlled Ho...   \n",
       "1  The federal appeals court in Washington, D.C.,...   \n",
       "2  The top Democrat on the Senate Foreign Relatio...   \n",
       "3  In his quest to regain a Senate seat he once h...   \n",
       "4  Count Sen. Bernie Sanders among the people who...   \n",
       "\n",
       "                                       article_title  \\\n",
       "0  House passes bipartisan coronavirus relief bil...   \n",
       "1  Full appeals court to hear challenges over bor...   \n",
       "2  Menendez calls on Trump to admit Iranian deter...   \n",
       "3  Jeff Sessions gets the conservative establishm...   \n",
       "4  Sanders says Trump should get tested for coron...   \n",
       "\n",
       "                       date  \\\n",
       "0  Saturday, March 14, 2020   \n",
       "1    Friday, March 13, 2020   \n",
       "2    Friday, March 13, 2020   \n",
       "3    Friday, March 13, 2020   \n",
       "4    Friday, March 13, 2020   \n",
       "\n",
       "                                                link         publisher  \n",
       "0  https://www.washingtontimes.com/news/2020/mar/...  washington_times  \n",
       "1  https://www.washingtontimes.com/news/2020/mar/...  washington_times  \n",
       "2  https://www.washingtontimes.com/news/2020/mar/...  washington_times  \n",
       "3  https://www.washingtontimes.com/news/2020/mar/...  washington_times  \n",
       "4  https://www.washingtontimes.com/news/2020/mar/...  washington_times  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 27\n",
      "total number of entries in new data: 30\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_wt_data = pd.read_csv('data/wt_data.csv')\n",
    "num_old = len(old_wt_data)\n",
    "\n",
    "# append new data\n",
    "wt_data = old_wt_data.append(wt_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "wt_data.to_csv(\"data/wt_data.csv\", index = False)\n",
    "num_now = len(wt_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Associated Press - Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "ap_requests = requests.get('https://apnews.com/apf-politics')\n",
    "ap_homepage = ap_requests.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a soup to allow BeautifulSoup to work\n",
    "ap_soup = BeautifulSoup(ap_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate articles\n",
    "ap_tags = ap_soup.find_all('a', class_=\"Component-headline-0-2-105\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "number_of_articles = min(len(ap_tags), 30)\n",
    "\n",
    "ap_links = []\n",
    "ap_text = []\n",
    "ap_titles = []\n",
    "ap_dates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get homepage article links\n",
    "for link in ap_tags:\n",
    "    link = link.get('href')\n",
    "    link = \"https://apnews.com\" + link\n",
    "    ap_links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep for article content\n",
    "for link in ap_links:\n",
    "    ap_article_request = requests.get(link)\n",
    "    ap_article = ap_article_request.content\n",
    "    ap_article_soup = BeautifulSoup(ap_article, 'html.parser')\n",
    "    \n",
    "    # article titles\n",
    "    title = ap_article_soup.find_all('meta')[14]\n",
    "    title = title['content']\n",
    "    ap_titles.append(title)\n",
    "    \n",
    "    # article date\n",
    "    date = ap_article_soup.find_all('meta')[24]\n",
    "    date = date['content']\n",
    "    ap_dates.append(date)\n",
    "    \n",
    "    # article content: <div class=\"Article\" data-key=Article.\n",
    "    body = ap_article_soup.find_all('div')\n",
    "    x = body[0].find_all('p')\n",
    "\n",
    "    # combine paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        paragraph = paragraph.replace('\\n',\"\")\n",
    "        paragraph = paragraph.replace('CHICAGO (AP) -',\"\")\n",
    "        paragraph = paragraph.replace('DETROIT (AP) -',\"\")\n",
    "        paragraph = paragraph.replace('WASHINGTON (AP) -',\"\")\n",
    "        paragraph = paragraph.replace('___ Catch up on the 2020 election campaign with AP experts on our weekly politics podcast, “Ground Game.',\"\")\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "    ap_text.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WASHINGTON (AP) — Americans normally hear from...</td>\n",
       "      <td>AP FACT CHECK: Trump, American exceptionalism ...</td>\n",
       "      <td>2020-03-14T12:32:37Z</td>\n",
       "      <td>https://apnews.com/19963ea122c12eb72b8f0ff14e8...</td>\n",
       "      <td>AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WASHINGTON (AP) — The House approved legislati...</td>\n",
       "      <td>House passes aid bill after Trump declares vir...</td>\n",
       "      <td>2020-03-14T05:44:03Z</td>\n",
       "      <td>https://apnews.com/cb2685490d126350d41cae7562c...</td>\n",
       "      <td>AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And then there were two.  Joe Biden and Bernie...</td>\n",
       "      <td>Debate questions: Biden, Sanders are finally t...</td>\n",
       "      <td>2020-03-14T13:55:32Z</td>\n",
       "      <td>https://apnews.com/a47e1dd4f28e090fb2375cab87e...</td>\n",
       "      <td>AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WASHINGTON (AP) — President Donald Trump said ...</td>\n",
       "      <td>Trump says he's likely to be tested after repe...</td>\n",
       "      <td>2020-03-14T02:12:54Z</td>\n",
       "      <td>https://apnews.com/715fb5cd41518ac46e3a73c85b5...</td>\n",
       "      <td>AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WASHINGTON (AP) —  Sen. Bernie Sanders won the...</td>\n",
       "      <td>Sanders wins Northern Mariana Islands caucus, ...</td>\n",
       "      <td>2020-03-14T13:45:30Z</td>\n",
       "      <td>https://apnews.com/bdf5de197d0471306b47415c474...</td>\n",
       "      <td>AP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  \\\n",
       "0  WASHINGTON (AP) — Americans normally hear from...   \n",
       "1  WASHINGTON (AP) — The House approved legislati...   \n",
       "2  And then there were two.  Joe Biden and Bernie...   \n",
       "3  WASHINGTON (AP) — President Donald Trump said ...   \n",
       "4  WASHINGTON (AP) —  Sen. Bernie Sanders won the...   \n",
       "\n",
       "                                       article_title                  date  \\\n",
       "0  AP FACT CHECK: Trump, American exceptionalism ...  2020-03-14T12:32:37Z   \n",
       "1  House passes aid bill after Trump declares vir...  2020-03-14T05:44:03Z   \n",
       "2  Debate questions: Biden, Sanders are finally t...  2020-03-14T13:55:32Z   \n",
       "3  Trump says he's likely to be tested after repe...  2020-03-14T02:12:54Z   \n",
       "4  Sanders wins Northern Mariana Islands caucus, ...  2020-03-14T13:45:30Z   \n",
       "\n",
       "                                                link publisher  \n",
       "0  https://apnews.com/19963ea122c12eb72b8f0ff14e8...        AP  \n",
       "1  https://apnews.com/cb2685490d126350d41cae7562c...        AP  \n",
       "2  https://apnews.com/a47e1dd4f28e090fb2375cab87e...        AP  \n",
       "3  https://apnews.com/715fb5cd41518ac46e3a73c85b5...        AP  \n",
       "4  https://apnews.com/bdf5de197d0471306b47415c474...        AP  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join ap data\n",
    "ap_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'AP',\n",
    "    'date': ap_dates,\n",
    "    'link': ap_links,\n",
    "    'article_title': ap_titles,\n",
    "    'article_text': ap_text \n",
    "})\n",
    "\n",
    "ap_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in old data\n",
    "old_ap_data = pd.read_csv('data/ap_data.csv')\n",
    "num_old = len(old_ap_data)\n",
    "\n",
    "# append new data\n",
    "ap_data = old_ap_data.append(ap_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "ap_data.to_csv(\"data/ap_data.csv\", index = False)\n",
    "num_now = len(ap_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 156\n",
      "total number of entries in new data: 156\n",
      "difference: 0\n"
     ]
    }
   ],
   "source": [
    "# see number of articles\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))\n",
    "print(\"difference: {}\".format(num_now-num_old))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. NBC - Center-Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "nbc_request = requests.get('https://www.nbcnews.com/politics')\n",
    "nbc_homepage = nbc_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "nbc_soup = BeautifulSoup(nbc_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate article URLs\n",
    "nbc_tags = nbc_soup.find_all('h2', class_=\"teaseCard__headline\") + nbc_soup.find_all('h2', class_=\"title___2T5qK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "number_of_articles = len(nbc_tags)\n",
    "\n",
    "# get article titles, content, and links\n",
    "nbc_links = []\n",
    "nbc_titles = []\n",
    "nbc_dates = []\n",
    "nbc_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get article titles, content, and links\n",
    "for n in np.arange(0, number_of_articles):\n",
    "\n",
    "    # get article link\n",
    "    link = nbc_tags[n].find('a')['href']\n",
    "    nbc_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = nbc_tags[n].find('a').get_text()\n",
    "    nbc_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    if soup_article.time != None:\n",
    "        date = soup_article.time.attrs['datetime']\n",
    "        date = date[4:-24] \n",
    "    else:\n",
    "        date = None\n",
    "    nbc_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', class_= 'article-body__content')    \n",
    "    final_article = \" \".join([item.text for item in body])\n",
    "       \n",
    "    nbc_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling data\n",
    "nbc_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'nbc',\n",
    "    'date': nbc_dates,\n",
    "    'link': nbc_links,\n",
    "    'article_title': nbc_titles,\n",
    "    'article_text': nbc_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows that are not text articles (these will have NA in date)\n",
    "nbc_data = nbc_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>President Donald Trump on Friday announced a n...</td>\n",
       "      <td>Trump declares national emergency to combat co...</td>\n",
       "      <td>Mar 13 2020</td>\n",
       "      <td>https://www.nbcnews.com/politics/donald-trump/...</td>\n",
       "      <td>nbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With the coronavirus pandemic wreaking havoc o...</td>\n",
       "      <td>Coronavirus forces candidates to shift to 'vir...</td>\n",
       "      <td>Mar 13 2020</td>\n",
       "      <td>https://www.nbcnews.com/politics/2020-election...</td>\n",
       "      <td>nbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WASHINGTON — Joe Biden is winning the delegate...</td>\n",
       "      <td>Biden's delicate dance to win over the 'Bernie...</td>\n",
       "      <td>Mar 11 2020</td>\n",
       "      <td>https://www.nbcnews.com/politics/2020-election...</td>\n",
       "      <td>nbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The only way to beat President Donald Trump in...</td>\n",
       "      <td>Bill de Blasio: The Democratic primary isn't o...</td>\n",
       "      <td>Mar 11 2020</td>\n",
       "      <td>https://www.nbcnews.com/think/opinion/democrat...</td>\n",
       "      <td>nbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's not Super Tuesday, but there are six more...</td>\n",
       "      <td>It gets a whole lot harder for Sanders today. ...</td>\n",
       "      <td>Mar 09 2020</td>\n",
       "      <td>https://www.nbcnews.com/politics/2020-election...</td>\n",
       "      <td>nbc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  \\\n",
       "0  President Donald Trump on Friday announced a n...   \n",
       "1  With the coronavirus pandemic wreaking havoc o...   \n",
       "2  WASHINGTON — Joe Biden is winning the delegate...   \n",
       "3  The only way to beat President Donald Trump in...   \n",
       "4  It's not Super Tuesday, but there are six more...   \n",
       "\n",
       "                                       article_title         date  \\\n",
       "0  Trump declares national emergency to combat co...  Mar 13 2020   \n",
       "1  Coronavirus forces candidates to shift to 'vir...  Mar 13 2020   \n",
       "2  Biden's delicate dance to win over the 'Bernie...  Mar 11 2020   \n",
       "3  Bill de Blasio: The Democratic primary isn't o...  Mar 11 2020   \n",
       "4  It gets a whole lot harder for Sanders today. ...  Mar 09 2020   \n",
       "\n",
       "                                                link publisher  \n",
       "0  https://www.nbcnews.com/politics/donald-trump/...       nbc  \n",
       "1  https://www.nbcnews.com/politics/2020-election...       nbc  \n",
       "2  https://www.nbcnews.com/politics/2020-election...       nbc  \n",
       "3  https://www.nbcnews.com/think/opinion/democrat...       nbc  \n",
       "4  https://www.nbcnews.com/politics/2020-election...       nbc  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 52\n",
      "total number of entries in new data: 57\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_nbc_data = pd.read_csv('data/nbc_data.csv')\n",
    "num_old = len(old_nbc_data)\n",
    "\n",
    "# append new data\n",
    "nbc_data = old_nbc_data.append(nbc_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "nbc_data.to_csv(\"data/nbc_data.csv\", index = False)\n",
    "num_now = len(nbc_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. New York Times - Liberal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "nyt_request = requests.get('https://www.nytimes.com/section/politics')\n",
    "nyt_homepage = nyt_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "nyt_soup = BeautifulSoup(nyt_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homepage URLs\n",
    "nyt_tags_home = nyt_soup.find_all('h2', class_=\"css-l2vidh e4e4i5l1\")\n",
    "\n",
    "# archive URLs\n",
    "nyt_tags_archive = nyt_soup.find_all('div', class_='css-1l4spti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup \n",
    "nyt_links = []\n",
    "nyt_titles = []\n",
    "nyt_dates = []\n",
    "nyt_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homepage articles\n",
    "for n in np.arange(0, len(nyt_tags_home)):\n",
    "\n",
    "    # get article link\n",
    "    link = nyt_tags_home[n].find('a')['href']\n",
    "    link = \"https://www.nytimes.com\" + link\n",
    "    nyt_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = nyt_tags_home[n].find('a').get_text()\n",
    "    nyt_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.time.attrs['datetime']\n",
    "    date = date[:-15]\n",
    "    nyt_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', {'class':['css-53u6y8', 'css-1fanzo5']})\n",
    "    final_article = \" \".join([item.text for item in body])\n",
    "        \n",
    "    nyt_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archive articles\n",
    "for n in np.arange(0, len(nyt_tags_archive)):\n",
    "\n",
    "    # get article link\n",
    "    link = nyt_tags_archive[n].find('a')['href']\n",
    "    link = \"https://www.nytimes.com\" + link\n",
    "    nyt_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = nyt_tags_archive[n].find('a').get_text()\n",
    "    nyt_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.time.attrs['datetime']\n",
    "    date = date[:-15]\n",
    "    nyt_dates.append(date)\n",
    "        \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', attrs = {'class':['css-53u6y8', 'css-1fanzo5 StoryBodyCompanionColumn']})\n",
    "    final_article = \" \".join([item.text for item in body])\n",
    "        \n",
    "    nyt_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling data\n",
    "nyt_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'new_york_times',\n",
    "    'date': nyt_dates,\n",
    "    'link': nyt_links,\n",
    "    'article_title': nyt_titles,\n",
    "    'article_text': nyt_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WASHINGTON — President Trump declared a nation...</td>\n",
       "      <td>House Passes Coronavirus Relief After Democrat...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>https://www.nytimes.com/2020/03/13/us/politics...</td>\n",
       "      <td>new_york_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As he declared a national emergency over the c...</td>\n",
       "      <td>Trump’s False Claims About His Response to the...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>https://www.nytimes.com/2020/03/13/us/politics...</td>\n",
       "      <td>new_york_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WASHINGTON — There was one big question loomin...</td>\n",
       "      <td>Trump Won’t Be Getting a Coronavirus Test, His...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>https://www.nytimes.com/2020/03/13/us/politics...</td>\n",
       "      <td>new_york_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The magnitude of former Vice President Joseph ...</td>\n",
       "      <td>Joe Biden’s Young Voter Problem: They Don’t Th...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>https://www.nytimes.com/2020/03/13/us/politics...</td>\n",
       "      <td>new_york_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The coronavirus outbreak is inflicting new dis...</td>\n",
       "      <td>Could the 2020 Election Be Postponed? Only Wit...</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.nytimes.com/2020/03/14/us/politics...</td>\n",
       "      <td>new_york_times</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  \\\n",
       "0  WASHINGTON — President Trump declared a nation...   \n",
       "1  As he declared a national emergency over the c...   \n",
       "2  WASHINGTON — There was one big question loomin...   \n",
       "3  The magnitude of former Vice President Joseph ...   \n",
       "4  The coronavirus outbreak is inflicting new dis...   \n",
       "\n",
       "                                       article_title        date  \\\n",
       "0  House Passes Coronavirus Relief After Democrat...  2020-03-13   \n",
       "1  Trump’s False Claims About His Response to the...  2020-03-13   \n",
       "2  Trump Won’t Be Getting a Coronavirus Test, His...  2020-03-13   \n",
       "3  Joe Biden’s Young Voter Problem: They Don’t Th...  2020-03-13   \n",
       "4  Could the 2020 Election Be Postponed? Only Wit...  2020-03-14   \n",
       "\n",
       "                                                link       publisher  \n",
       "0  https://www.nytimes.com/2020/03/13/us/politics...  new_york_times  \n",
       "1  https://www.nytimes.com/2020/03/13/us/politics...  new_york_times  \n",
       "2  https://www.nytimes.com/2020/03/13/us/politics...  new_york_times  \n",
       "3  https://www.nytimes.com/2020/03/13/us/politics...  new_york_times  \n",
       "4  https://www.nytimes.com/2020/03/14/us/politics...  new_york_times  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure it looks nice\n",
    "nyt_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 55\n",
      "total number of entries in new data: 69\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_nyt_data = pd.read_csv('data/nyt_data.csv')\n",
    "num_old = len(old_nyt_data)\n",
    "\n",
    "# append new data\n",
    "nyt_data = old_nyt_data.append(nyt_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "nyt_data.to_csv(\"data/nyt_data.csv\", index = False)\n",
    "num_now = len(nyt_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Politico - Liberal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "politico_request = requests.get('https://www.politico.com/politics')\n",
    "politico_homepage = politico_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "politico_soup = BeautifulSoup(politico_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate article URLs\n",
    "politico_tags = politico_soup.find_all('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "number_of_articles = len(politico_tags)\n",
    "\n",
    "# get article titles, content, and links\n",
    "politico_links = []\n",
    "politico_titles = []\n",
    "politico_dates = []\n",
    "politico_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get article titles, content, and links\n",
    "for n in np.arange(0, number_of_articles):\n",
    "\n",
    "    # get article link\n",
    "    link = politico_tags[n].find('a')['href']\n",
    "    politico_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = politico_tags[n].find('a').get_text()\n",
    "    politico_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.time.attrs['datetime']\n",
    "    date = date[:-9]\n",
    "    politico_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    body = soup_article.find_all('p', attrs={'class':'story-text__paragraph'})\n",
    "    final_article = \" \".join([item.text for item in body])\n",
    "    \n",
    "    politico_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling data\n",
    "politico_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'politico',\n",
    "    'date': politico_dates,\n",
    "    'link': politico_links,\n",
    "    'article_title': politico_titles,\n",
    "    'article_text': politico_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows that are not text articles (these will have NA in text)\n",
    "politico_data = politico_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the weeks since the coronavirus outbreak fi...</td>\n",
       "      <td>What Trump’s Twitter Feed Tells Him About the ...</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.politico.com/news/magazine/2020/03...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As Americans brace for the rapid spread of the...</td>\n",
       "      <td>Who gets saved and who collapses?</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.politico.com/news/2020/03/14/trump...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arkansas Republican Tom Cotton, a freshman sen...</td>\n",
       "      <td>Two senators take extreme measures to show all...</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.politico.com/news/2020/03/14/cotto...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Local officials from around the country are wo...</td>\n",
       "      <td>A health system overwhelmed</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.politico.com/news/2020/03/14/healt...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joe Biden has long complained about the crowde...</td>\n",
       "      <td>Biden readies plan to finish off Bernie</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.politico.com/news/2020/03/14/biden...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  \\\n",
       "0  In the weeks since the coronavirus outbreak fi...   \n",
       "1  As Americans brace for the rapid spread of the...   \n",
       "2  Arkansas Republican Tom Cotton, a freshman sen...   \n",
       "3  Local officials from around the country are wo...   \n",
       "4  Joe Biden has long complained about the crowde...   \n",
       "\n",
       "                                       article_title        date  \\\n",
       "0  What Trump’s Twitter Feed Tells Him About the ...  2020-03-14   \n",
       "1                  Who gets saved and who collapses?  2020-03-14   \n",
       "2  Two senators take extreme measures to show all...  2020-03-14   \n",
       "3                        A health system overwhelmed  2020-03-14   \n",
       "4            Biden readies plan to finish off Bernie  2020-03-14   \n",
       "\n",
       "                                                link publisher  \n",
       "0  https://www.politico.com/news/magazine/2020/03...  politico  \n",
       "1  https://www.politico.com/news/2020/03/14/trump...  politico  \n",
       "2  https://www.politico.com/news/2020/03/14/cotto...  politico  \n",
       "3  https://www.politico.com/news/2020/03/14/healt...  politico  \n",
       "4  https://www.politico.com/news/2020/03/14/biden...  politico  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politico_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 69\n",
      "total number of entries in new data: 73\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_politico_data = pd.read_csv('data/politico_data.csv')\n",
    "num_old = len(old_politico_data)\n",
    "\n",
    "# append new data\n",
    "politico_data = old_politico_data.append(politico_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "politico_data.to_csv(\"data/politico_data.csv\", index = False)\n",
    "num_now = len(politico_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Buzzfeed - Very Liberal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "buzz_request = requests.get('https://www.buzzfeednews.com/section/politics')\n",
    "buzz_homepage = buzz_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "buzz_soup = BeautifulSoup(buzz_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate article URLs\n",
    "buzz_tags = buzz_soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "number_of_articles = min(len(buzz_tags), 30)\n",
    "\n",
    "# get article titles, content, and links\n",
    "buzz_links = []\n",
    "buzz_titles = []\n",
    "buzz_dates = []\n",
    "buzz_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get article titles, content, and links\n",
    "for n in np.arange(0, number_of_articles):\n",
    "\n",
    "    # get article link\n",
    "    link = buzz_tags[n].find('a')['href']\n",
    "    buzz_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = buzz_tags[n].find('a').get_text()\n",
    "    buzz_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.find_all('div', class_=\"news-article-header__timestamps\")    \n",
    "    date = \" \".join([item.text for item in date]).replace('\\n', '')\n",
    "    buzz_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', attrs={'data-module':'subbuzz-text'})\n",
    "    article = \" \".join([item.text for item in body]).replace('\\n', '')\n",
    "    final_article = re.sub(r' {[^}]*}', '', article)\n",
    "        \n",
    "    buzz_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling data\n",
    "buzz_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'buzzfeed',\n",
    "    'date': buzz_dates,\n",
    "    'link': buzz_links,\n",
    "    'article_title': buzz_titles,\n",
    "    'article_text': buzz_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WASHINGTON — When Washington state emerge...</td>\n",
       "      <td>A Senator From The State Hit Hardest By Corona...</td>\n",
       "      <td>Posted on March 13, 2020, at 9:28 ...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/kadiagoba...</td>\n",
       "      <td>buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WASHINGTON — After shaking hands with sev...</td>\n",
       "      <td>Trump Said He Won't Self-Isolate After Coming ...</td>\n",
       "      <td>Posted on March 13, 2020, at 5:41 ...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/paulmcleo...</td>\n",
       "      <td>buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On Thursday, a prominent Chinese diplomat...</td>\n",
       "      <td>Chinese Diplomats Are Pushing Conspiracy Theor...</td>\n",
       "      <td>Last updated on March 13, 2020, at...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/ryanhates...</td>\n",
       "      <td>buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>President Trump declared a national state...</td>\n",
       "      <td>Trump Declared A National State Of Emergency O...</td>\n",
       "      <td>Last updated on March 13, 2020, at...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/clarissaj...</td>\n",
       "      <td>buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thousands of Americans think President Do...</td>\n",
       "      <td>What Happens If Trump Tries To Cancel The Elec...</td>\n",
       "      <td>Posted on March 13, 2020, at 2:12 ...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/dominicho...</td>\n",
       "      <td>buzzfeed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  \\\n",
       "0       WASHINGTON — When Washington state emerge...   \n",
       "1       WASHINGTON — After shaking hands with sev...   \n",
       "2       On Thursday, a prominent Chinese diplomat...   \n",
       "3       President Trump declared a national state...   \n",
       "4       Thousands of Americans think President Do...   \n",
       "\n",
       "                                       article_title  \\\n",
       "0  A Senator From The State Hit Hardest By Corona...   \n",
       "1  Trump Said He Won't Self-Isolate After Coming ...   \n",
       "2  Chinese Diplomats Are Pushing Conspiracy Theor...   \n",
       "3  Trump Declared A National State Of Emergency O...   \n",
       "4  What Happens If Trump Tries To Cancel The Elec...   \n",
       "\n",
       "                                                date  \\\n",
       "0              Posted on March 13, 2020, at 9:28 ...   \n",
       "1              Posted on March 13, 2020, at 5:41 ...   \n",
       "2              Last updated on March 13, 2020, at...   \n",
       "3              Last updated on March 13, 2020, at...   \n",
       "4              Posted on March 13, 2020, at 2:12 ...   \n",
       "\n",
       "                                                link publisher  \n",
       "0  https://www.buzzfeednews.com/article/kadiagoba...  buzzfeed  \n",
       "1  https://www.buzzfeednews.com/article/paulmcleo...  buzzfeed  \n",
       "2  https://www.buzzfeednews.com/article/ryanhates...  buzzfeed  \n",
       "3  https://www.buzzfeednews.com/article/clarissaj...  buzzfeed  \n",
       "4  https://www.buzzfeednews.com/article/dominicho...  buzzfeed  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buzz_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 54\n",
      "total number of entries in new data: 60\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_buzz_data = pd.read_csv('data/buzzfeed_data.csv')\n",
    "num_old = len(old_buzz_data)\n",
    "\n",
    "# append new data\n",
    "buzz_data = old_buzz_data.append(buzz_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "buzz_data.to_csv(\"data/buzzfeed_data.csv\", index = False)\n",
    "num_now = len(buzz_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
