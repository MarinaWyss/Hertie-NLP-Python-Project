{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper Tool for US Media Outlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import UnicodeDammit\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Breitbart - Very Conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "breitbart_request = requests.get('https://www.breitbart.com/politics/')\n",
    "breitbart_homepage = breitbart_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "breitbart_soup = BeautifulSoup(breitbart_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# locate article URLs\n",
    "breitbart_tags = breitbart_soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "number_of_articles = min(len(breitbart_tags), 30)\n",
    "\n",
    "breitbart_links = []\n",
    "breitbart_titles = []\n",
    "breitbart_dates = []\n",
    "breitbart_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get article titles, content, and links\n",
    "for n in np.arange(0, number_of_articles):\n",
    "\n",
    "    # get article link\n",
    "    link = breitbart_tags[n].find('a')['href']\n",
    "    link = \"https://www.breitbart.com\" + link\n",
    "    breitbart_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = breitbart_tags[n].find('a').get_text()\n",
    "    breitbart_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.time.attrs['datetime']\n",
    "    date = date[:-10]\n",
    "    breitbart_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', class_='entry-content')\n",
    "    x = body[0].find_all('p')\n",
    "    \n",
    "    # combine paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    breitbart_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling data\n",
    "breitbart_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'Breitbart',\n",
    "    'date': breitbart_dates,\n",
    "    'link': breitbart_links,\n",
    "    'article_title': breitbart_titles,\n",
    "    'article_text': breitbart_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Greek authorities have deployed large fans at ...</td>\n",
       "      <td>Greece Deploys Large Fans to Blow Back Migrant...</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.breitbart.com/europe/2020/03/14/gr...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>President Donald Trump announced during his pr...</td>\n",
       "      <td>Trump Orders Department of Energy to Replenish...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>https://www.breitbart.com/politics/2020/03/13/...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Former Vice President Joe Biden (D) is refusin...</td>\n",
       "      <td>Joe Biden Refuses to Listen to Experts, Reject...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>https://www.breitbart.com/politics/2020/03/13/...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>President Donald Trump on Friday night announc...</td>\n",
       "      <td>Donald Trump Endorses Nancy Pelosi Coronavirus...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>https://www.breitbart.com/politics/2020/03/13/...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>President Donald Trump suggested that he would...</td>\n",
       "      <td>Donald Trump Says He Will Get Tested for Coron...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>https://www.breitbart.com/politics/2020/03/13/...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  \\\n",
       "0  Greek authorities have deployed large fans at ...   \n",
       "1  President Donald Trump announced during his pr...   \n",
       "2  Former Vice President Joe Biden (D) is refusin...   \n",
       "3  President Donald Trump on Friday night announc...   \n",
       "4  President Donald Trump suggested that he would...   \n",
       "\n",
       "                                       article_title        date  \\\n",
       "0  Greece Deploys Large Fans to Blow Back Migrant...  2020-03-14   \n",
       "1  Trump Orders Department of Energy to Replenish...  2020-03-13   \n",
       "2  Joe Biden Refuses to Listen to Experts, Reject...  2020-03-13   \n",
       "3  Donald Trump Endorses Nancy Pelosi Coronavirus...  2020-03-13   \n",
       "4  Donald Trump Says He Will Get Tested for Coron...  2020-03-13   \n",
       "\n",
       "                                                link  publisher  \n",
       "0  https://www.breitbart.com/europe/2020/03/14/gr...  Breitbart  \n",
       "1  https://www.breitbart.com/politics/2020/03/13/...  Breitbart  \n",
       "2  https://www.breitbart.com/politics/2020/03/13/...  Breitbart  \n",
       "3  https://www.breitbart.com/politics/2020/03/13/...  Breitbart  \n",
       "4  https://www.breitbart.com/politics/2020/03/13/...  Breitbart  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure it looks nice\n",
    "breitbart_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 180\n",
      "total number of entries in new data: 210\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_breitbart_data = pd.read_csv('data/breitbart_data.csv')\n",
    "num_old = len(old_breitbart_data)\n",
    "\n",
    "# append new data\n",
    "breitbart_data = old_breitbart_data.append(breitbart_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "breitbart_data.to_csv(\"data/breitbart_data.csv\", index = False)\n",
    "num_now = len(breitbart_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fox - Conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "fox_requests = requests.get('https://www.foxnews.com/politics')\n",
    "fox_homepage = fox_requests.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a soup to allow BeautifulSoup to work\n",
    "fox_soup = BeautifulSoup(fox_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate article links\n",
    "fox_tags = fox_soup.find_all('article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_articles = 30\n",
    "\n",
    "fox_links = []\n",
    "\n",
    "# get homepage article links\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    link = fox_tags[n].find('a')\n",
    "    link = link.get('href')\n",
    "    link = \"https://foxnews.com\" + link\n",
    "    fox_links.append(link)\n",
    "    fox_links = [x for x in fox_links if \"/v/\" not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_text = []\n",
    "fox_titles = []\n",
    "fox_dates = []\n",
    "\n",
    "# prep for article content\n",
    "for link in fox_links:\n",
    "    fox_article_request = requests.get(link)\n",
    "    fox_article = fox_article_request.content\n",
    "    fox_article_soup = BeautifulSoup(fox_article, 'html.parser')\n",
    "    \n",
    "    # get article metadata\n",
    "    fox_metadata = fox_article_soup.find_all('script')[2].get_text()\n",
    "    fox_metadata = fox_metadata.split(\",\")\n",
    "    \n",
    "    for item in fox_metadata:\n",
    "\n",
    "        # get article title\n",
    "        if 'headline' in item:\n",
    "            item = item.replace('\\n',\"\")\n",
    "            item = item.replace('headline', \"\")\n",
    "            item = item.replace(':', \"\")\n",
    "            item = item.replace('\"', '')\n",
    "            fox_titles.append(item)\n",
    "        \n",
    "        # get article date\n",
    "        elif 'datePublished' in item:\n",
    "            item = item.replace('\\n',\"\")\n",
    "            item = item.replace('datePublished', \"\")\n",
    "            item = item.replace(':', \"\")\n",
    "            item = item.replace('\"', '')\n",
    "            fox_dates.append(item)\n",
    "    \n",
    "    # get article text\n",
    "    body = fox_article_soup.find_all('div')\n",
    "    x = body[0].find_all('p')\n",
    "    \n",
    "    # combine paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        paragraph = paragraph.replace('\\n',\"\")\n",
    "        list_paragraphs.append(paragraph)\n",
    "        \n",
    "        # removing copyright info and newsletter junk from the article\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        final_article = final_article.replace(\"This material may not be published, broadcast, rewritten, or redistributed. ©2020 FOX News Network, LLC. All rights reserved. All market data delayed 20 minutes.\", \" \")\n",
    "        final_article = final_article.replace(\"This material may not be published, broadcast, rewritten,\", \" \")\n",
    "        final_article = final_article.replace(\"or redistributed. ©2020 FOX News Network, LLC. All rights reserved.\", \" \")\n",
    "        final_article = final_article.replace(\"All market data delayed 20 minutes.\", \" \")\n",
    "        final_article = final_article.replace(\"Get all the stories you need-to-know from the most powerful name in news delivered first thing every morning to your inbox Subscribed You've successfully subscribed to this newsletter!\", \" \")\n",
    "    fox_text.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publisher</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Fox</td>\n",
       "      <td>2020-03-07T154057-0500</td>\n",
       "      <td>https://foxnews.com/politics/pence-coronavirus...</td>\n",
       "      <td>Pence calls for 'whole of America' ap...</td>\n",
       "      <td>VP Pence meets with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Fox</td>\n",
       "      <td>2020-03-06T211054-0500</td>\n",
       "      <td>https://foxnews.com/politics/house-prepares-te...</td>\n",
       "      <td>House prepares for telework scenarios...</td>\n",
       "      <td>Fox News Flash top h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Fox</td>\n",
       "      <td>2020-03-07T131303-0500</td>\n",
       "      <td>https://foxnews.com/politics/sanders-biden-ent...</td>\n",
       "      <td>Sanders says Biden can’t ‘generate en...</td>\n",
       "      <td>Former Vice Presiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Fox</td>\n",
       "      <td>2020-03-07T154057-0500</td>\n",
       "      <td>https://foxnews.com/politics/pence-coronavirus...</td>\n",
       "      <td>Pence calls for 'whole of America' ap...</td>\n",
       "      <td>VP Pence meets with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Fox</td>\n",
       "      <td>2020-03-07T131303-0500</td>\n",
       "      <td>https://foxnews.com/politics/sanders-biden-ent...</td>\n",
       "      <td>Sanders says Biden can’t ‘generate en...</td>\n",
       "      <td>Former Vice Presiden...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publisher                             date  \\\n",
       "0       Fox           2020-03-07T154057-0500   \n",
       "1       Fox           2020-03-06T211054-0500   \n",
       "2       Fox           2020-03-07T131303-0500   \n",
       "3       Fox           2020-03-07T154057-0500   \n",
       "4       Fox           2020-03-07T131303-0500   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://foxnews.com/politics/pence-coronavirus...   \n",
       "1  https://foxnews.com/politics/house-prepares-te...   \n",
       "2  https://foxnews.com/politics/sanders-biden-ent...   \n",
       "3  https://foxnews.com/politics/pence-coronavirus...   \n",
       "4  https://foxnews.com/politics/sanders-biden-ent...   \n",
       "\n",
       "                                       article_title  \\\n",
       "0           Pence calls for 'whole of America' ap...   \n",
       "1           House prepares for telework scenarios...   \n",
       "2           Sanders says Biden can’t ‘generate en...   \n",
       "3           Pence calls for 'whole of America' ap...   \n",
       "4           Sanders says Biden can’t ‘generate en...   \n",
       "\n",
       "                                        article_text  \n",
       "0                            VP Pence meets with ...  \n",
       "1                            Fox News Flash top h...  \n",
       "2                            Former Vice Presiden...  \n",
       "3                            VP Pence meets with ...  \n",
       "4                            Former Vice Presiden...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join fox data\n",
    "fox_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'Fox',\n",
    "    'date': fox_dates,\n",
    "    'link': fox_links,\n",
    "    'article_title': fox_titles,\n",
    "    'article_text': fox_text \n",
    "})\n",
    "\n",
    "fox_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_data.to_csv(\"data/fox_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in old data\n",
    "old_fox_data = pd.read_csv('data/fox_data.csv')\n",
    "\n",
    "# append new data\n",
    "fox_data = old_fox_data.append(fox_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "fox_data.to_csv(\"data/fox_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The Washington Times - Center Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "wt_request = requests.get('https://www.washingtontimes.com/news/politics/')\n",
    "wt_homepage = wt_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "wt_soup = BeautifulSoup(wt_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate article URLs\n",
    "wt_tags = wt_soup.find_all('h2', class_=\"article-headline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "number_of_articles = len(wt_tags)\n",
    "\n",
    "# get article titles, content, and links\n",
    "wt_links = []\n",
    "wt_titles = []\n",
    "wt_dates = []\n",
    "wt_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get article titles, content, and links\n",
    "for n in np.arange(0, number_of_articles):\n",
    "\n",
    "    # get article link\n",
    "    link = wt_tags[n].find('a')['href']\n",
    "    link = 'https://www.washingtontimes.com' + link\n",
    "    wt_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = wt_tags[n].find('a').get_text()\n",
    "    wt_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    meta = soup_article.find(\"div\", class_=\"meta\").find(\"span\", class_=\"source\").text\n",
    "    strip = meta.replace(' -\\n\\t\\t\\t\\n\\t\\t\\t\\tAssociated Press\\n -\\n                      \\n                        \\n                        ', '')\n",
    "    strip = strip.replace(' -\\n\\t\\t\\t\\n\\t\\t\\t\\tThe Washington Times\\n -\\n                      \\n                        \\n                        ', '')\n",
    "    date = strip.replace('\\n                      \\n                    ', '')\n",
    "    wt_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    for div in soup_article.find_all(\"div\", {'class':'article-toplinks'}): \n",
    "        div.decompose()\n",
    "    \n",
    "    body = soup_article.find_all('div', class_= 'bigtext')  \n",
    "    x = body[0].find_all('p')\n",
    "    \n",
    "    # combine paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs).split(\"\\n\")[0]\n",
    "       \n",
    "    wt_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# assembling data\n",
    "wt_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'washington_times',\n",
    "    'date': wt_dates,\n",
    "    'link': wt_links,\n",
    "    'article_title': wt_titles,\n",
    "    'article_text': wt_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WASHINGTON (AP) — The Democratic-controlled Ho...</td>\n",
       "      <td>House passes bipartisan coronavirus relief bil...</td>\n",
       "      <td>Saturday, March 14, 2020</td>\n",
       "      <td>https://www.washingtontimes.com/news/2020/mar/...</td>\n",
       "      <td>washington_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The federal appeals court in Washington, D.C.,...</td>\n",
       "      <td>Full appeals court to hear challenges over bor...</td>\n",
       "      <td>Friday, March 13, 2020</td>\n",
       "      <td>https://www.washingtontimes.com/news/2020/mar/...</td>\n",
       "      <td>washington_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The top Democrat on the Senate Foreign Relatio...</td>\n",
       "      <td>Menendez calls on Trump to admit Iranian deter...</td>\n",
       "      <td>Friday, March 13, 2020</td>\n",
       "      <td>https://www.washingtontimes.com/news/2020/mar/...</td>\n",
       "      <td>washington_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In his quest to regain a Senate seat he once h...</td>\n",
       "      <td>Jeff Sessions gets the conservative establishm...</td>\n",
       "      <td>Friday, March 13, 2020</td>\n",
       "      <td>https://www.washingtontimes.com/news/2020/mar/...</td>\n",
       "      <td>washington_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Count Sen. Bernie Sanders among the people who...</td>\n",
       "      <td>Sanders says Trump should get tested for coron...</td>\n",
       "      <td>Friday, March 13, 2020</td>\n",
       "      <td>https://www.washingtontimes.com/news/2020/mar/...</td>\n",
       "      <td>washington_times</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  \\\n",
       "0  WASHINGTON (AP) — The Democratic-controlled Ho...   \n",
       "1  The federal appeals court in Washington, D.C.,...   \n",
       "2  The top Democrat on the Senate Foreign Relatio...   \n",
       "3  In his quest to regain a Senate seat he once h...   \n",
       "4  Count Sen. Bernie Sanders among the people who...   \n",
       "\n",
       "                                       article_title  \\\n",
       "0  House passes bipartisan coronavirus relief bil...   \n",
       "1  Full appeals court to hear challenges over bor...   \n",
       "2  Menendez calls on Trump to admit Iranian deter...   \n",
       "3  Jeff Sessions gets the conservative establishm...   \n",
       "4  Sanders says Trump should get tested for coron...   \n",
       "\n",
       "                       date  \\\n",
       "0  Saturday, March 14, 2020   \n",
       "1    Friday, March 13, 2020   \n",
       "2    Friday, March 13, 2020   \n",
       "3    Friday, March 13, 2020   \n",
       "4    Friday, March 13, 2020   \n",
       "\n",
       "                                                link         publisher  \n",
       "0  https://www.washingtontimes.com/news/2020/mar/...  washington_times  \n",
       "1  https://www.washingtontimes.com/news/2020/mar/...  washington_times  \n",
       "2  https://www.washingtontimes.com/news/2020/mar/...  washington_times  \n",
       "3  https://www.washingtontimes.com/news/2020/mar/...  washington_times  \n",
       "4  https://www.washingtontimes.com/news/2020/mar/...  washington_times  "
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 27\n",
      "total number of entries in new data: 27\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_wt_data = pd.read_csv('data/wt_data.csv')\n",
    "num_old = len(old_wt_data)\n",
    "\n",
    "# append new data\n",
    "wt_data = old_wt_data.append(wt_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "wt_data.to_csv(\"data/wt_data.csv\", index = False)\n",
    "num_now = len(wt_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Wall Street Journal - Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "wsj_requests = requests.get('https://www.wsj.com/news/politics')\n",
    "wsj_homepage = wsj_requests.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a soup to allow BeautifulSoup to work\n",
    "wsj_soup = BeautifulSoup(wsj_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate article links\n",
    "wsj_tags = wsj_soup.find_all('div')\n",
    "# print(wsj_tags)\n",
    "\n",
    "# LOOKING FOR -- a class=\"wsj-headline-link\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. NBC - Center-Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "nbc_request = requests.get('https://www.nbcnews.com/politics')\n",
    "nbc_homepage = nbc_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "nbc_soup = BeautifulSoup(nbc_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate article URLs\n",
    "nbc_tags = nbc_soup.find_all('h2', class_=\"teaseCard__headline\") + nbc_soup.find_all('h2', class_=\"title___2T5qK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "number_of_articles = len(nbc_tags)\n",
    "\n",
    "# get article titles, content, and links\n",
    "nbc_links = []\n",
    "nbc_titles = []\n",
    "nbc_dates = []\n",
    "nbc_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get article titles, content, and links\n",
    "for n in np.arange(0, number_of_articles):\n",
    "\n",
    "    # get article link\n",
    "    link = nbc_tags[n].find('a')['href']\n",
    "    nbc_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = nbc_tags[n].find('a').get_text()\n",
    "    nbc_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    if soup_article.time != None:\n",
    "        date = soup_article.time.attrs['datetime']\n",
    "        date = date[4:-24] \n",
    "    else:\n",
    "        date = None\n",
    "    nbc_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', class_= 'article-body__content')    \n",
    "    final_article = \" \".join([item.text for item in body])\n",
    "       \n",
    "    nbc_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling data\n",
    "nbc_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'nbc',\n",
    "    'date': nbc_dates,\n",
    "    'link': nbc_links,\n",
    "    'article_title': nbc_titles,\n",
    "    'article_text': nbc_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows that are not text articles (these will have NA in date)\n",
    "nbc_data = nbc_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>President Donald Trump on Friday announced a n...</td>\n",
       "      <td>Trump declares national emergency to combat co...</td>\n",
       "      <td>Mar 13 2020</td>\n",
       "      <td>https://www.nbcnews.com/politics/donald-trump/...</td>\n",
       "      <td>nbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With the coronavirus pandemic wreaking havoc o...</td>\n",
       "      <td>Coronavirus forces candidates to shift to 'vir...</td>\n",
       "      <td>Mar 13 2020</td>\n",
       "      <td>https://www.nbcnews.com/politics/2020-election...</td>\n",
       "      <td>nbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WASHINGTON — Joe Biden is winning the delegate...</td>\n",
       "      <td>Biden's delicate dance to win over the 'Bernie...</td>\n",
       "      <td>Mar 11 2020</td>\n",
       "      <td>https://www.nbcnews.com/politics/2020-election...</td>\n",
       "      <td>nbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The only way to beat President Donald Trump in...</td>\n",
       "      <td>Bill de Blasio: The Democratic primary isn't o...</td>\n",
       "      <td>Mar 11 2020</td>\n",
       "      <td>https://www.nbcnews.com/think/opinion/democrat...</td>\n",
       "      <td>nbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's not Super Tuesday, but there are six more...</td>\n",
       "      <td>It gets a whole lot harder for Sanders today. ...</td>\n",
       "      <td>Mar 09 2020</td>\n",
       "      <td>https://www.nbcnews.com/politics/2020-election...</td>\n",
       "      <td>nbc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  \\\n",
       "0  President Donald Trump on Friday announced a n...   \n",
       "1  With the coronavirus pandemic wreaking havoc o...   \n",
       "2  WASHINGTON — Joe Biden is winning the delegate...   \n",
       "3  The only way to beat President Donald Trump in...   \n",
       "4  It's not Super Tuesday, but there are six more...   \n",
       "\n",
       "                                       article_title         date  \\\n",
       "0  Trump declares national emergency to combat co...  Mar 13 2020   \n",
       "1  Coronavirus forces candidates to shift to 'vir...  Mar 13 2020   \n",
       "2  Biden's delicate dance to win over the 'Bernie...  Mar 11 2020   \n",
       "3  Bill de Blasio: The Democratic primary isn't o...  Mar 11 2020   \n",
       "4  It gets a whole lot harder for Sanders today. ...  Mar 09 2020   \n",
       "\n",
       "                                                link publisher  \n",
       "0  https://www.nbcnews.com/politics/donald-trump/...       nbc  \n",
       "1  https://www.nbcnews.com/politics/2020-election...       nbc  \n",
       "2  https://www.nbcnews.com/politics/2020-election...       nbc  \n",
       "3  https://www.nbcnews.com/think/opinion/democrat...       nbc  \n",
       "4  https://www.nbcnews.com/politics/2020-election...       nbc  "
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 20\n",
      "total number of entries in new data: 52\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_nbc_data = pd.read_csv('data/nbc_data.csv')\n",
    "num_old = len(old_nbc_data)\n",
    "\n",
    "# append new data\n",
    "nbc_data = old_nbc_data.append(nbc_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "nbc_data.to_csv(\"data/nbc_data.csv\", index = False)\n",
    "num_now = len(nbc_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. New York Times - Liberal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "nyt_request = requests.get('https://www.nytimes.com/section/politics')\n",
    "nyt_homepage = nyt_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "nyt_soup = BeautifulSoup(nyt_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homepage URLs\n",
    "nyt_tags_home = nyt_soup.find_all('h2', class_=\"css-l2vidh e4e4i5l1\")\n",
    "\n",
    "# archive URLs\n",
    "nyt_tags_archive = nyt_soup.find_all('div', class_='css-1l4spti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup \n",
    "nyt_links = []\n",
    "nyt_titles = []\n",
    "nyt_dates = []\n",
    "nyt_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homepage articles\n",
    "for n in np.arange(0, len(nyt_tags_home)):\n",
    "\n",
    "    # get article link\n",
    "    link = nyt_tags_home[n].find('a')['href']\n",
    "    link = \"https://www.nytimes.com\" + link\n",
    "    nyt_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = nyt_tags_home[n].find('a').get_text()\n",
    "    nyt_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.time.attrs['datetime']\n",
    "    date = date[:-15]\n",
    "    nyt_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', {'class':['css-53u6y8', 'css-1fanzo5']})\n",
    "    final_article = \" \".join([item.text for item in body])\n",
    "        \n",
    "    nyt_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archive articles\n",
    "for n in np.arange(0, len(nyt_tags_archive)):\n",
    "\n",
    "    # get article link\n",
    "    link = nyt_tags_archive[n].find('a')['href']\n",
    "    link = \"https://www.nytimes.com\" + link\n",
    "    nyt_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = nyt_tags_archive[n].find('a').get_text()\n",
    "    nyt_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.time.attrs['datetime']\n",
    "    date = date[:-15]\n",
    "    nyt_dates.append(date)\n",
    "        \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', attrs = {'class':['css-53u6y8', 'css-1fanzo5 StoryBodyCompanionColumn']})\n",
    "    final_article = \" \".join([item.text for item in body])\n",
    "        \n",
    "    nyt_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling data\n",
    "nyt_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'new_york_times',\n",
    "    'date': nyt_dates,\n",
    "    'link': nyt_links,\n",
    "    'article_title': nyt_titles,\n",
    "    'article_text': nyt_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WASHINGTON — The Justice Department this week ...</td>\n",
       "      <td>Justice Dept. Religious Freedom Training Spurs...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>https://www.nytimes.com/2020/03/13/us/politics...</td>\n",
       "      <td>new_york_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>“Today I’d like to provide an update to the Am...</td>\n",
       "      <td>Transcript: Trump’s Coronavirus News Conferenc...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>https://www.nytimes.com/2020/03/13/us/politics...</td>\n",
       "      <td>new_york_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WASHINGTON — The Centers for Disease Control a...</td>\n",
       "      <td>Administration Offers Guidance to Schools as T...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>https://www.nytimes.com/2020/03/13/us/politics...</td>\n",
       "      <td>new_york_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tom Turnipseed, who after working on the presi...</td>\n",
       "      <td>Tom Turnipseed, a ‘Reformed Racist’ After Back...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>https://www.nytimes.com/2020/03/13/us/politics...</td>\n",
       "      <td>new_york_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WASHINGTON — Despite the worsening pandemic, t...</td>\n",
       "      <td>Amid a Pandemic, Trump Moves Forward With Safe...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>https://www.nytimes.com/2020/03/13/us/politics...</td>\n",
       "      <td>new_york_times</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         article_text  \\\n",
       "9   WASHINGTON — The Justice Department this week ...   \n",
       "10  “Today I’d like to provide an update to the Am...   \n",
       "11  WASHINGTON — The Centers for Disease Control a...   \n",
       "12  Tom Turnipseed, who after working on the presi...   \n",
       "13  WASHINGTON — Despite the worsening pandemic, t...   \n",
       "\n",
       "                                        article_title        date  \\\n",
       "9   Justice Dept. Religious Freedom Training Spurs...  2020-03-13   \n",
       "10  Transcript: Trump’s Coronavirus News Conferenc...  2020-03-13   \n",
       "11  Administration Offers Guidance to Schools as T...  2020-03-13   \n",
       "12  Tom Turnipseed, a ‘Reformed Racist’ After Back...  2020-03-13   \n",
       "13  Amid a Pandemic, Trump Moves Forward With Safe...  2020-03-13   \n",
       "\n",
       "                                                 link       publisher  \n",
       "9   https://www.nytimes.com/2020/03/13/us/politics...  new_york_times  \n",
       "10  https://www.nytimes.com/2020/03/13/us/politics...  new_york_times  \n",
       "11  https://www.nytimes.com/2020/03/13/us/politics...  new_york_times  \n",
       "12  https://www.nytimes.com/2020/03/13/us/politics...  new_york_times  \n",
       "13  https://www.nytimes.com/2020/03/13/us/politics...  new_york_times  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure it looks nice\n",
    "nyt_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 69\n",
      "total number of entries in new data: 83\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_nyt_data = pd.read_csv('data/nyt_data.csv')\n",
    "num_old = len(old_nyt_data)\n",
    "\n",
    "# append new data\n",
    "nyt_data = old_nyt_data.append(nyt_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "nyt_data.to_csv(\"data/nyt_data.csv\", index = False)\n",
    "num_now = len(nyt_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Politico - Liberal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "politico_request = requests.get('https://www.politico.com/politics')\n",
    "politico_homepage = politico_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "politico_soup = BeautifulSoup(politico_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate article URLs\n",
    "politico_tags = politico_soup.find_all('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "number_of_articles = len(politico_tags)\n",
    "\n",
    "# get article titles, content, and links\n",
    "politico_links = []\n",
    "politico_titles = []\n",
    "politico_dates = []\n",
    "politico_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get article titles, content, and links\n",
    "for n in np.arange(0, number_of_articles):\n",
    "\n",
    "    # get article link\n",
    "    link = politico_tags[n].find('a')['href']\n",
    "    politico_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = politico_tags[n].find('a').get_text()\n",
    "    politico_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.time.attrs['datetime']\n",
    "    date = date[:-9]\n",
    "    politico_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    body = soup_article.find_all('p', attrs={'class':'story-text__paragraph'})\n",
    "    final_article = \" \".join([item.text for item in body])\n",
    "    \n",
    "    politico_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling data\n",
    "politico_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'politico',\n",
    "    'date': politico_dates,\n",
    "    'link': politico_links,\n",
    "    'article_title': politico_titles,\n",
    "    'article_text': politico_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows that are not text articles (these will have NA in text)\n",
    "politico_data = politico_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the weeks since the coronavirus outbreak fi...</td>\n",
       "      <td>What Trump’s Twitter Feed Tells Him About the ...</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.politico.com/news/magazine/2020/03...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As Americans brace for the rapid spread of the...</td>\n",
       "      <td>Trump’s monumental challenge: Rescuing industr...</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.politico.com/news/2020/03/14/trump...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arkansas Republican Tom Cotton, a freshman sen...</td>\n",
       "      <td>Two senators take extreme measures to show fea...</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.politico.com/news/2020/03/14/cotto...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Local officials from around the country are wo...</td>\n",
       "      <td>Local officials: We’re not ready</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.politico.com/news/2020/03/14/healt...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joe Biden has long complained about the crowde...</td>\n",
       "      <td>Biden readies plan to finish off Bernie</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>https://www.politico.com/news/2020/03/14/biden...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  \\\n",
       "0  In the weeks since the coronavirus outbreak fi...   \n",
       "1  As Americans brace for the rapid spread of the...   \n",
       "2  Arkansas Republican Tom Cotton, a freshman sen...   \n",
       "3  Local officials from around the country are wo...   \n",
       "4  Joe Biden has long complained about the crowde...   \n",
       "\n",
       "                                       article_title        date  \\\n",
       "0  What Trump’s Twitter Feed Tells Him About the ...  2020-03-14   \n",
       "1  Trump’s monumental challenge: Rescuing industr...  2020-03-14   \n",
       "2  Two senators take extreme measures to show fea...  2020-03-14   \n",
       "3                   Local officials: We’re not ready  2020-03-14   \n",
       "4            Biden readies plan to finish off Bernie  2020-03-14   \n",
       "\n",
       "                                                link publisher  \n",
       "0  https://www.politico.com/news/magazine/2020/03...  politico  \n",
       "1  https://www.politico.com/news/2020/03/14/trump...  politico  \n",
       "2  https://www.politico.com/news/2020/03/14/cotto...  politico  \n",
       "3  https://www.politico.com/news/2020/03/14/healt...  politico  \n",
       "4  https://www.politico.com/news/2020/03/14/biden...  politico  "
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politico_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 69\n",
      "total number of entries in new data: 69\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_politico_data = pd.read_csv('data/politico_data.csv')\n",
    "num_old = len(old_politico_data)\n",
    "\n",
    "# append new data\n",
    "politico_data = old_politico_data.append(politico_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "politico_data.to_csv(\"data/politico_data.csv\", index = False)\n",
    "num_now = len(politico_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Buzzfeed - Very Liberal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "buzz_request = requests.get('https://www.buzzfeednews.com/section/politics')\n",
    "buzz_homepage = buzz_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "buzz_soup = BeautifulSoup(buzz_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate article URLs\n",
    "buzz_tags = buzz_soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "number_of_articles = min(len(buzz_tags), 30)\n",
    "\n",
    "# get article titles, content, and links\n",
    "buzz_links = []\n",
    "buzz_titles = []\n",
    "buzz_dates = []\n",
    "buzz_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get article titles, content, and links\n",
    "for n in np.arange(0, number_of_articles):\n",
    "\n",
    "    # get article link\n",
    "    link = buzz_tags[n].find('a')['href']\n",
    "    buzz_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = buzz_tags[n].find('a').get_text()\n",
    "    buzz_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.find_all('div', class_=\"news-article-header__timestamps\")    \n",
    "    date = \" \".join([item.text for item in date]).replace('\\n', '')\n",
    "    buzz_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', attrs={'data-module':'subbuzz-text'})\n",
    "    article = \" \".join([item.text for item in body]).replace('\\n', '')\n",
    "    final_article = re.sub(r' {[^}]*}', '', article)\n",
    "        \n",
    "    buzz_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling data\n",
    "buzz_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'buzzfeed',\n",
    "    'date': buzz_dates,\n",
    "    'link': buzz_links,\n",
    "    'article_title': buzz_titles,\n",
    "    'article_text': buzz_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WASHINGTON — When Washington state emerge...</td>\n",
       "      <td>A Senator From The State Hit Hardest By Corona...</td>\n",
       "      <td>Posted on March 13, 2020, at 9:28 ...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/kadiagoba...</td>\n",
       "      <td>buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WASHINGTON — After shaking hands with sev...</td>\n",
       "      <td>Trump Said He Won't Self-Isolate After Coming ...</td>\n",
       "      <td>Posted on March 13, 2020, at 5:41 ...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/paulmcleo...</td>\n",
       "      <td>buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On Thursday, a prominent Chinese diplomat...</td>\n",
       "      <td>Chinese Diplomats Are Pushing Conspiracy Theor...</td>\n",
       "      <td>Last updated on March 13, 2020, at...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/ryanhates...</td>\n",
       "      <td>buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>President Trump declared a national state...</td>\n",
       "      <td>Trump Declared A National State Of Emergency O...</td>\n",
       "      <td>Last updated on March 13, 2020, at...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/clarissaj...</td>\n",
       "      <td>buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thousands of Americans think President Do...</td>\n",
       "      <td>What Happens If Trump Tries To Cancel The Elec...</td>\n",
       "      <td>Posted on March 13, 2020, at 2:12 ...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/dominicho...</td>\n",
       "      <td>buzzfeed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  \\\n",
       "0       WASHINGTON — When Washington state emerge...   \n",
       "1       WASHINGTON — After shaking hands with sev...   \n",
       "2       On Thursday, a prominent Chinese diplomat...   \n",
       "3       President Trump declared a national state...   \n",
       "4       Thousands of Americans think President Do...   \n",
       "\n",
       "                                       article_title  \\\n",
       "0  A Senator From The State Hit Hardest By Corona...   \n",
       "1  Trump Said He Won't Self-Isolate After Coming ...   \n",
       "2  Chinese Diplomats Are Pushing Conspiracy Theor...   \n",
       "3  Trump Declared A National State Of Emergency O...   \n",
       "4  What Happens If Trump Tries To Cancel The Elec...   \n",
       "\n",
       "                                                date  \\\n",
       "0              Posted on March 13, 2020, at 9:28 ...   \n",
       "1              Posted on March 13, 2020, at 5:41 ...   \n",
       "2              Last updated on March 13, 2020, at...   \n",
       "3              Last updated on March 13, 2020, at...   \n",
       "4              Posted on March 13, 2020, at 2:12 ...   \n",
       "\n",
       "                                                link publisher  \n",
       "0  https://www.buzzfeednews.com/article/kadiagoba...  buzzfeed  \n",
       "1  https://www.buzzfeednews.com/article/paulmcleo...  buzzfeed  \n",
       "2  https://www.buzzfeednews.com/article/ryanhates...  buzzfeed  \n",
       "3  https://www.buzzfeednews.com/article/clarissaj...  buzzfeed  \n",
       "4  https://www.buzzfeednews.com/article/dominicho...  buzzfeed  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buzz_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 57\n",
      "total number of entries in new data: 62\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_buzz_data = pd.read_csv('data/buzzfeed_data.csv')\n",
    "num_old = len(old_buzz_data)\n",
    "\n",
    "# append new data\n",
    "buzz_data = old_buzz_data.append(buzz_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "buzz_data.to_csv(\"data/buzzfeed_data.csv\", index = False)\n",
    "num_now = len(buzz_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
