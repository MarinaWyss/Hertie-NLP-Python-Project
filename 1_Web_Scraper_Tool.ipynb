{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper Tool for 5 US Media Outlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Breitbart - Very Conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "breitbart_request = requests.get('https://www.breitbart.com/politics/')\n",
    "breitbart_homepage = breitbart_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "breitbart_soup = BeautifulSoup(breitbart_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# locate article URLs\n",
    "breitbart_tags = breitbart_soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_articles = min(len(breitbart_tags), 30)\n",
    "\n",
    "# get article titles, content, and links\n",
    "breitbart_links = []\n",
    "breitbart_titles = []\n",
    "breitbart_dates = []\n",
    "breitbart_contents = []\n",
    "\n",
    "for n in np.arange(0, number_of_articles):\n",
    "\n",
    "    # get article link\n",
    "    link = breitbart_tags[n].find('a')['href']\n",
    "    link = \"https://www.breitbart.com\" + link\n",
    "    breitbart_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = breitbart_tags[n].find('a').get_text()\n",
    "    breitbart_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.time.attrs['datetime']\n",
    "    date = date[:-10]\n",
    "    breitbart_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', class_='entry-content')\n",
    "    x = body[0].find_all('p')\n",
    "    \n",
    "    # combine paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    breitbart_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling data\n",
    "breitbart_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'Breitbart',\n",
    "    'date': breitbart_dates,\n",
    "    'link': breitbart_links,\n",
    "    'article_title': breitbart_titles,\n",
    "    'article_text': breitbart_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The American Conservative Union (ACU) announce...</td>\n",
       "      <td>ACU: CPAC Attendee Tested Positive for Coronav...</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>https://www.breitbart.com/politics/2020/03/07/...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Former Illinois Gov. Rod Blagojevich ripped in...</td>\n",
       "      <td>Exclusive — Rod Blagojevich Explains Why He Is...</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>https://www.breitbart.com/politics/2020/03/07/...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politico continued the media’s effort to trash...</td>\n",
       "      <td>Pollak: Politico Continues Media’s Quest to Bl...</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>https://www.breitbart.com/the-media/2020/03/07...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Friday’s “PBS NewsHour,” columnist Mark Shi...</td>\n",
       "      <td>Shields: Nobody Understands ‘What a Biden Pres...</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>https://www.breitbart.com/clips/2020/03/07/shi...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>On Saturday’s broadcast of the Fox News Channe...</td>\n",
       "      <td>Rick Scott: We Need ‘Way More Transparency’ on...</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>https://www.breitbart.com/clips/2020/03/07/ric...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  \\\n",
       "0  The American Conservative Union (ACU) announce...   \n",
       "1  Former Illinois Gov. Rod Blagojevich ripped in...   \n",
       "2  Politico continued the media’s effort to trash...   \n",
       "3  On Friday’s “PBS NewsHour,” columnist Mark Shi...   \n",
       "4  On Saturday’s broadcast of the Fox News Channe...   \n",
       "\n",
       "                                       article_title        date  \\\n",
       "0  ACU: CPAC Attendee Tested Positive for Coronav...  2020-03-07   \n",
       "1  Exclusive — Rod Blagojevich Explains Why He Is...  2020-03-07   \n",
       "2  Pollak: Politico Continues Media’s Quest to Bl...  2020-03-07   \n",
       "3  Shields: Nobody Understands ‘What a Biden Pres...  2020-03-07   \n",
       "4  Rick Scott: We Need ‘Way More Transparency’ on...  2020-03-07   \n",
       "\n",
       "                                                link  publisher  \n",
       "0  https://www.breitbart.com/politics/2020/03/07/...  Breitbart  \n",
       "1  https://www.breitbart.com/politics/2020/03/07/...  Breitbart  \n",
       "2  https://www.breitbart.com/the-media/2020/03/07...  Breitbart  \n",
       "3  https://www.breitbart.com/clips/2020/03/07/shi...  Breitbart  \n",
       "4  https://www.breitbart.com/clips/2020/03/07/ric...  Breitbart  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure it looks nice\n",
    "breitbart_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 60\n",
      "total number of entries in new data: 60\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_breitbart_data = pd.read_csv('data/breitbart_data.csv')\n",
    "num_old = len(old_breitbart_data)\n",
    "\n",
    "# append new data\n",
    "breitbart_data = old_breitbart_data.append(breitbart_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "breitbart_data.to_csv(\"data/breitbart_data.csv\", index = False)\n",
    "num_now = len(breitbart_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fox - Conservative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get links from the Fox Politics homepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.foxnews.com/category/politics/2020-presidential-election <- 127 links // 15  without junk\n",
    "# https://www.foxnews.com/politics <- 150 links (but junk?) // 31 without junk\n",
    "# i also tried running this for each candidate's section but there is overlap in articles\n",
    "\n",
    "# load the HTML content using requests and save into a variable\n",
    "r2 = requests.get('https://www.foxnews.com/politics')\n",
    "homepage2 = r2.content\n",
    "\n",
    "# create a soup to allow BeautifulSoup to work\n",
    "soup2 = BeautifulSoup(homepage2, 'html.parser')\n",
    "\n",
    "# locate and retrieve all links - WAS NOT ABLE TO ISOLATE JUST ARTICLE LINKS BECUASE NO UNIQUE TAG ELEMENT!!\n",
    "# you can see how complex the homepage is: print(soup2.prettify())\n",
    "homepage_tags2 = soup2.find_all('a')\n",
    "homepage_links2 = []\n",
    "\n",
    "for link in homepage_tags2:\n",
    "    homepage_links2.append(link.get('href'))\n",
    "\n",
    "# remove duplicates by turning list into a set\n",
    "homepage_links2 = set(homepage_links2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove junk links from list <- MAYBE THIS COULD BE DONE IN A BETTER WAY??\n",
    "# creating a list of exact junk links that will never be needed when pulling from the homepage\n",
    "junk_links2 = ['/', '/us', '/world', '/opinion', '/politics', '/entertainment', '//www.foxbusiness.com', \n",
    "                  '/lifestyle', '/shows', '//www.foxnews.com/shows/fox-nation', '//radio.foxnews.com/podcast',\n",
    "                  '#', '//foxnews.com/weather/your-weather/index.html', '#', '#', \n",
    "                  '//video.foxnews.com/v/5614615980001/?#sp=watch-live', '#', '/us', '/world', '/opinion', '/politics', \n",
    "                  '/official-polls', '/category/politics/elections', \n",
    "                  '/entertainment', '//video.foxnews.com/playlist/entertainment-latest-entertainment/', \n",
    "                  '//www.foxbusiness.com/', '//www.foxbusiness.com/markets', '//www.foxbusiness.com/politics', \n",
    "                  '//www.foxbusiness.com/category/technology', '//www.foxbusiness.com/features', \n",
    "                  '//www.foxbusiness.com/category/business-leaders', '/lifestyle', '/food-drink', '/auto', \n",
    "                  '/travel', '/family', '/science', '/tech', '/health', '/shows', '/shows', \n",
    "                  '/person/personalities', '//video.foxnews.com/v/5614615980001/?#sp=watch-live', \n",
    "                  '//video.foxnews.com/playlist/episodic-most-recent-episodes/', \n",
    "                  '//video.foxnews.com/#sp=show-clips', '//video.foxnews.com/#sp=news-clips', \n",
    "                  '//www.foxnews.com/contact', '//foxcareers.com/Search/SearchResults?brand=Fox%20News%20Careers', \n",
    "                  '/foxaroundtheworld/', 'mailto:adsales@foxnews.com?subject=Advertising%20Inquiry', \n",
    "                  '//press.foxnews.com/media-contacts/', '//press.foxnews.com/', '/compliance', \n",
    "                  'https://supplierdiversity.foxnews.com/', '//www.foxnews.com/shows/fox-nation', '//shop.foxnews.com', \n",
    "                  '/go', '//radio.foxnews.com/', '/alerts/subscribe', '/newsletter-signup/alerts', '//radio.foxnews.com/podcast', \n",
    "                  '/apps-products', '//www.foxnews.com', '/terms-of-use', '/privacy-policy', '/donotsell', '/closed-captioning', \n",
    "                  '//help.foxnews.com', '/contact', '//www.facebook.com/FoxNews', '//twitter.com/foxnews', '//www.google.com/+FoxNews', \n",
    "                  '//www.instagram.com/foxnews', '/about/rss/', '/alerts/subscribe', 'https://flipboard.com/@FoxNews', \n",
    "                  '//www.foxnews.com/rss/index.html', '//www.foxnews.com/alerts/subscribe.html', \n",
    "                   '/accessibility-statement', 'https://video.foxnews.com/v/', 'https://foxnews.com/elections']\n",
    "\n",
    "# removing all links that lead to section pages AND other specified junk URLs\n",
    "homepage_links2 = [x for x in homepage_links2 if \"/category\" not in x] \n",
    "homepage_links2 = [x for x in homepage_links2 if \"/v/\" not in x] \n",
    "homepage_links2 = [x for x in homepage_links2 if x not in junk_links2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.foxnews.com/media/chris-matthews-resignation-from-msnbc-hastened-by-series-of-blunders', 'https://www.foxnews.com/politics/warren-huddles-with-advisers-as-progressive-pressure-for-her-to-drop-out-mounts', 'https://www.foxnews.com/politics/supreme-court-at-apparent-odds-over-key-abortion-case-over-clinic-access-restrictions', 'https://www.foxnews.com/politics/fox-news-poll-sanders-knocks-biden-out-of-first-majority-thinks-trump-wins', 'https://www.foxnews.com/politics/issa-mounts-a-comeback-in-california', 'https://www.foxnews.com/politics/biden-campaign-hits-back-at-sanders-ad-showing-obama-praise', 'https://www.foxnews.com/media/francis-tarlov-joe-biden-trump-clinton', 'https://www.foxnews.com/politics/pence-says-passengers-on-flights-from-italy-and-south-korea-will-be-screened-multiple-times-for-coronavirus', 'https://www.foxnews.com/politics/mike-bloomberg-suspends-presidential-campaign-after-super-tuesday-show', 'https://www.foxnews.com/politics/aoc-aligned-progressive-candidates-fall-flat-in-super-tuesday-contests', 'https://www.foxnews.com/politics/texas-congressional-race-results', 'https://www.foxnews.com/politics/super-tuesday-results-where-the-2020-democratic-candidates-stand', 'https://www.foxnews.com/media/biden-shocks-the-pundits-again-with-surprisingly-strong-super-tuesday', 'https://www.foxnews.com/media/bidens-super-tuesday-challenge-after-defying-the-medias-obituaries', 'https://www.foxnews.com/politics/ex-cuban-prisoner-alan-gross-accuses-sanders-of-praising-cuba-during-visit', 'https://www.foxnews.com/media/markets-sigh-relief-bernie-falls-behind-biden', 'https://www.foxnews.com/politics/fox-news-poll-capitalism-socialism-bernie-sanders', 'https://www.foxnews.com/politics/biden-thumps-bernie', 'https://www.foxnews.com/opinion/newt-gingrich-50-hours-that-changed-the-2020-democratic-presidential-race', 'https://www.foxnews.com/politics/fox-news-voter-analysis-biden-sanders-emerge-from-dem-pack-on-super-tuesday', 'https://www.foxnews.com/politics/trump-calls-warren-selfish-for-staying-in-2020-democratic-contest', 'https://www.foxnews.com/politics/fox-news-poll-trump-economy-failed-to-unify']\n"
     ]
    }
   ],
   "source": [
    "# add https://www.foxnews.com to links that don't have this\n",
    "# create new list for prepared links\n",
    "hp_links2 = []\n",
    "\n",
    "for item in homepage_links2:\n",
    "    new_item = 'https://www.foxnews.com' + item\n",
    "    hp_links2.append(new_item)\n",
    "\n",
    "print(hp_links2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: check for duplicates to set of existing URLs to make sure we only pull data for new ones\n",
    "    # import full dataset in\n",
    "    # remove urls in homepage_links3 that are already included in the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get article text and other relevent data from new articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas dataframe where all of the retrieved data will be stored\n",
    "#article_data2 = pd.DataFrame()\n",
    "\n",
    "#article_data2['headline'] = []\n",
    "#article_data2['date_published'] = []\n",
    "#article_data2['date_modified'] = []\n",
    "#article_data2['description'] = []\n",
    "#article_data2['author'] = []\n",
    "#article_data2['url'] = []\n",
    "\n",
    "#export empty df to file\n",
    "#article_data2.to_csv('Fox_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "headline = []\n",
    "date_published = []\n",
    "date_modified = []\n",
    "description = []\n",
    "author = []\n",
    "url = []\n",
    "\n",
    "for link in hp_links2:\n",
    "    # load the HTML content using requests and save into a variable\n",
    "    r2_article = requests.get(link)\n",
    "    article2 = r2_article.content\n",
    "\n",
    "    # create a soups to allow BeautifulSoup to work\n",
    "    article_soup2 = BeautifulSoup(article2, 'html.parser')\n",
    "\n",
    "    # get article text\n",
    "    article_text = article_soup2.find(\"body\").get_text()\n",
    "    text.append(text)\n",
    "    \n",
    "    # retrieve article specific metadata\n",
    "    metadata2 = article_soup2.find_all(\"script\")[2].get_text()\n",
    "    metadata2 = metadata2.split(\",\")\n",
    "\n",
    "    for item in metadata2:\n",
    "        if 'headline' in item:\n",
    "            headline.append(item)\n",
    "        elif 'datePublished' in item:\n",
    "            date_published.append(item)\n",
    "        elif 'dateModified' in item:\n",
    "            date_modified.append(item)\n",
    "        elif 'description' in item:\n",
    "            description.append(item)\n",
    "        elif 'name' in item and 'Fox News' not in item:\n",
    "            author.append(item)\n",
    "        elif 'mainEntityOfPage' in item:\n",
    "            url.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 22\n",
      "headline 22\n",
      "date_published 22\n",
      "date_modified 22\n",
      "description 22\n",
      "author 21\n",
      "url 22\n"
     ]
    }
   ],
   "source": [
    "print('text', len(text))\n",
    "print('headline', len(headline))\n",
    "print('date_published', len(date_published))\n",
    "print('date_modified', len(date_modified))\n",
    "print('description', len(description))\n",
    "print('author', len(author))\n",
    "print('url', len(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-931919f5c54e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marticle_data2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mheadline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date_published'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdate_published\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date_modified'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdate_modified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'description'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'author'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'url'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    409\u001b[0m             )\n\u001b[1;32m    410\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         ]\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arrays must all be same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "#article_data2 = pd.DataFrame({'text':text,'title':headline, 'date_published':date_published, 'date_modified':date_modified, 'description':description, 'author':author, 'url':url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import collection of all responses\n",
    "all_articles2 = pd.read_csv(\"Fox_Data.csv\", index_col=[0])\n",
    "\n",
    "# merge new increment of data to the csv\n",
    "all_articles2 = article_data2.append(article_data2, ignore_index=True)\n",
    "\n",
    "#export with new increment to save\n",
    "all_articles.to_csv('Fox_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<meta content=\"Dana Blanton\" data-hid=\"dc.creator\" data-n-head=\"true\" name=\"dc.creator\" scheme=\"dcterms.creator\"/>\n"
     ]
    }
   ],
   "source": [
    "# another spot the author is located just in case\n",
    "# test = article_soup3.find_all(\"meta\")[17]\n",
    "#print(yay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Wall Street Journal - Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. New York Times - Liberal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "nyt_request = requests.get('https://www.nytimes.com/section/politics')\n",
    "nyt_homepage = nyt_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "nyt_soup = BeautifulSoup(nyt_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homepage URLs\n",
    "nyt_tags_home = nyt_soup.find_all('h2', class_=\"css-l2vidh e4e4i5l1\")\n",
    "\n",
    "# archive URLs\n",
    "nyt_tags_archive = nyt_soup.find_all('div', class_='css-1l4spti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup \n",
    "nyt_links = []\n",
    "nyt_titles = []\n",
    "nyt_dates = []\n",
    "nyt_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homepage articles\n",
    "for n in np.arange(0, len(nyt_tags_home)):\n",
    "\n",
    "    # get article link\n",
    "    link = nyt_tags_home[n].find('a')['href']\n",
    "    link = \"https://www.nytimes.com\" + link\n",
    "    nyt_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = nyt_tags_home[n].find('a').get_text()\n",
    "    nyt_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.time.attrs['datetime']\n",
    "    date = date[:-15]\n",
    "    nyt_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', {'class':['css-53u6y8', 'css-1fanzo5']})\n",
    "    final_article = \" \".join([item.text for item in body])\n",
    "        \n",
    "    nyt_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archive articles\n",
    "for n in np.arange(0, len(nyt_tags_archive)):\n",
    "\n",
    "    # get article link\n",
    "    link = nyt_tags_archive[n].find('a')['href']\n",
    "    link = \"https://www.nytimes.com\" + link\n",
    "    nyt_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = nyt_tags_archive[n].find('a').get_text()\n",
    "    nyt_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.time.attrs['datetime']\n",
    "    date = date[:-15]\n",
    "    nyt_dates.append(date)\n",
    "        \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', attrs = {'class':['css-53u6y8', 'css-1fanzo5 StoryBodyCompanionColumn']})\n",
    "    final_article = \" \".join([item.text for item in body])\n",
    "        \n",
    "    nyt_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling data\n",
    "nyt_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'new_york_times',\n",
    "    'date': nyt_dates,\n",
    "    'link': nyt_links,\n",
    "    'article_title': nyt_titles,\n",
    "    'article_text': nyt_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WEST PALM BEACH, Fla. — President Trump on Fri...</td>\n",
       "      <td>Trump Names Mark Meadows Chief of Staff, Ousti...</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>https://www.nytimes.com/2020/03/06/us/politics...</td>\n",
       "      <td>new_york_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>President Trump claimed again on Friday that a...</td>\n",
       "      <td>With Test Kits in Short Supply, Health Officia...</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>https://www.nytimes.com/2020/03/06/health/test...</td>\n",
       "      <td>new_york_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bernie Sanders was several takes into a video ...</td>\n",
       "      <td>The Bernie Sanders Personality Test</td>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>https://www.nytimes.com/2020/03/06/us/politics...</td>\n",
       "      <td>new_york_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joseph R. Biden Jr.’s campaign organization in...</td>\n",
       "      <td>Joe Biden Has Had Flimsy Organization. It Hasn...</td>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>https://www.nytimes.com/2020/03/06/us/politics...</td>\n",
       "      <td>new_york_times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FLINT, Mich. — Cornel West pleaded with his “o...</td>\n",
       "      <td>Sanders Is Behind With Black Voters. He Didn’t...</td>\n",
       "      <td>2020-03-08</td>\n",
       "      <td>https://www.nytimes.com/2020/03/08/us/politics...</td>\n",
       "      <td>new_york_times</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  \\\n",
       "0  WEST PALM BEACH, Fla. — President Trump on Fri...   \n",
       "1  President Trump claimed again on Friday that a...   \n",
       "2  Bernie Sanders was several takes into a video ...   \n",
       "3  Joseph R. Biden Jr.’s campaign organization in...   \n",
       "4  FLINT, Mich. — Cornel West pleaded with his “o...   \n",
       "\n",
       "                                       article_title        date  \\\n",
       "0  Trump Names Mark Meadows Chief of Staff, Ousti...  2020-03-07   \n",
       "1  With Test Kits in Short Supply, Health Officia...  2020-03-07   \n",
       "2                The Bernie Sanders Personality Test  2020-03-06   \n",
       "3  Joe Biden Has Had Flimsy Organization. It Hasn...  2020-03-06   \n",
       "4  Sanders Is Behind With Black Voters. He Didn’t...  2020-03-08   \n",
       "\n",
       "                                                link       publisher  \n",
       "0  https://www.nytimes.com/2020/03/06/us/politics...  new_york_times  \n",
       "1  https://www.nytimes.com/2020/03/06/health/test...  new_york_times  \n",
       "2  https://www.nytimes.com/2020/03/06/us/politics...  new_york_times  \n",
       "3  https://www.nytimes.com/2020/03/06/us/politics...  new_york_times  \n",
       "4  https://www.nytimes.com/2020/03/08/us/politics...  new_york_times  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure it looks nice\n",
    "nyt_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 14\n",
      "total number of entries in new data: 14\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_nyt_data = pd.read_csv('data/nyt_data.csv')\n",
    "num_old = len(old_nyt_data)\n",
    "\n",
    "# append new data\n",
    "nyt_data = old_nyt_data.append(nyt_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "nyt_data.to_csv(\"data/nyt_data.csv\", index = False)\n",
    "num_now = len(nyt_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
