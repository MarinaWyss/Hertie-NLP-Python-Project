{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper Tool for US Media Outlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import UnicodeDammit\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    \"\"\"\n",
    "    A class used to scrape articles from eight different U.S. media outlets.\n",
    "    \"\"\"\n",
    "    def scrape_breitbart():\n",
    "        \"\"\"\n",
    "        Scrapes new articles from breitbart.com/politics\n",
    "        return: pd.DataFrame\n",
    "        \"\"\"\n",
    "        # load the HTML content using requests and save into a variable\n",
    "        breitbart_request = requests.get('https://www.breitbart.com/politics/')\n",
    "        breitbart_homepage = breitbart_request.content\n",
    "\n",
    "        # create soup \n",
    "        breitbart_soup = BeautifulSoup(breitbart_homepage, 'html.parser')\n",
    "\n",
    "        # locate article URLs\n",
    "        breitbart_tags = breitbart_soup.find_all('h2')\n",
    "\n",
    "        # get article titles, content, dates, and links\n",
    "        breitbart_links = []\n",
    "        breitbart_titles = []\n",
    "        breitbart_dates = []\n",
    "        breitbart_contents = []\n",
    "\n",
    "        for n in np.arange(0, min(len(breitbart_tags), 30)):\n",
    "\n",
    "            # get article link\n",
    "            link = breitbart_tags[n].find('a')['href']\n",
    "            link = \"https://www.breitbart.com\" + link\n",
    "            breitbart_links.append(link)\n",
    "\n",
    "            # get article title\n",
    "            title = breitbart_tags[n].find('a').get_text()\n",
    "            breitbart_titles.append(title)\n",
    "\n",
    "            # prep article content\n",
    "            article = requests.get(link)\n",
    "            article_content = article.content\n",
    "            soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "\n",
    "            # get publication datetime\n",
    "            date = soup_article.time.attrs['datetime']\n",
    "            date = date[:-10]\n",
    "            breitbart_dates.append(date)\n",
    "\n",
    "            # get article content\n",
    "            body = soup_article.find_all('div', class_='entry-content')\n",
    "            x = body[0].find_all('p')\n",
    "\n",
    "            # combine paragraphs\n",
    "            list_paragraphs = []\n",
    "            for p in np.arange(0, len(x)):\n",
    "                paragraph = x[p].get_text()\n",
    "                list_paragraphs.append(paragraph)\n",
    "                final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "            breitbart_contents.append(final_article)\n",
    "\n",
    "        # assembling data\n",
    "        breitbart_data = pd.DataFrame.from_dict({\n",
    "            'publisher': 'Breitbart',\n",
    "            'date': breitbart_dates,\n",
    "            'link': breitbart_links,\n",
    "            'article_title': breitbart_titles,\n",
    "            'article_text': breitbart_contents \n",
    "        })\n",
    "\n",
    "        return breitbart_data\n",
    "        \n",
    "    def scrape_fox():\n",
    "        \"\"\"\n",
    "        Scrapes new articles from foxnews.com/politics\n",
    "        return: pd.DataFrame\n",
    "        \"\"\"\n",
    "        # load the HTML content using requests and save into a variable\n",
    "        fox_requests = requests.get('https://www.foxnews.com/politics')\n",
    "        fox_homepage = fox_requests.content\n",
    "\n",
    "        # create a soup to allow BeautifulSoup to work\n",
    "        fox_soup = BeautifulSoup(fox_homepage, 'html.parser')\n",
    "\n",
    "        # locate article links\n",
    "        fox_tags = fox_soup.find_all('article')\n",
    "\n",
    "        # get homepage article links\n",
    "        fox_links = []\n",
    "        fox_text = []\n",
    "        fox_titles = []\n",
    "        fox_dates = []\n",
    "\n",
    "        for n in np.arange(0, len(fox_tags)):\n",
    "            link = fox_tags[n].find('a')\n",
    "            link = link.get('href')\n",
    "            link = \"https://foxnews.com\" + link\n",
    "            fox_links.append(link)\n",
    "            fox_links = [x for x in fox_links if \"/v/\" not in x]\n",
    "            fox_links = [x for x in fox_links if \"https://foxnews.comhttps://www.foxnews.com\" not in x]\n",
    "\n",
    "        # prep for article content\n",
    "        for link in fox_links:\n",
    "            fox_article_request = requests.get(link)\n",
    "            fox_article = fox_article_request.content\n",
    "            fox_article_soup = BeautifulSoup(fox_article, 'html.parser')\n",
    "\n",
    "            # get article metadata\n",
    "            fox_metadata = fox_article_soup.find_all('script')[2].get_text()\n",
    "            fox_metadata = fox_metadata.split(\",\")\n",
    "\n",
    "            for item in fox_metadata:\n",
    "\n",
    "                # get article title\n",
    "                if 'headline' in item:\n",
    "                    item = item.replace('\\n',\"\")\n",
    "                    item = item.replace('headline', \"\")\n",
    "                    item = item.replace(':', \"\")\n",
    "                    item = item.replace('\"', '')\n",
    "                    fox_titles.append(item)\n",
    "\n",
    "                # get article date\n",
    "                elif 'datePublished' in item:\n",
    "                    item = item.replace('\\n',\"\")\n",
    "                    item = item.replace('datePublished', \"\")\n",
    "                    item = item.replace(':', \"\")\n",
    "                    item = item.replace('\"', '')\n",
    "                    fox_dates.append(item)\n",
    "\n",
    "            # get article text\n",
    "            body = fox_article_soup.find_all('div')\n",
    "            x = body[0].find_all('p')\n",
    "\n",
    "            # combine paragraphs\n",
    "            list_paragraphs = []\n",
    "            for p in np.arange(0, len(x)):\n",
    "                paragraph = x[p].get_text()\n",
    "                paragraph = paragraph.replace('\\n',\"\")\n",
    "                list_paragraphs.append(paragraph)\n",
    "\n",
    "                # removing copyright info and newsletter junk from the article\n",
    "                final_article = \" \".join(list_paragraphs)\n",
    "                final_article = final_article.replace(\"This material may not be published, broadcast, rewritten, or redistributed. ©2020 FOX News Network, LLC. All rights reserved. All market data delayed 20 minutes.\", \" \")\n",
    "                final_article = final_article.replace(\"This material may not be published, broadcast, rewritten,\", \" \")\n",
    "                final_article = final_article.replace(\"or redistributed. ©2020 FOX News Network, LLC. All rights reserved.\", \" \")\n",
    "                final_article = final_article.replace(\"All market data delayed 20 minutes.\", \" \")\n",
    "                final_article = final_article.replace(\"Get all the stories you need-to-know from the most powerful name in news delivered first thing every morning to your inbox Subscribed You've successfully subscribed to this newsletter!\", \" \")\n",
    "            fox_text.append(final_article)\n",
    "\n",
    "        # join fox data\n",
    "        fox_data = pd.DataFrame.from_dict({\n",
    "            'publisher': 'Fox',\n",
    "            'date': fox_dates,\n",
    "            'link': fox_links,\n",
    "            'article_title': fox_titles,\n",
    "            'article_text': fox_text \n",
    "        })\n",
    "        \n",
    "        return fox_data\n",
    "    \n",
    "    def scrape_wt():\n",
    "        \"\"\"\n",
    "        Scrapes new articles from washingtontimes.com/news/politics\n",
    "        return: pd.DataFrame\n",
    "        \"\"\"\n",
    "        # load the HTML content using requests and save into a variable\n",
    "        wt_request = requests.get('https://www.washingtontimes.com/news/politics/')\n",
    "        wt_homepage = wt_request.content\n",
    "\n",
    "        # create soup \n",
    "        wt_soup = BeautifulSoup(wt_homepage, 'html.parser')\n",
    "\n",
    "        # locate article URLs\n",
    "        wt_tags = wt_soup.find_all('h2', class_='article-headline')\n",
    "\n",
    "        # get article titles, content, dates, and links\n",
    "        wt_links = []\n",
    "        wt_titles = []\n",
    "        wt_dates = []\n",
    "        wt_contents = []\n",
    "\n",
    "        for n in np.arange(0, len(wt_tags)):\n",
    "\n",
    "            # get article link\n",
    "            link = wt_tags[n].find('a')['href']\n",
    "            link = 'https://www.washingtontimes.com' + link\n",
    "            wt_links.append(link)\n",
    "\n",
    "            # get article title\n",
    "            title = wt_tags[n].find('a').get_text()\n",
    "            wt_titles.append(title)\n",
    "    \n",
    "            # prep article content\n",
    "            article = requests.get(link)\n",
    "            article_content = article.content\n",
    "            soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "\n",
    "            # get publication datetime\n",
    "            meta = soup_article.find('div', class_='meta').find('span', class_='source').text\n",
    "            strip = meta.replace(' -\\n\\t\\t\\t\\n\\t\\t\\t\\tAssociated Press\\n -\\n                      \\n                        \\n                        ', '')\n",
    "            strip = strip.replace(' -\\n\\t\\t\\t\\n\\t\\t\\t\\tThe Washington Times\\n -\\n                      \\n                        \\n                        ', '')\n",
    "            date = strip.replace('\\n                      \\n                    ', '')\n",
    "            wt_dates.append(date)\n",
    "\n",
    "            # get article content\n",
    "            for div in soup_article.find_all('div', {'class':'article-toplinks'}):\n",
    "                div.decompose()\n",
    "\n",
    "            body = soup_article.find_all('div', class_= 'bigtext')  \n",
    "            x = body[0].find_all('p')\n",
    "\n",
    "            # combine paragraphs\n",
    "            list_paragraphs = []\n",
    "            for p in np.arange(0, len(x)):\n",
    "                paragraph = x[p].get_text()\n",
    "                list_paragraphs.append(paragraph)\n",
    "                final_article = \" \".join(list_paragraphs).split(\"\\n\")[0]\n",
    "\n",
    "            wt_contents.append(final_article)\n",
    "\n",
    "        # assembling data\n",
    "        wt_data = pd.DataFrame.from_dict({\n",
    "            'publisher': 'washington_times',\n",
    "            'date': wt_dates,\n",
    "            'link': wt_links,\n",
    "            'article_title': wt_titles,\n",
    "            'article_text': wt_contents \n",
    "        })\n",
    "\n",
    "        return wt_data\n",
    "\n",
    "    def scrape_ap():\n",
    "        \"\"\"\n",
    "        Scrapes new articles from apnews.com/apf-politics \n",
    "        return: pd.DataFrame\n",
    "        \"\"\" \n",
    "        # load the HTML content using requests and save into a variable\n",
    "        ap_requests = requests.get('https://apnews.com/apf-politics')\n",
    "        ap_homepage = ap_requests.content\n",
    "\n",
    "        # create a soup to allow BeautifulSoup to work\n",
    "        ap_soup = BeautifulSoup(ap_homepage, 'html.parser')\n",
    "\n",
    "        # locate articles\n",
    "        ap_tags = ap_soup.find_all('a', class_='Component-headline-0-2-106')\n",
    "\n",
    "        # get homepage article links\n",
    "        ap_links = []\n",
    "\n",
    "        for link in ap_tags:\n",
    "            link = link.get('href')\n",
    "            link = 'https://apnews.com' + link\n",
    "            ap_links.append(link)\n",
    "\n",
    "        # get article title, date, and content\n",
    "        ap_text = []\n",
    "        ap_titles = []\n",
    "        ap_dates = []\n",
    "\n",
    "        for link in ap_links:\n",
    "            ap_article_request = requests.get(link)\n",
    "            ap_article = ap_article_request.content\n",
    "            ap_article_soup = BeautifulSoup(ap_article, 'html.parser')\n",
    "\n",
    "            # article titles\n",
    "            title = ap_article_soup.find_all('meta')[14]\n",
    "            title = title['content']\n",
    "            ap_titles.append(title)\n",
    "\n",
    "            # article date\n",
    "            date = ap_article_soup.find_all('meta')[24]\n",
    "            date = date['content']\n",
    "            ap_dates.append(date)\n",
    "\n",
    "            # article content: <div class=\"Article\" data-key=Article.\n",
    "            body = ap_article_soup.find_all('div')\n",
    "            x = body[0].find_all('p')\n",
    "\n",
    "            # combine paragraphs\n",
    "            list_paragraphs = []\n",
    "            for p in np.arange(0, len(x)):\n",
    "                paragraph = x[p].get_text()\n",
    "                paragraph = paragraph.replace('\\n',\"\")\n",
    "                paragraph = paragraph.replace('CHICAGO (AP) -',\"\")\n",
    "                paragraph = paragraph.replace('DETROIT (AP) -',\"\")\n",
    "                paragraph = paragraph.replace('WASHINGTON (AP) -',\"\")\n",
    "                paragraph = paragraph.replace('___ Catch up on the 2020 election campaign with AP experts on our weekly politics podcast, “Ground Game.',\"\")\n",
    "                list_paragraphs.append(paragraph)\n",
    "                final_article = \" \".join(list_paragraphs)\n",
    "            ap_text.append(final_article)\n",
    "\n",
    "        # join ap data\n",
    "        ap_data = pd.DataFrame.from_dict({\n",
    "            'publisher': 'AP',\n",
    "            'date': ap_dates,\n",
    "            'link': ap_links,\n",
    "            'article_title': ap_titles,\n",
    "            'article_text': ap_text \n",
    "        })\n",
    "        \n",
    "        return ap_data \n",
    "        \n",
    "    def scrape_nbc():\n",
    "        \"\"\"\n",
    "        Scrapes new articles from nbcnews.com/politics \n",
    "        return: pd.DataFrame\n",
    "        \"\"\" \n",
    "        # load the HTML content using requests and save into a variable\n",
    "        nbc_request = requests.get('https://www.nbcnews.com/politics')\n",
    "        nbc_homepage = nbc_request.content\n",
    "\n",
    "        # create soup \n",
    "        nbc_soup = BeautifulSoup(nbc_homepage, 'html.parser')\n",
    "\n",
    "        # locate article URLs\n",
    "        nbc_tags = nbc_soup.find_all('h2', class_='teaseCard__headline') + nbc_soup.find_all('h2', class_='title___2T5qK')\n",
    "\n",
    "        # get article titles, content, dates, and links\n",
    "        nbc_links = []\n",
    "        nbc_titles = []\n",
    "        nbc_dates = []\n",
    "        nbc_contents = []\n",
    "\n",
    "        for n in np.arange(0, len(nbc_tags)):\n",
    "\n",
    "            # get article link\n",
    "            link = nbc_tags[n].find('a')['href']\n",
    "            nbc_links.append(link)\n",
    "\n",
    "            # get article title\n",
    "            title = nbc_tags[n].find('a').get_text()\n",
    "            nbc_titles.append(title)\n",
    "\n",
    "            # prep article content\n",
    "            article = requests.get(link)\n",
    "            article_content = article.content\n",
    "            soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "\n",
    "            # get publication datetime\n",
    "            if soup_article.time != None:\n",
    "                date = soup_article.time.attrs['datetime']\n",
    "                date = date[4:-24] \n",
    "            else:\n",
    "                date = None\n",
    "            nbc_dates.append(date)\n",
    "\n",
    "            # get article content\n",
    "            body = soup_article.find_all('div', class_= 'article-body__content')    \n",
    "            final_article = \" \".join([item.text for item in body])\n",
    "\n",
    "            nbc_contents.append(final_article)\n",
    "\n",
    "        # assembling data\n",
    "        nbc_data = pd.DataFrame.from_dict({\n",
    "            'publisher': 'nbc',\n",
    "            'date': nbc_dates,\n",
    "            'link': nbc_links,\n",
    "            'article_title': nbc_titles,\n",
    "            'article_text': nbc_contents \n",
    "        })\n",
    "\n",
    "        # dropping rows that are not text articles (these will have NA in date)\n",
    "        nbc_data = nbc_data.dropna()\n",
    "        \n",
    "        return nbc_data\n",
    "    \n",
    "    def scrape_nyt():\n",
    "        \"\"\"\n",
    "        Scrapes new articles from nytimes.com/section/politics \n",
    "        return: pd.DataFrame\n",
    "        \"\"\" \n",
    "        # load the HTML content using requests and save into a variable\n",
    "        nyt_request = requests.get('https://www.nytimes.com/section/politics')\n",
    "        nyt_homepage = nyt_request.content\n",
    "\n",
    "        # create soup \n",
    "        nyt_soup = BeautifulSoup(nyt_homepage, 'html.parser')\n",
    "\n",
    "        # homepage URLs\n",
    "        nyt_tags_home = nyt_soup.find_all('h2', class_='css-l2vidh e4e4i5l1')\n",
    "\n",
    "        # setup \n",
    "        nyt_links = []\n",
    "        nyt_titles = []\n",
    "        nyt_dates = []\n",
    "        nyt_contents = []\n",
    "\n",
    "        # articles, links, titles, and content\n",
    "        for n in np.arange(0, len(nyt_tags_home)):\n",
    "\n",
    "            # get article link\n",
    "            link = nyt_tags_home[n].find('a')['href']\n",
    "            link = \"https://www.nytimes.com\" + link\n",
    "            nyt_links.append(link)\n",
    "\n",
    "            # get article title\n",
    "            title = nyt_tags_home[n].find('a').get_text()\n",
    "            nyt_titles.append(title)\n",
    "\n",
    "            # prep article content\n",
    "            article = requests.get(link)\n",
    "            article_content = article.content\n",
    "            soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "\n",
    "            # get publication datetime\n",
    "            date = soup_article.time.attrs['datetime']\n",
    "            date = date[:-15]\n",
    "            nyt_dates.append(date)\n",
    "\n",
    "            # get article content\n",
    "            for div in soup_article.find_all(\"div\", {'class': 'css-9tf9ac'}):\n",
    "                div.decompose()\n",
    "\n",
    "            body = soup_article.find_all('div', {'class':['css-53u6y8', 'css-1fanzo5']})\n",
    "            final_article = \" \".join([item.text for item in body])\n",
    "\n",
    "            nyt_contents.append(final_article)\n",
    "            \n",
    "        # assembling data\n",
    "        nyt_data = pd.DataFrame.from_dict({\n",
    "            'publisher': 'new_york_times',\n",
    "            'date': nyt_dates,\n",
    "            'link': nyt_links,\n",
    "            'article_title': nyt_titles,\n",
    "            'article_text': nyt_contents \n",
    "        })\n",
    "        \n",
    "        return nyt_data\n",
    "    \n",
    "    def scrape_politico():\n",
    "        \"\"\"\n",
    "        Scrapes new articles from politico.com/politics\n",
    "        return: pd.DataFrame\n",
    "        \"\"\" \n",
    "        # load the HTML content using requests and save into a variable\n",
    "        politico_request = requests.get('https://www.politico.com/politics')\n",
    "        politico_homepage = politico_request.content\n",
    "\n",
    "        # create soup \n",
    "        politico_soup = BeautifulSoup(politico_homepage, 'html.parser')\n",
    "\n",
    "        # locate article URLs\n",
    "        politico_tags = politico_soup.find_all('h3')\n",
    "\n",
    "        # get article titles, content, dates, and links\n",
    "        politico_links = []\n",
    "\n",
    "        for n in np.arange(0, len(politico_tags)):\n",
    "            # get article link\n",
    "            link = politico_tags[n].find('a')['href']\n",
    "            if \"/news/\" in link:\n",
    "                politico_links.append(link)\n",
    "\n",
    "        politico_titles = []\n",
    "        politico_dates = []\n",
    "        politico_contents = []\n",
    "\n",
    "        for link in politico_links:\n",
    "\n",
    "            # prep article content\n",
    "            article = requests.get(link)\n",
    "            article_content = article.content\n",
    "            soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "\n",
    "            # get article title\n",
    "            title = soup_article.find('h2', attrs={'class':'headline'}).get_text()\n",
    "            politico_titles.append(title)\n",
    "\n",
    "            # get publication datetime\n",
    "            date = soup_article.time.attrs['datetime']\n",
    "            date = date[:-9]\n",
    "            politico_dates.append(date)\n",
    "\n",
    "            # get article content\n",
    "            body = soup_article.find_all('p', attrs={'class':'story-text__paragraph'})\n",
    "            final_article = \" \".join([item.text for item in body])\n",
    "\n",
    "            politico_contents.append(final_article)\n",
    "\n",
    "        # assembling data\n",
    "        politico_data = pd.DataFrame.from_dict({\n",
    "            'publisher': 'politico',\n",
    "            'date': politico_dates,\n",
    "            'link': politico_links,\n",
    "            'article_title': politico_titles,\n",
    "            'article_text': politico_contents \n",
    "        })\n",
    "        \n",
    "        return politico\n",
    "    \n",
    "    def scrape_buzzfeed():\n",
    "        \"\"\"\n",
    "        Scrapes new articles from buzzfeednews.com/section/politics\n",
    "        return: pd.DataFrame\n",
    "        \"\"\" \n",
    "        # load the HTML content using requests and save into a variable\n",
    "        buzz_request = requests.get('https://www.buzzfeednews.com/section/politics')\n",
    "        buzz_homepage = buzz_request.content\n",
    "\n",
    "        # create soup\n",
    "        buzz_soup = BeautifulSoup(buzz_homepage, 'html.parser')\n",
    "\n",
    "        # locate article URLs\n",
    "        buzz_tags = buzz_soup.find_all('h2')\n",
    "\n",
    "        # get article titles, content, dates, and links\n",
    "        buzz_links = []\n",
    "        buzz_titles = []\n",
    "        buzz_dates = []\n",
    "        buzz_contents = []\n",
    "\n",
    "        for n in np.arange(0, min(len(buzz_tags), 30)):\n",
    "\n",
    "            # get article link\n",
    "            link = buzz_tags[n].find('a')['href']\n",
    "            buzz_links.append(link)\n",
    "\n",
    "            # get article title\n",
    "            title = buzz_tags[n].find('a').get_text()\n",
    "            buzz_titles.append(title)\n",
    "\n",
    "            # prep article content\n",
    "            article = requests.get(link)\n",
    "            article_content = article.content\n",
    "            soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "\n",
    "            # get publication datetime\n",
    "            date = soup_article.find_all('div', class_='news-article-header__timestamps')\n",
    "            date = \" \".join([item.text for item in date]).replace('\\n', '')\n",
    "            buzz_dates.append(date)\n",
    "\n",
    "            # get article content\n",
    "            body = soup_article.find_all('div', attrs={'data-module':'subbuzz-text'})\n",
    "            article = \" \".join([item.text for item in body]).replace('\\n', '')\n",
    "            final_article = re.sub(r' {[^}]*}', '', article)\n",
    "\n",
    "            buzz_contents.append(final_article)\n",
    "\n",
    "        # assembling data\n",
    "        buzz_data = pd.DataFrame.from_dict({\n",
    "            'publisher': 'buzzfeed',\n",
    "            'date': buzz_dates,\n",
    "            'link': buzz_links,\n",
    "            'article_title': buzz_titles,\n",
    "            'article_text': buzz_contents \n",
    "        })\n",
    "\n",
    "    def save_data(outlet, scraped_df):\n",
    "        \"\"\"\n",
    "        Concatenates scraped data to old df and saves new data set\n",
    "        return: pd.DataFrame\n",
    "        \"\"\"\n",
    "        # read in old data\n",
    "        old_data = pd.read_csv('data/' + outlet + '_data.csv')\n",
    "        num_old = len(old_data)\n",
    "\n",
    "        # append new data\n",
    "        new_data = old_data.append(scraped_df).drop_duplicates()\n",
    "\n",
    "        # save new .csv\n",
    "        new_data.to_csv('data/' + outlet + '_data.csv', index = False)\n",
    "        num_now = len(new_data)\n",
    "\n",
    "        print(\"number of entries in old {} data: {}\".format(outlet, num_old))\n",
    "        print(\"total number of entries in new {} data: {}\".format(outlet, num_now))\n",
    "        print(\"difference: {}\".format(num_now - num_old))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old nyt data: 230\n",
      "total number of entries in new nyt data: 230\n",
      "difference: 0\n"
     ]
    }
   ],
   "source": [
    "# use example\n",
    "nyt_data = WebScraper.scrape_nyt()\n",
    "WebScraper.save_data(\"nyt\", nyt_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
