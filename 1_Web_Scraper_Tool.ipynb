{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper Tool for 5 US Media Outlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import UnicodeDammit\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Breitbart - Very Conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "breitbart_request = requests.get('https://www.breitbart.com/politics/')\n",
    "breitbart_homepage = breitbart_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "breitbart_soup = BeautifulSoup(breitbart_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# locate article URLs\n",
    "breitbart_tags = breitbart_soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "number_of_articles = min(len(breitbart_tags), 30)\n",
    "\n",
    "breitbart_links = []\n",
    "breitbart_titles = []\n",
    "breitbart_dates = []\n",
    "breitbart_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get article titles, content, and links\n",
    "for n in np.arange(0, number_of_articles):\n",
    "\n",
    "    # get article link\n",
    "    link = breitbart_tags[n].find('a')['href']\n",
    "    link = \"https://www.breitbart.com\" + link\n",
    "    breitbart_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = breitbart_tags[n].find('a').get_text()\n",
    "    breitbart_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.time.attrs['datetime']\n",
    "    date = date[:-10]\n",
    "    breitbart_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', class_='entry-content')\n",
    "    x = body[0].find_all('p')\n",
    "    \n",
    "    # combine paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    breitbart_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling data\n",
    "breitbart_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'Breitbart',\n",
    "    'date': breitbart_dates,\n",
    "    'link': breitbart_links,\n",
    "    'article_title': breitbart_titles,\n",
    "    'article_text': breitbart_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publisher</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>https://www.breitbart.com/middle-east/2020/03/...</td>\n",
       "      <td>Israel Slams Russian FM for Meeting with Islam...</td>\n",
       "      <td>TEL AVIV – Russia’s top diplomat met with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>https://www.breitbart.com/politics/2020/03/12/...</td>\n",
       "      <td>Joe Biden to Fight Coronavirus by Rejoining Pa...</td>\n",
       "      <td>Former Vice President Joe Biden proposed his p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>https://www.breitbart.com/politics/2020/03/12/...</td>\n",
       "      <td>NY Gov. Andrew Cuomo Bans Gatherings of over 5...</td>\n",
       "      <td>New York Gov. Andrew Cuomo (D) on Thursday ann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>https://www.breitbart.com/politics/2020/03/12/...</td>\n",
       "      <td>Tammy Duckworth, Doug Jones Praise Trump’s E.U...</td>\n",
       "      <td>A pair of Democrat senators offered rare prais...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>https://www.breitbart.com/politics/2020/03/12/...</td>\n",
       "      <td>New York City Declares State of Emergency Over...</td>\n",
       "      <td>New York City Mayor Bill de Blasio (D) on Thur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publisher        date                                               link  \\\n",
       "0  Breitbart  2020-03-12  https://www.breitbart.com/middle-east/2020/03/...   \n",
       "1  Breitbart  2020-03-12  https://www.breitbart.com/politics/2020/03/12/...   \n",
       "2  Breitbart  2020-03-12  https://www.breitbart.com/politics/2020/03/12/...   \n",
       "3  Breitbart  2020-03-12  https://www.breitbart.com/politics/2020/03/12/...   \n",
       "4  Breitbart  2020-03-12  https://www.breitbart.com/politics/2020/03/12/...   \n",
       "\n",
       "                                       article_title  \\\n",
       "0  Israel Slams Russian FM for Meeting with Islam...   \n",
       "1  Joe Biden to Fight Coronavirus by Rejoining Pa...   \n",
       "2  NY Gov. Andrew Cuomo Bans Gatherings of over 5...   \n",
       "3  Tammy Duckworth, Doug Jones Praise Trump’s E.U...   \n",
       "4  New York City Declares State of Emergency Over...   \n",
       "\n",
       "                                        article_text  \n",
       "0  TEL AVIV – Russia’s top diplomat met with the ...  \n",
       "1  Former Vice President Joe Biden proposed his p...  \n",
       "2  New York Gov. Andrew Cuomo (D) on Thursday ann...  \n",
       "3  A pair of Democrat senators offered rare prais...  \n",
       "4  New York City Mayor Bill de Blasio (D) on Thur...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure it looks nice\n",
    "breitbart_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 120\n",
      "total number of entries in new data: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_breitbart_data = pd.read_csv('data/breitbart_data.csv')\n",
    "num_old = len(old_breitbart_data)\n",
    "\n",
    "# append new data\n",
    "breitbart_data = old_breitbart_data.append(breitbart_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "breitbart_data.to_csv(\"data/breitbart_data.csv\", index = False)\n",
    "num_now = len(breitbart_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fox - Conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "fox_requests = requests.get('https://www.foxnews.com/politics')\n",
    "fox_homepage = fox_requests.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a soup to allow BeautifulSoup to work\n",
    "fox_soup = BeautifulSoup(fox_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate article links\n",
    "fox_tags = fox_soup.find_all('article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "fox_links = []\n",
    "fox_text = []\n",
    "fox_titles = []\n",
    "fox_dates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_articles = 30\n",
    "\n",
    "# get homepage article links\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    link = fox_tags[n].find('a')\n",
    "    link = link.get('href')\n",
    "    link = \"https://foxnews.com\" + link\n",
    "    fox_links.append(link)\n",
    "    fox_links = [x for x in fox_links if \"/v/\" not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep for article content\n",
    "for link in fox_links:\n",
    "    fox_article_request = requests.get(link)\n",
    "    fox_article = fox_article_request.content\n",
    "    fox_article_soup = BeautifulSoup(fox_article, 'html.parser')\n",
    "    \n",
    "    # get article metadata\n",
    "    fox_metadata = fox_article_soup.find_all('script')[2].get_text()\n",
    "    fox_metadata = fox_metadata.split(\",\")\n",
    "    \n",
    "    for item in fox_metadata:\n",
    "\n",
    "        # get article title\n",
    "        if 'headline' in item:\n",
    "            item = item.replace('\\n',\"\")\n",
    "            item = item.replace('headline', \"\")\n",
    "            item = item.replace(':', \"\")\n",
    "            item = item.replace('\"', '')\n",
    "            fox_titles.append(item)\n",
    "        \n",
    "        # get article date\n",
    "        elif 'datePublished' in item:\n",
    "            item = item.replace('\\n',\"\")\n",
    "            item = item.replace('datePublished', \"\")\n",
    "            item = item.replace(':', \"\")\n",
    "            item = item.replace('\"', '')\n",
    "            fox_dates.append(item)\n",
    "    \n",
    "    # get article text\n",
    "    body = fox_article_soup.find_all('div')\n",
    "    x = body[0].find_all('p')\n",
    "    \n",
    "    # combine paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        paragraph = paragraph.replace('\\n',\"\")\n",
    "        list_paragraphs.append(paragraph)\n",
    "        \n",
    "        # removing copyright info and newsletter junk from the article\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        final_article = final_article.replace(\"This material may not be published, broadcast, rewritten, or redistributed. ©2020 FOX News Network, LLC. All rights reserved. All market data delayed 20 minutes.\", \" \")\n",
    "        final_article = final_article.replace(\"This material may not be published, broadcast, rewritten,\", \" \")\n",
    "        final_article = final_article.replace(\"or redistributed. ©2020 FOX News Network, LLC. All rights reserved.\", \" \")\n",
    "        final_article = final_article.replace(\"All market data delayed 20 minutes.\", \" \")\n",
    "        final_article = final_article.replace(\"Get all the stories you need-to-know from the most powerful name in news delivered first thing every morning to your inbox Subscribed You've successfully subscribed to this newsletter!\", \" \")\n",
    "    fox_text.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publisher</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Fox</td>\n",
       "      <td>2020-03-12T121511-0400</td>\n",
       "      <td>https://foxnews.com/politics/trump-shrugs-off-...</td>\n",
       "      <td>Trump shrugs off EU anger over corona...</td>\n",
       "      <td>Dr. Janette Nesheiwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Fox</td>\n",
       "      <td>2020-03-12T135537-0400</td>\n",
       "      <td>https://foxnews.com/politics/biden-campaign-ma...</td>\n",
       "      <td>Biden campaign shakeup Veteran of Beto</td>\n",
       "      <td>Joe Biden lays out h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Fox</td>\n",
       "      <td>2020-03-12T101059-0400</td>\n",
       "      <td>https://foxnews.com/politics/coronavirus-impac...</td>\n",
       "      <td>Coronavirus impact on federal governm...</td>\n",
       "      <td>World Health Organiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Fox</td>\n",
       "      <td>2020-03-12T150522-0400</td>\n",
       "      <td>https://foxnews.com/politics/supreme-court-clo...</td>\n",
       "      <td>Supreme Court closes to the public am...</td>\n",
       "      <td>The coronavirs and i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Fox</td>\n",
       "      <td>2020-03-12T093115-0400</td>\n",
       "      <td>https://foxnews.com/politics/march-17-primarie...</td>\n",
       "      <td>March 17 primaries Here are the state...</td>\n",
       "      <td>Fox News contributor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publisher                             date  \\\n",
       "0       Fox           2020-03-12T121511-0400   \n",
       "1       Fox           2020-03-12T135537-0400   \n",
       "2       Fox           2020-03-12T101059-0400   \n",
       "3       Fox           2020-03-12T150522-0400   \n",
       "4       Fox           2020-03-12T093115-0400   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://foxnews.com/politics/trump-shrugs-off-...   \n",
       "1  https://foxnews.com/politics/biden-campaign-ma...   \n",
       "2  https://foxnews.com/politics/coronavirus-impac...   \n",
       "3  https://foxnews.com/politics/supreme-court-clo...   \n",
       "4  https://foxnews.com/politics/march-17-primarie...   \n",
       "\n",
       "                                       article_title  \\\n",
       "0           Trump shrugs off EU anger over corona...   \n",
       "1             Biden campaign shakeup Veteran of Beto   \n",
       "2           Coronavirus impact on federal governm...   \n",
       "3           Supreme Court closes to the public am...   \n",
       "4           March 17 primaries Here are the state...   \n",
       "\n",
       "                                        article_text  \n",
       "0                            Dr. Janette Nesheiwa...  \n",
       "1                            Joe Biden lays out h...  \n",
       "2                            World Health Organiz...  \n",
       "3                            The coronavirs and i...  \n",
       "4                            Fox News contributor...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join fox data\n",
    "fox_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'Fox',\n",
    "    'date': fox_dates,\n",
    "    'link': fox_links,\n",
    "    'article_title': fox_titles,\n",
    "    'article_text': fox_text \n",
    "})\n",
    "\n",
    "fox_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in old data\n",
    "old_fox_data = pd.read_csv('data/fox_data.csv')\n",
    "num_old = len(old_fox_data)\n",
    "\n",
    "# append new data\n",
    "fox_data = old_fox_data.append(fox_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "fox_data.to_csv(\"data/fox_data.csv\", index = False)\n",
    "num_now = len(fox_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 50\n",
      "total number of entries in new data: 67\n",
      "difference: 17\n"
     ]
    }
   ],
   "source": [
    "# see number of articles\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))\n",
    "print(\"difference: {}\".format(num_now-num_old))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Associated Press - Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "ap_requests = requests.get('https://apnews.com/apf-politics')\n",
    "ap_homepage = ap_requests.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a soup to allow BeautifulSoup to work\n",
    "ap_soup = BeautifulSoup(ap_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate articles\n",
    "ap_tags = ap_soup.find_all('a', class_=\"Component-headline-0-2-105\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "number_of_articles = min(len(ap_tags), 30)\n",
    "\n",
    "ap_links = []\n",
    "ap_text = []\n",
    "ap_titles = []\n",
    "ap_dates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get homepage article links\n",
    "for link in ap_tags:\n",
    "    link = link.get('href')\n",
    "    link = \"https://apnews.com\" + link\n",
    "    ap_links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep for article content\n",
    "for link in ap_links:\n",
    "    ap_article_request = requests.get(link)\n",
    "    ap_article = ap_article_request.content\n",
    "    ap_article_soup = BeautifulSoup(ap_article, 'html.parser')\n",
    "    \n",
    "    # article titles\n",
    "    title = ap_article_soup.find_all('meta')[14]\n",
    "    title = title['content']\n",
    "    ap_titles.append(title)\n",
    "    \n",
    "    # article date\n",
    "    date = ap_article_soup.find_all('meta')[24]\n",
    "    date = date['content']\n",
    "    ap_dates.append(date)\n",
    "    \n",
    "    # article content: <div class=\"Article\" data-key=Article.\n",
    "    body = ap_article_soup.find_all('div')\n",
    "    x = body[0].find_all('p')\n",
    "\n",
    "    # combine paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        paragraph = paragraph.replace('\\n',\"\")\n",
    "        paragraph = paragraph.replace('CHICAGO (AP) -',\"\")\n",
    "        paragraph = paragraph.replace('DETROIT (AP) -',\"\")\n",
    "        paragraph = paragraph.replace('WASHINGTON (AP) -',\"\")\n",
    "        paragraph = paragraph.replace('___ Catch up on the 2020 election campaign with AP experts on our weekly politics podcast, “Ground Game.',\"\")\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "    ap_text.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publisher</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AP</td>\n",
       "      <td>2020-03-12T18:28:26Z</td>\n",
       "      <td>https://apnews.com/5878d878b1c1b5c3274c529ed2b...</td>\n",
       "      <td>Trump economic team grasps for credibility wit...</td>\n",
       "      <td>WASHINGTON (AP) — During the financial crisis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AP</td>\n",
       "      <td>2020-03-12T04:56:31Z</td>\n",
       "      <td>https://apnews.com/6052e677cfc38b76224eccddb9f...</td>\n",
       "      <td>Washington strains for virus response as insti...</td>\n",
       "      <td>WASHINGTON (AP) — Washington is straining for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>AP</td>\n",
       "      <td>2020-03-12T16:46:03Z</td>\n",
       "      <td>https://apnews.com/f20e530ae6dfe4926f81a119054...</td>\n",
       "      <td>Brazilian who met Trump has virus; no plans to...</td>\n",
       "      <td>WASHINGTON (AP) — A senior Brazilian official ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AP</td>\n",
       "      <td>2020-03-12T17:05:37Z</td>\n",
       "      <td>https://apnews.com/4ab8fb97f6434dfd442bb2c8c3f...</td>\n",
       "      <td>Debate moves from Phoenix to DC over coronavir...</td>\n",
       "      <td>WASHINGTON (AP) — The Democratic National Comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AP</td>\n",
       "      <td>2020-03-12T15:57:45Z</td>\n",
       "      <td>https://apnews.com/f6d9c0b8d9504b5708a89718e56...</td>\n",
       "      <td>Biden, Sanders offer contrasts to Trump during...</td>\n",
       "      <td>WILMINGTON, Del. (AP) — Democratic presidentia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publisher                  date  \\\n",
       "0        AP  2020-03-12T18:28:26Z   \n",
       "1        AP  2020-03-12T04:56:31Z   \n",
       "2        AP  2020-03-12T16:46:03Z   \n",
       "3        AP  2020-03-12T17:05:37Z   \n",
       "4        AP  2020-03-12T15:57:45Z   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://apnews.com/5878d878b1c1b5c3274c529ed2b...   \n",
       "1  https://apnews.com/6052e677cfc38b76224eccddb9f...   \n",
       "2  https://apnews.com/f20e530ae6dfe4926f81a119054...   \n",
       "3  https://apnews.com/4ab8fb97f6434dfd442bb2c8c3f...   \n",
       "4  https://apnews.com/f6d9c0b8d9504b5708a89718e56...   \n",
       "\n",
       "                                       article_title  \\\n",
       "0  Trump economic team grasps for credibility wit...   \n",
       "1  Washington strains for virus response as insti...   \n",
       "2  Brazilian who met Trump has virus; no plans to...   \n",
       "3  Debate moves from Phoenix to DC over coronavir...   \n",
       "4  Biden, Sanders offer contrasts to Trump during...   \n",
       "\n",
       "                                        article_text  \n",
       "0  WASHINGTON (AP) — During the financial crisis ...  \n",
       "1  WASHINGTON (AP) — Washington is straining for ...  \n",
       "2  WASHINGTON (AP) — A senior Brazilian official ...  \n",
       "3  WASHINGTON (AP) — The Democratic National Comm...  \n",
       "4  WILMINGTON, Del. (AP) — Democratic presidentia...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join ap data\n",
    "ap_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'AP',\n",
    "    'date': ap_dates,\n",
    "    'link': ap_links,\n",
    "    'article_title': ap_titles,\n",
    "    'article_text': ap_text \n",
    "})\n",
    "\n",
    "ap_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in old data\n",
    "old_ap_data = pd.read_csv('data/ap_data.csv')\n",
    "num_old = len(old_ap_data)\n",
    "\n",
    "# append new data\n",
    "ap_data = old_ap_data.append(ap_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "ap_data.to_csv(\"data/ap_data.csv\", index = False)\n",
    "num_now = len(ap_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries in old data: 66\n",
      "total number of entries in new data: 115\n",
      "difference: 49\n"
     ]
    }
   ],
   "source": [
    "# see number of articles\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))\n",
    "print(\"difference: {}\".format(num_now-num_old))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. New York Times - Liberal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "nyt_request = requests.get('https://www.nytimes.com/section/politics')\n",
    "nyt_homepage = nyt_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "nyt_soup = BeautifulSoup(nyt_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homepage URLs\n",
    "nyt_tags_home = nyt_soup.find_all('h2', class_=\"css-l2vidh e4e4i5l1\")\n",
    "\n",
    "# archive URLs\n",
    "nyt_tags_archive = nyt_soup.find_all('div', class_='css-1l4spti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup \n",
    "nyt_links = []\n",
    "nyt_titles = []\n",
    "nyt_dates = []\n",
    "nyt_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homepage articles\n",
    "for n in np.arange(0, len(nyt_tags_home)):\n",
    "\n",
    "    # get article link\n",
    "    link = nyt_tags_home[n].find('a')['href']\n",
    "    link = \"https://www.nytimes.com\" + link\n",
    "    nyt_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = nyt_tags_home[n].find('a').get_text()\n",
    "    nyt_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.time.attrs['datetime']\n",
    "    date = date[:-15]\n",
    "    nyt_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', {'class':['css-53u6y8', 'css-1fanzo5']})\n",
    "    final_article = \" \".join([item.text for item in body])\n",
    "        \n",
    "    nyt_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ae6966b82e57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0marticle_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0msoup_article\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html5lib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# get publication datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mParserRejectedMarkup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/bs4/builder/_html5lib.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mextra_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_specified_encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Set the character encoding detected by the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/html5lib/html5parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \"\"\"\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/html5lib/html5parser.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(self, stream, innerHTML, container, scripting, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscripting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscripting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTMLTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/html5lib/_tokenizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stream, parser, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHTMLInputStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/html5lib/_inputstream.py\u001b[0m in \u001b[0;36mHTMLInputStream\u001b[0;34m(source, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mHTMLUnicodeInputStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mHTMLBinaryInputStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/html5lib/_inputstream.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, override_encoding, transport_encoding, same_origin_parent_encoding, likely_encoding, default_encoding, useChardet)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;31m# Determine encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharEncoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetermineEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0museChardet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharEncoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/html5lib/_inputstream.py\u001b[0m in \u001b[0;36mdetermineEncoding\u001b[0;34m(self, chardet)\u001b[0m\n\u001b[1;32m    502\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                     \u001b[0mbuffers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m                     \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m                 \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookupEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/chardet/universaldetector.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLatin1Prober\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mprober\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mProbingState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFOUND_IT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m                     self.result = {'encoding': prober.charset_name,\n\u001b[1;32m    213\u001b[0m                                    \u001b[0;34m'confidence'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_confidence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/chardet/latin1prober.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mbyte_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_with_english_letters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbyte_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mchar_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLatin1_CharToClass\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/chardet/charsetprober.py\u001b[0m in \u001b[0;36mfilter_with_english_letters\u001b[0;34m(buf)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m# If current character is not extended-ASCII and not alphabetic...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mbuf_char\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34mb'\\x80'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf_char\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                 \u001b[0;31m# ...and we're not in a tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcurr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mprev\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_tag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# archive articles\n",
    "for n in np.arange(0, len(nyt_tags_archive)):\n",
    "\n",
    "    # get article link\n",
    "    link = nyt_tags_archive[n].find('a')['href']\n",
    "    link = \"https://www.nytimes.com\" + link\n",
    "    nyt_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = nyt_tags_archive[n].find('a').get_text()\n",
    "    nyt_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.time.attrs['datetime']\n",
    "    date = date[:-15]\n",
    "    nyt_dates.append(date)\n",
    "        \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', attrs = {'class':['css-53u6y8', 'css-1fanzo5 StoryBodyCompanionColumn']})\n",
    "    final_article = \" \".join([item.text for item in body])\n",
    "        \n",
    "    nyt_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling data\n",
    "nyt_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'new_york_times',\n",
    "    'date': nyt_dates,\n",
    "    'link': nyt_links,\n",
    "    'article_title': nyt_titles,\n",
    "    'article_text': nyt_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure it looks nice\n",
    "nyt_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in old data\n",
    "old_nyt_data = pd.read_csv('data/nyt_data.csv')\n",
    "num_old = len(old_nyt_data)\n",
    "\n",
    "# append new data\n",
    "nyt_data = old_nyt_data.append(nyt_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "nyt_data.to_csv(\"data/nyt_data.csv\", index = False)\n",
    "num_now = len(nyt_data)\n",
    "\n",
    "print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Buzzfeed - Very Liberal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HTML content using requests and save into a variable\n",
    "buzz_request = requests.get('https://www.buzzfeednews.com/section/politics')\n",
    "buzz_homepage = buzz_request.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create soup \n",
    "buzz_soup = BeautifulSoup(buzz_homepage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate article URLs\n",
    "buzz_tags = buzz_soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "number_of_articles = min(len(buzz_tags), 30)\n",
    "\n",
    "# get article titles, content, and links\n",
    "buzz_links = []\n",
    "buzz_titles = []\n",
    "buzz_dates = []\n",
    "buzz_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get article titles, content, and links\n",
    "for n in np.arange(0, number_of_articles):\n",
    "\n",
    "    # get article link\n",
    "    link = buzz_tags[n].find('a')['href']\n",
    "    buzz_links.append(link)\n",
    "    \n",
    "    # get article title\n",
    "    title = buzz_tags[n].find('a').get_text()\n",
    "    buzz_titles.append(title)\n",
    "    \n",
    "    # prep article content\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    \n",
    "    # get publication datetime\n",
    "    date = soup_article.find_all('div', class_=\"news-article-header__timestamps\")    \n",
    "    date = \" \".join([item.text for item in date]).replace('\\n', '')\n",
    "    buzz_dates.append(date)\n",
    "    \n",
    "    # get article content\n",
    "    body = soup_article.find_all('div', attrs={'data-module':'subbuzz-text'})\n",
    "    article = \" \".join([item.text for item in body]).replace('\\n', '')\n",
    "    final_article = re.sub(r' {[^}]*}', '', article)\n",
    "        \n",
    "    buzz_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling data\n",
    "buzz_data = pd.DataFrame.from_dict({\n",
    "    'publisher': 'buzzfeed',\n",
    "    'date': buzz_dates,\n",
    "    'link': buzz_links,\n",
    "    'article_title': buzz_titles,\n",
    "    'article_text': buzz_contents \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publisher</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>buzzfeed</td>\n",
       "      <td>Posted on March 12, 2020, at 4:46 ...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/paulmcleo...</td>\n",
       "      <td>The Trump Administration Will Move Ahead With ...</td>\n",
       "      <td>WASHINGTON — The Trump administration is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>buzzfeed</td>\n",
       "      <td>Posted on March 12, 2020, at 2:51 ...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/kadiagoba...</td>\n",
       "      <td>Members Of Congress Are Furious At The Lack Of...</td>\n",
       "      <td>WASHINGTON — A day after the World Health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>buzzfeed</td>\n",
       "      <td>Posted on March 12, 2020, at 2:44 ...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/henrygome...</td>\n",
       "      <td>Joe Biden’s Coronavirus Speech And Campaign Sh...</td>\n",
       "      <td>Joe Biden is looking past the remaining p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>buzzfeed</td>\n",
       "      <td>Last updated on March 12, 2020, at...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/matthewch...</td>\n",
       "      <td>A Brazilian Official Who Met Trump At Mar-A-La...</td>\n",
       "      <td>Brazilian President Jair Bolsonaro's pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>buzzfeed</td>\n",
       "      <td>Posted on March 11, 2020, at 5:06 ...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/ryanmac/c...</td>\n",
       "      <td>Secret Users Of Clearview AI’s Facial Recognit...</td>\n",
       "      <td>On a flight to Boston in January, James, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publisher                                               date  \\\n",
       "0  buzzfeed              Posted on March 12, 2020, at 4:46 ...   \n",
       "1  buzzfeed              Posted on March 12, 2020, at 2:51 ...   \n",
       "2  buzzfeed              Posted on March 12, 2020, at 2:44 ...   \n",
       "3  buzzfeed              Last updated on March 12, 2020, at...   \n",
       "4  buzzfeed              Posted on March 11, 2020, at 5:06 ...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.buzzfeednews.com/article/paulmcleo...   \n",
       "1  https://www.buzzfeednews.com/article/kadiagoba...   \n",
       "2  https://www.buzzfeednews.com/article/henrygome...   \n",
       "3  https://www.buzzfeednews.com/article/matthewch...   \n",
       "4  https://www.buzzfeednews.com/article/ryanmac/c...   \n",
       "\n",
       "                                       article_title  \\\n",
       "0  The Trump Administration Will Move Ahead With ...   \n",
       "1  Members Of Congress Are Furious At The Lack Of...   \n",
       "2  Joe Biden’s Coronavirus Speech And Campaign Sh...   \n",
       "3  A Brazilian Official Who Met Trump At Mar-A-La...   \n",
       "4  Secret Users Of Clearview AI’s Facial Recognit...   \n",
       "\n",
       "                                        article_text  \n",
       "0       WASHINGTON — The Trump administration is ...  \n",
       "1       WASHINGTON — A day after the World Health...  \n",
       "2       Joe Biden is looking past the remaining p...  \n",
       "3       Brazilian President Jair Bolsonaro's pres...  \n",
       "4       On a flight to Boston in January, James, ...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buzz_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of entries in new data: 50\n"
     ]
    }
   ],
   "source": [
    "# read in old data\n",
    "old_buzz_data = pd.read_csv('data/buzzfeed_data.csv')\n",
    "num_old = len(old_buzz_data)\n",
    "\n",
    "# append new data\n",
    "buzz_data = old_buzz_data.append(buzz_data).drop_duplicates()\n",
    "\n",
    "# save new .csv\n",
    "buzz_data.to_csv(\"data/buzzfeed_data.csv\", index = False)\n",
    "num_now = len(buzz_data)\n",
    "\n",
    "#print(\"number of entries in old data: {}\".format(num_old))\n",
    "print(\"total number of entries in new data: {}\".format(num_now))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
